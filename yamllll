Based on our preprod validation, we've identified an issue in the Aurora Schema where some records appear to be missing. This discrepancy may be attributed to a checkpoint problem during the data processing from Event Hub. It's possible that data was read from the same Event Hub into various bronze layers, resulting in the loss of records that were already processed. To address this, we need to perform schema cleanup and reload data in the preprod environment.

It's important to note that our preprod environment doesn't fully mirror production in terms of performance, data volume, and functionality. Therefore, we've made the decision to proceed with the planned production release on September 28th, with the approval of associated risks and a backup plan in place. We are confident that this issue will not occur in the production environment. However, it's worth mentioning that this issue will have an impact on downstream testing activities for the next release.

For the current release, we have decided to implement the following recommendations:

Maintain a local production current version code copy.
Implement a time-travel mechanism.
Consider having an analytical production copy in the preprod environment for post-release code comparison.
Ensure that the POMI user validates post-release data as part of the UAT signoff.
Looking ahead to future releases, we recognize the importance of data cleanup in lower environments and the need to maintain a single true schema in preprod to prevent data synchronization issues.


-----


We've divided this problem into two aspects: one concerning duplicates and the other related to data discrepancies. The duplicate problem has been successfully resolved in the lower test environment; we have verified that there are no duplicates. However, regarding the data issue, we've noticed some gaps in the years loaded, and this doesn't align with our current production data. Further investigation is needed in this area.

Since the primary concern, which is the duplicates, has been addressed and resolved, we are categorizing this issue as a defect in S3. It has been assigned to the designer for in-depth analysis regarding the data aspect.
