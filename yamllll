Subject: Azure Cost Analysis Challenges and Clarity on Preproduction Cost Estimates

Hey there,

We're in the midst of tackling some key challenges related to Azure cost analysis for our preproduction phase. In our quest for clarity on cost estimates, we're considering three distinct approaches:

Option 1: Utilizing Test Environment Data for Cost Analysis
Our first option involves conducting a cost analysis by leveraging the compute and storage resources in our lower test environment over a 2-3 day period, focusing on the latest code version. However, a few roadblocks stand in our way:

The data volume in our test environment doesn't align with that of preproduction, which might affect the accuracy of our cost analysis.
For a more precise estimate, we'd ideally require actual and projected data on reads, writes, and volume in both environments. This would help us arrive at a closer approximation of preproduction costs.
In our test environment, tracking compute costs proves challenging, given that job runs are initiated manually rather than through an analytical layer workflow. Consequently, we lack insight into the frequency of these manual triggers.
In the test environment, we face difficulties in isolating storage costs for Aurora in the cost analysis portal due to a lack of project-level segregation.
Option 2: Estimating Costs Based on Previous Preproduction Runs
Our second option explores the possibility of estimating costs based on past runs in the preproduction environment. But this path isn't without its obstacles:

We're hesitant to rely on this method due to historical data ingestion practices that involved multiple schemas running in parallel, which caused some unexpected spikes. Even if we were to focus solely on compute costs for specific components (like gold quotas), it wouldn't provide a comprehensive estimated cost.
This option also introduces uncertainty since we're planning to copy data from production to the analytical layer this time around, which could alter the cost estimate dynamics.
Option 3: Adapting the Microsoft Pricing Calculator
Option three involves adopting the Microsoft Pricing Calculator, a native tool. However, embracing this approach hinges on obtaining the following information upfront:

Preproduction projections for reads, writes, data scans, data returns, and expected DBU utilization. These are integral to kicking off the adaptation process.
If someone within our team has previous experience with this method, we'll gladly follow in their footsteps.
Option 4: Benchmarking Against Production Costs
Our final option proposes benchmarking preproduction costs against the existing production environment. We're planning to introduce a new fact table and five reference tables. The foundation for this estimate will be production costs at the silver and gold levels, with a modest addition of 2-3% for an estimated cost that aligns more closely with reality.

It's important to note that none of these options have been finalized, and we're open to your thoughts and suggestions as we collectively work towards a solution. Your feedback is incredibly valuable in helping us navigate these challenges and make informed decisions.

Cheers,
