# Import necessary libraries
import pandas as pd
from sklearn.ensemble import IsolationForest

# Function to perform advanced pattern analysis
def analyze_table_anomalies(database_name, table_name):
    # Read data from the specified table
    query = f"SELECT * FROM {database_name}.{table_name}"
    df = spark.sql(query).toPandas()

    # Loop through columns and perform anomaly detection
    anomalies = {}
    for column in df.columns:
        # Skip non-numeric columns (you can customize this)
        if not pd.api.types.is_numeric_dtype(df[column]):
            continue

        # Perform anomaly detection using Isolation Forest
        clf = IsolationForest(contamination=0.05)  # Adjust contamination based on your data
        X = df[[column]].values
        clf.fit(X)
        df[column + '_anomaly'] = clf.predict(X)
        anomalies[column] = df[df[column + '_anomaly'] == -1]

    return anomalies

# Example usage
database_name = "your_database_name"
table_name = "your_table_name"
result = analyze_table_anomalies(database_name, table_name)

# Display the results
for column, anomaly_df in result.items():
    print(f"Anomalies in {column}:")
    print(anomaly_df)
