from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when
from pyspark.sql.types import StringType

# Initialize a Spark session
spark = SparkSession.builder.appName("ValuePatternAnalysis").getOrCreate()

# Replace with your database and table names
database_name = "your_database_name"
table_name = "your_table_name"

# Read data from the specified table
table_df = spark.table(f"{database_name}.{table_name}")

# Define a function to perform pattern analysis for a column
def analyze_column(column_name):
    # Calculate the count of distinct values for the column
    distinct_count = table_df.select(column_name).distinct().count()

    # Calculate the total count of values in the column
    total_count = table_df.select(column_name).count()

    # Calculate the ratio of distinct values to total values
    distinct_ratio = distinct_count / total_count

    # Define a threshold for anomaly detection (you can adjust this threshold)
    anomaly_threshold = 0.05  # For example, 5% distinct values could be considered an anomaly

    # Determine if the column has an anomaly based on the threshold
    is_anomaly = when(distinct_ratio < anomaly_threshold, "Anomaly").otherwise("Normal")

    return distinct_count, total_count, is_anomaly

# Perform pattern analysis for each column in the table
column_analyses = []
for column in table_df.columns:
    distinct_count, total_count, is_anomaly = analyze_column(column)
    column_analyses.append((column, distinct_count, total_count, is_anomaly))

# Create a DataFrame with the results
result_df = spark.createDataFrame(column_analyses, ["Column", "DistinctCount", "TotalCount", "AnomalyStatus"])

# Show the results
result_df.show()

# Stop the Spark session
spark.stop()
