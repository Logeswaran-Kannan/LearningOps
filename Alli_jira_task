from pyspark.sql.functions import col, lit
from functools import reduce

# Define your list of target tables (without catalog/schema)
input_tables = ["sales_fact", "customer_dim", "inventory_snapshot"]  # ðŸ‘ˆ Update this list

catalog = "core_tst"
schema = "curated_edw"
full_names = [f"{catalog}.{schema}.{t}" for t in input_tables]

# Load lineage data once
lineage_df = spark.read.table("system.access.table_lineage")

results = []

for full_name in full_names:
    try:
        # Get the physical path of the table
        path = spark.sql(f"DESCRIBE DETAIL {full_name}").select("location").head()[0]

        # Find upstream lineage (sources feeding this table)
        upstream = lineage_df.filter(
            (col("target_table_full_name") == full_name) |
            (col("target_path") == path)
        ).select(
            lit(full_name).alias("table_name"),
            lit("upstream").alias("relation_type"),
            col("source_table_full_name").alias("related_table"),
            col("source_path").alias("related_path"),
            col("execution_time")
        )

        # Find downstream lineage (targets fed by this table)
        downstream = lineage_df.filter(
            (col("source_table_full_name") == full_name) |
            (col("source_path") == path)
        ).select(
            lit(full_name).alias("table_name"),
            lit("downstream").alias("relation_type"),
            col("target_table_full_name").alias("related_table"),
            col("target_path").alias("related_path"),
            col("execution_time")
        )

        results.extend([upstream, downstream])

    except Exception as e:
        print(f"Skipping {full_name}: {e}")

# Combine and display the lineage results
if results:
    final_df = reduce(lambda a, b: a.unionByName(b), results)
    display(final_df.orderBy("table_name", "relation_type", "execution_time"))
else:
    print("No lineage information found for input tables.")
