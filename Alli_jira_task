from pyspark.sql import SparkSession, functions as F

# Initialize Spark
spark = SparkSession.builder.getOrCreate()

# Step 1: Read the Delta table
df = spark.table("core_preprod.default.table_counts")

# Step 2: Get the 2 latest run_timestamps
latest_runs = (
    df.select("run_timestamp")
    .distinct()
    .orderBy(F.col("run_timestamp").desc())
    .limit(2)
    .collect()
)

# Ensure we have enough runs to compare
if len(latest_runs) < 2:
    raise Exception("â— Not enough run history to compare. You need at least two run_timestamp entries.")

latest_ts = latest_runs[0][0]
previous_ts = latest_runs[1][0]

print(f"ðŸ” Comparing runs: {previous_ts} â†’ {latest_ts}")

# Step 3: Filter relevant data
df_filtered = df.filter(F.col("run_timestamp").isin([latest_ts, previous_ts]))

# Step 4: Pivot table so each run_timestamp becomes a column
df_pivot = df_filtered.groupBy("table_name").pivot("run_timestamp").agg(F.first("record_count"))

# Step 5: Rename columns for clarity
df_pivot = df_pivot.withColumnRenamed(str(previous_ts), "previous_count") \
                   .withColumnRenamed(str(latest_ts), "current_count")

# Step 6: Add a column showing the difference
df_result = df_pivot.withColumn("count_diff", F.col("current_count") - F.col("previous_count"))

# Step 7: Display results in notebook
df_result.display()  # Or use df_result.show() if display() is not supported
