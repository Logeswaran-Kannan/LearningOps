from pyspark.sql import SparkSession
from delta.tables import DeltaTable
import time

spark = SparkSession.builder.getOrCreate()

# Settings
source_catalog = "core"
source_db = "ods"
target_table = "core_preprod.default.record_counts"
prefixes = ("policy", "billing", "claim")

# Step 1: Get filtered table list
tables_df = spark.sql(f"SHOW TABLES IN {source_catalog}.{source_db}")
filtered_tables = tables_df.filter(
    tables_df.tableName.startswith(prefixes[0]) |
    tables_df.tableName.startswith(prefixes[1]) |
    tables_df.tableName.startswith(prefixes[2])
)

table_list = [f"{source_catalog}.{source_db}.{row.tableName}" for row in filtered_tables.collect()]
total_tables = len(table_list)

# Broadcast the table list for efficient distribution
sc = spark.sparkContext
table_rdd = sc.parallelize(table_list, numSlices=min(50, total_tables))  # Adjust numSlices for parallelism

# Function to get row count using Delta metadata
def get_table_count(table_name):
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.getOrCreate()
    try:
        # Try to get count from DESCRIBE DETAIL
        details = spark.sql(f"DESCRIBE DETAIL {table_name}").collect()[0]
        count = details['numRecords'] if details['numRecords'] is not None else -1
        return (table_name, count, "OK")
    except Exception as e:
        return (table_name, None, f"ERROR: {str(e)}")

# Step 2: Parallel count with progress tracking
results = []
processed = 0

for res in table_rdd.map(get_table_count).toLocalIterator():
    results.append(res)
    processed += 1
    print(f"Progress: {processed} of {total_tables} tables processed.")

# Step 3: Save results
results_df = spark.createDataFrame(results, ["TableName", "RecordCount", "Status"])
results_df.write.mode("overwrite").format("delta").saveAsTable(target_table)

# Display results
results_df.display()
