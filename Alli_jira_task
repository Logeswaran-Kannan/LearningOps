ğŸ” Test Scenario ID: EH-BR-01 â€“ Message Receipt Validation
ğŸ”¹ Purpose
This test verifies that all messages sent from Athena (or other MQS-integrated applications) through MuleSoft to Azure Event Hub are successfully ingested into the Bronze (Landed) table via Databricks DLT. The test ensures the integrity and completeness of raw data ingestion, which is foundational for all downstream data transformations, analytics, and business processes.

ğŸ”¹ Scope and Components Involved
Source Application: Athena / CRS â†’ MQS

Integration: MuleSoft (MQS Adapter publishes to EventHub)

Stream Target: Azure Event Hub (dedicated for Radar Live)

Processing Layer: Databricks DLT pipeline

Storage Layer: Bronze (ODS / Landed Table in Delta Lake)

ğŸ”¹ Key Validation Objectives
Validate message delivery from source to Bronze with no loss or duplication.

Confirm all messages have been correctly tagged with ingestion metadata such as eventhub_enqueued_time and _ingest_time.

Establish traceability of records from source system identifiers (e.g., quote_id, message_id) through to the Bronze layer.

ğŸ§ª Testing Options & Analysis
âœ… Option 1: Validate message count at the source (Athena or Mule logs) vs. Bronze table
Description: Compare outbound log counts or intermediate staging tables (e.g., CosmosDB or audit trail in Mule) with Bronze records.

Feasibility: Possible only if Mule logs or database records are available and contain unique identifiers (e.g., quote_id).

Risk:

Logs may be truncated or sampled.

Retry logic could lead to overcounting.

Lack of access or schema drift may hinder comparison.

âœ… Option 2: Compare EventHub message count from Azure diagnostics vs. Bronze table
Description: Use EventHub diagnostic logs or metrics (e.g., incoming requests per partition) to validate Bronze DLT consumption.

Feasibility: Very feasible using Azure Monitor or diagnostic settings.

Risk:

Time skew could cause inaccurate match unless test window is clearly defined.

Metric granularity may be too coarse (not message-specific).

âœ… Option 3: Create controlled message batch and match to Bronze
Description: Push known message count (e.g., 100 unique quote_id messages) to EventHub and validate exact count lands in Bronze.

Feasibility: Highly controlled and reliable for regression or smoke testing.

Risk:

Requires test data isolation.

May not represent production-scale performance or failures.

ğŸ§¾ Additional Validations
In addition to count matching, validate that the following metadata fields are present and accurate in Bronze:

message_id / quote_id â€“ unique identifiers from source

eventhub_enqueued_time â€“ EventHub system time of receipt

_ingest_time â€“ Databricks DLT ingestion timestamp

event_type, source_system â€“ to enable filtering and lineage

âœ… Success Criteria
Message count in Bronze exactly matches expected from the source or EventHub for the defined test window.

No duplicate message_id or quote_id values unless allowed by design.

Ingestion metadata is populated and accurate.

All messages are traceable back to their original event stream.

ğŸ“Œ Data Quality Requirement Traceability
DQ Code	Dimension	Mapping to EH-BR-01
RL DQ01	Accuracy	Ensures the records in Bronze match those sent from source without modification.
RL DQ03	Completeness	Validates no message is lost between EventHub and Bronze.
RL DQ06	Uniqueness	Prevents duplicates by ensuring one-to-one mapping of message_id or quote_id.
RL DQ09	Traceability	Enables lineage by verifying message_id, eventhub_enqueued_time, _ingest_time, etc.
RL DQ04	Timeliness	Ensures ingestion is in near real-time, validated by _ingest_time vs. enqueued time.

Let me know if youâ€™d like this scenario added into your consolidated Excel test suite, or if you'd like similar full-detail entries for the rest of your scenarios (like EH-BR-02: Schema Conformance).
