from pyspark.sql.functions import col, explode, from_json
from pyspark.sql.types import *

# ----------------------------
# Step 1: Define schema for source Payload JSON
# ----------------------------
schema = StructType([
    StructField("coreRisk", ArrayType(
        StructType([
            StructField("drivers", ArrayType(
                StructType([
                    StructField("driver_Prn", StringType()),
                    StructField("incidents", StructType([
                        StructField("claims", ArrayType(
                            StructType([
                                StructField("claim_ClaimType", StringType()),
                                StructField("claim_Date", StringType()),
                                StructField("claim_DriverAtFaultInd", StringType()),
                                StructField("claim_NcdLostInd", StringType()),
                                StructField("claim_Prn", IntegerType())
                            ])
                        ))
                    ]))
                ])
            ))
        ])
    ))
])

# ----------------------------
# Step 2: Extract source data
# ----------------------------
source_df = spark.table("mqs.dlt_quote_request")
source_df = source_df.withColumn("Payload_str", col("Payload").cast("string"))
parsed_df = source_df.withColumn("parsed", from_json(col("Payload_str"), schema))

core_risk_df = parsed_df.select(explode("parsed.coreRisk").alias("coreRisk"))
drivers_df = core_risk_df.select(explode("coreRisk.drivers").alias("driver"))
claims_df = drivers_df.select(
    col("driver.driver_Prn"),
    explode("driver.incidents.claims").alias("claim")
)

source_final = claims_df.select(
    col("driver_Prn"),
    col("claim.claim_ClaimType").alias("claim_ClaimType"),
    col("claim.claim_Date").alias("claim_Date"),
    col("claim.claim_DriverAtFaultInd").alias("claim_DriverAtFaultInd"),
    col("claim.claim_NcdLostInd").alias("claim_NcdLostInd"),
    col("claim.claim_Prn").cast("string").alias("claim_Prn")
)

# ----------------------------
# Step 3: Load target data and exclude audit columns
# ----------------------------
exclude_cols = ["QUOTE_KEY", "MULTI_ACTIVE_SQNC", "LOAD_DT", "DWN_ANCHOR_DT", "REC_SRCE"]
target_df = spark.table("claim_mtr_atna")
target_final = target_df.drop(*exclude_cols)

# ----------------------------
# Step 4: Align columns for comparison (lowercase & ordered)
# ----------------------------
source_final = source_final.select(sorted(source_final.columns))
target_final = target_final.select(sorted(target_final.columns))

# ----------------------------
# Step 5: Compare source vs target
# ----------------------------
source_count = source_final.count()
target_count = target_final.count()

source_minus_target = source_final.subtract(target_final)
target_minus_source = target_final.subtract(source_final)

# Show counts and differences
print("üî¢ Source count:", source_count)
print("üì¶ Target count:", target_count)

print("\n‚úÖ Records in source but not in target:")
source_minus_target.show(truncate=False)

print("\n‚ö†Ô∏è Records in target but not in source:")
target_minus_source.show(truncate=False)
