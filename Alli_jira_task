from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lower

# Initialize Spark session
spark = SparkSession.builder.appName("BooleanColumnConversionAndComparison").getOrCreate()

# Define catalogues and databases
source_catalog = "core_tst_sys9"
source_db = "rr_source"
target_catalog = "core_tst_std001"
target_db = "ods"

# List of input table names
input_table_names = ["ptable1", "btable2", "ctable3"]  # Update with actual input table names

# Function to convert boolean-like columns
def convert_boolean_columns(df):
    for column in df.columns:
        unique_vals = df.select(column).distinct().rdd.flatMap(lambda x: x).collect()
        if set(unique_vals).issubset({"t", "f"}):
            df = df.withColumn(column, when(col(column) == "t", 1).otherwise(0))
    return df

# Output results
output = []

for table in input_table_names:
    source_table = f"{source_catalog}.{source_db}.{table}"

    if table.startswith("p"):
        target_table = f"{target_catalog}.{target_db}.policycenter_{table}"
    elif table.startswith("b"):
        target_table = f"{target_catalog}.{target_db}.billingcenter_{table}"
    elif table.startswith("c"):
        target_table = f"{target_catalog}.{target_db}.claimcenter_{table}"
    else:
        target_table = f"{target_catalog}.{target_db}.{table}"

    try:
        source_df = spark.table(source_table)
        source_exists = True
    except:
        source_exists = False

    try:
        target_df = spark.table(target_table)
        target_exists = True
    except:
        target_exists = False

    result = {
        "input_table": table,
        "source_table": source_table if source_exists else "missing",
        "target_table": target_table if target_exists else "missing",
        "source_count": None,
        "target_count": None,
        "missing_columns_in_source": [],
        "missing_columns_in_target": [],
        "mismatch_columns_src_vs_tgt": [],
        "mismatch_columns_tgt_vs_src": [],
        "duplicates_in_target": 0
    }

    if source_exists and target_exists:
        source_df = convert_boolean_columns(source_df)
        target_df = convert_boolean_columns(target_df)
        target_df = target_df.drop("azure_load_date")

        source_df = source_df.select([col(c).alias(c.lower()) for c in source_df.columns])
        target_df = target_df.select([col(c).alias(c.lower()) for c in target_df.columns])

        result["source_count"] = source_df.count()
        result["target_count"] = target_df.count()

        if result["source_count"] == result["target_count"]:
            source_cols = set(source_df.columns)
            target_cols = set(target_df.columns)

            result["missing_columns_in_source"] = list(target_cols - source_cols)
            result["missing_columns_in_target"] = list(source_cols - target_cols)

            common_cols = list(source_cols & target_cols)

            for col_name in common_cols:
                mismatches_src = source_df.select(col_name).subtract(target_df.select(col_name)).count()
                mismatches_tgt = target_df.select(col_name).subtract(source_df.select(col_name)).count()
                if mismatches_src > 0:
                    result["mismatch_columns_src_vs_tgt"].append(col_name)
                if mismatches_tgt > 0:
                    result["mismatch_columns_tgt_vs_src"].append(col_name)

            duplicate_count = target_df.groupBy(common_cols).count().filter(col("count") > 1).count()
            result["duplicates_in_target"] = duplicate_count

    output.append(result)

# Convert output to DataFrame and create a temporary view
output_df = spark.createDataFrame(output)
output_df.createOrReplaceTempView("comparison_summary")
