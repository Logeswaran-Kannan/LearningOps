from pyspark.sql import SparkSession
import concurrent.futures
import threading

# Initialize Spark
spark = SparkSession.builder.getOrCreate()

# Settings
catalog = "core"
database = "ods"
prefixes = ("policy", "billing", "claim")
target_table = "core_preprod.default.delta_table_counts"
max_threads = 10  # Tune based on cluster size

# Lock for thread-safe progress updates
progress_lock = threading.Lock()
progress = {"done": 0, "total": 0}

# Step 1: Get filtered tables
tables_df = spark.sql(f"SHOW TABLES IN {catalog}.{database}")
filtered_tables = tables_df.filter(
    tables_df.tableName.startswith(prefixes[0]) |
    tables_df.tableName.startswith(prefixes[1]) |
    tables_df.tableName.startswith(prefixes[2])
)

table_list = [f"{catalog}.{database}.{row.tableName}" for row in filtered_tables.collect()]
progress["total"] = len(table_list)

# Step 2: Define parallel count function
def count_table(table_name):
    try:
        count = spark.read.table(table_name).count()
    except Exception as e:
        count = f"Error: {str(e)}"
    
    with progress_lock:
        progress["done"] += 1
        print(f"[{progress['done']}/{progress['total']}] Counted: {table_name}")
    
    return (table_name, count)

# Step 3: Execute in parallel
results = []
with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
    futures = [executor.submit(count_table, tbl) for tbl in table_list]
    for f in concurrent.futures.as_completed(futures):
        results.append(f.result())

# Step 4: Store in Delta table
result_df = spark.createDataFrame(results, ["TableName", "RecordCount"])
result_df.write.mode("overwrite").format("delta").saveAsTable(target_table)

# Final display
print("\nâœ… Table counts completed.")
result_df.display()
