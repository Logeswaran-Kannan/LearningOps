import os
from pyspark.sql import SparkSession

# Folder path in DBFS or mounted storage
folder_path = "dbfs:/mnt/your-folder-path/"  # Replace with your actual path

# Catalog and database
catalog_name = "core_tst_sys9"
database_name = "rr_source"

# Set the current catalog and database
spark.sql(f"USE CATALOG {catalog_name}")
spark.sql(f"USE {database_name}")

# List all CSV files in the folder
files = dbutils.fs.ls(folder_path)

for file in files:
    if file.name.endswith(".csv"):
        table_name = file.name.replace(".csv", "").lower()
        file_path = file.path
        
        # Read CSV
        df = spark.read.option("header", True).option("inferSchema", True).csv(file_path)
        
        # Create table (overwrite if exists)
        df.write.mode("overwrite").saveAsTable(f"{catalog_name}.{database_name}.{table_name}")
        
        print(f"Table created: {catalog_name}.{database_name}.{table_name}")
