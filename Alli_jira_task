from pyspark.sql import SparkSession
from delta.tables import DeltaTable
from concurrent.futures import ThreadPoolExecutor, as_completed

# Initialize Spark
spark = SparkSession.builder.getOrCreate()

# Config
catalog = "core"
database = "ods"
prefixes = ("policy", "billing", "claim")
max_threads = 10  # Adjust based on cluster size
result_table = "default.table_record_counts_parallel"

# Fetch tables
tables_df = spark.sql(f"SHOW TABLES IN {catalog}.{database}")
filtered_tables = tables_df.filter(
    tables_df.tableName.startswith(prefixes[0]) |
    tables_df.tableName.startswith(prefixes[1]) |
    tables_df.tableName.startswith(prefixes[2])
)

# Get full table names
qualified_tables = [
    f"{catalog}.{database}.{row.tableName}"
    for row in filtered_tables.collect()
]

# Function to get record count
def get_table_count(table_name):
    try:
        # Try Delta metadata-based count
        detail = spark.sql(f"DESCRIBE DETAIL {table_name}").collect()[0]
        num_records = detail.get("numRecords", None)
        if num_records is not None:
            return (table_name, num_records)
        else:
            # Fallback to full scan
            count = spark.read.table(table_name).count()
            return (table_name, count)
    except Exception as e:
        return (table_name, f"Error: {str(e)}")

# Run in parallel
results = []
with ThreadPoolExecutor(max_workers=max_threads) as executor:
    future_to_table = {executor.submit(get_table_count, tbl): tbl for tbl in qualified_tables}
    for future in as_completed(future_to_table):
        result = future.result()
        results.append(result)

# Convert results to DataFrame and store
results_df = spark.createDataFrame(results, ["TableName", "RecordCount"])
results_df.write.mode("overwrite").format("delta").saveAsTable(result_table)

# Display
results_df.display()
