from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lower
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType
from pyspark.sql.utils import AnalysisException
from concurrent.futures import ThreadPoolExecutor

# Initialize Spark session
spark = SparkSession.builder.appName("AutomatedValidation").getOrCreate()

# Catalog and database details
source_catalog = "core_tst_sys9"
source_db = "rr_source"
target_catalog = "core_tst_std001"
target_db = "ods"

# Table names to process
table_names = ["ptable1", "btable2", "ctable3"]

# Date range for filtering
filter_start = '2025-05-12T00:00:01.0025+00:00'
filter_end = '2025-05-13T00:00:01.0025+00:00'

# Functions

def convert_boolean_columns(df):
    for column in df.columns:
        unique_vals = df.select(column).distinct().limit(100).rdd.flatMap(lambda x: x).collect()
        if set(unique_vals).issubset({"t", "f"}):
            df = df.withColumn(column, when(col(column) == "t", 1).otherwise(0))
    return df

def to_lowercase_columns(df):
    return df.select([col(c).alias(c.lower()) for c in df.columns])

def cast_columns_to_string(df, columns):
    for col_name in columns:
        df = df.withColumn(col_name, col(col_name).cast("string"))
    return df.select([col(c) for c in columns])

def validate_table(table):
    # Initialize
    source_sql = ""
    target_sql = ""
    post_dedup_count = 0
    comment = ""
    missing_in_source = []
    missing_in_target = []
    mismatches = []
    mismatches_target = []
    duplicate_count = 0
    source_count = 0
    target_count = 0
    target_filtered_count = 0
    target_table = ""

    try:
        source_table = f"{source_catalog}.{source_db}.{table}"
        source_df = spark.table(source_table)
        source_df = convert_boolean_columns(source_df)
        source_df = to_lowercase_columns(source_df)
        source_columns = set(source_df.columns)
        source_count = source_df.count()
    except Exception:
        return {
            "input_table": table,
            "source_table": "missing",
            "target_table": "missing",
            "source_count": 0,
            "target_count": 0,
            "target_filtered_count": 0,
            "missing_in_source": [],
            "missing_in_target": [],
            "mismatches": [],
            "mismatches_target": [],
            "duplicate_check_in_target": 0,
            "source_sql": "",
            "target_sql": "",
            "post_dedup_count": 0,
            "comment": "source table load failure"
        }

    # Determine target
    prefix = table[0].lower()
    if prefix == 'p':
        target_table_name = f"policycenter_{table}"
    elif prefix == 'b':
        target_table_name = f"billingcenter_{table}"
    elif prefix == 'c':
        target_table_name = f"claimcenter_{table}"
    else:
        target_table_name = table

    try:
        target_table = f"{target_catalog}.{target_db}.{target_table_name}"
        target_df = spark.table(target_table)
        target_df = convert_boolean_columns(target_df)
        target_df = to_lowercase_columns(target_df)

        if "azure_load_date" in target_df.columns:
            target_df = target_df.drop("azure_load_date")

        target_columns = set(target_df.columns)
        has_createtime = "createtime" in target_columns
        has_updatetime = "updatetime" in target_columns

        if not (has_createtime or has_updatetime):
            target_df = target_df.dropDuplicates()
            target_count = target_df.count()
            target_filtered_count = target_count
        else:
            target_count = target_df.count()
            if has_createtime and target_count > 5000:
                target_df = target_df.filter((col("createtime") >= filter_start) & (col("createtime") <= filter_end))
            target_filtered_count = target_df.count()
    except Exception:
        return {
            "input_table": table,
            "source_table": source_table,
            "target_table": "missing",
            "source_count": source_count,
            "target_count": 0,
            "target_filtered_count": 0,
            "missing_in_source": [],
            "missing_in_target": [],
            "mismatches": [],
            "mismatches_target": [],
            "duplicate_check_in_target": 0,
            "source_sql": f"SELECT * FROM {source_table}",
            "target_sql": "",
            "post_dedup_count": 0,
            "comment": "target table load failure"
        }

    # Column alignment and comparison
    common_columns = list(source_columns & target_columns)
    missing_in_source = list(target_columns - source_columns)
    missing_in_target = list(source_columns - target_columns)

    source_comp_df = cast_columns_to_string(source_df.select(common_columns), common_columns)
    target_comp_df = cast_columns_to_string(target_df.select(common_columns), common_columns)

    if not ("createtime" in target_df.columns or "updatetime" in target_df.columns):
        source_comp_df = source_comp_df.dropDuplicates()
        target_comp_df = target_comp_df.dropDuplicates()

    source_only = source_comp_df.subtract(target_comp_df).limit(1).count()
    target_only = target_comp_df.subtract(source_comp_df).limit(1).count()

    if source_only > 0 or target_only > 0:
        mismatches = common_columns
        mismatches_target = common_columns

    post_dedup_count = target_comp_df.count()
    duplicate_count = max(0, target_count - post_dedup_count)

    source_sql = f"SELECT {', '.join(common_columns)} FROM {source_table}"
    target_sql = f"SELECT {', '.join(common_columns)} FROM {target_table}"

    comment_parts = []
    if source_count != post_dedup_count:
        comment_parts.append("count mismatch")
    if missing_in_source:
        comment_parts.append("missing columns in source")
    if missing_in_target:
        comment_parts.append("missing columns in target")
    if mismatches:
        comment_parts.append("data mismatch")
    if duplicate_count > 0:
        comment_parts.append("duplicates in target")
    comment = " | ".join(comment_parts) if comment_parts else "All checks passed"

    return {
        "input_table": table,
        "source_table": source_table,
        "target_table": target_table,
        "source_count": source_count,
        "target_count": target_count,
        "target_filtered_count": target_filtered_count,
        "missing_in_source": missing_in_source,
        "missing_in_target": missing_in_target,
        "mismatches": mismatches,
        "mismatches_target": mismatches_target,
        "duplicate_check_in_target": duplicate_count,
        "source_sql": source_sql,
        "target_sql": target_sql,
        "post_dedup_count": post_dedup_count,
        "comment": comment
    }

# Run validation in parallel
with ThreadPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(validate_table, table_names))

# Define schema for result rows
schema = StructType([
    StructField("input_table", StringType(), True),
    StructField("source_table", StringType(), True),
    StructField("target_table", StringType(), True),
    StructField("source_count", IntegerType(), True),
    StructField("target_count", IntegerType(), True),
    StructField("target_filtered_count", IntegerType(), True),
    StructField("missing_in_source", ArrayType(StringType()), True),
    StructField("missing_in_target", ArrayType(StringType()), True),
    StructField("mismatches", ArrayType(StringType()), True),
    StructField("mismatches_target", ArrayType(StringType()), True),
    StructField("duplicate_check_in_target", IntegerType(), True),
    StructField("source_sql", StringType(), True),
    StructField("target_sql", StringType(), True),
    StructField("post_dedup_count", IntegerType(), True),
    StructField("comment", StringType(), True)
])

# Convert to DataFrame using defined schema
result_rdd = spark.sparkContext.parallelize([(
    r["input_table"], r["source_table"], r["target_table"],
    r["source_count"], r["target_count"], r["target_filtered_count"],
    r["missing_in_source"], r["missing_in_target"], r["mismatches"],
    r["mismatches_target"], r["duplicate_check_in_target"],
    r["source_sql"], r["target_sql"], r["post_dedup_count"], r["comment"]
) for r in results])

result_df = spark.createDataFrame(result_rdd, schema)
result_df.write.mode("overwrite").saveAsTable("core_tst_sys9.default.validation_results")
result_df.show(truncate=False)
