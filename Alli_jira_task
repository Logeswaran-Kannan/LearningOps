from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lower
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType
from concurrent.futures import ThreadPoolExecutor

spark = SparkSession.builder.appName("AutomatedValidation").getOrCreate()

source_catalog = "core_tst_sys9"
source_db = "rr_source"
target_catalog = "core_tst_std001"
target_db = "ods"

table_names = ["ptable1", "btable2", "ctable3"]

filter_start = '2025-05-12T00:00:01.0025+00:00'
filter_end = '2025-05-13T00:00:01.0025+00:00'

def convert_boolean_columns(df):
    for column in df.columns:
        # Skip columns already numeric
        dt = dict(df.dtypes)[column]
        if dt not in ["string", "varchar"]:
            continue
        unique_vals = df.select(column).distinct().limit(100).rdd.flatMap(lambda x: x).collect()
        if set(unique_vals).issubset({"t", "f"}):
            df = df.withColumn(column, when(col(column) == "t", 1).otherwise(0))
    return df

def to_lowercase_columns(df):
    return df.select([col(c).alias(c.lower()) for c in df.columns])

def cast_columns_to_string(df, columns):
    for col_name in columns:
        df = df.withColumn(col_name, col(col_name).cast("string"))
    return df.select([col(c) for c in columns])

def normalize_dataframe(df, columns):
    from pyspark.sql.functions import when, col
    dtypes = dict(df.dtypes)
    for col_name in columns:
        if dtypes[col_name] in ["string", "varchar"]:
            df = df.withColumn(col_name, when(col(col_name).isNull(), "").otherwise(col(col_name)))
    return df.select(columns).dropDuplicates().orderBy(columns)

def validate_table(table):
    try:
        source_table = f"{source_catalog}.{source_db}.{table}"
        source_df = spark.table(source_table)
        source_df = convert_boolean_columns(source_df)
        source_df = to_lowercase_columns(source_df)
        source_columns = set(source_df.columns)
        source_count = source_df.count()
    except Exception:
        return {
            "input_table": table,
            "source_table": "missing",
            "target_table": "missing",
            "source_count": 0,
            "target_count": 0,
            "target_filtered_count": 0,
            "missing_in_source": [],
            "missing_in_target": [],
            "mismatches": [],
            "mismatches_target": [],
            "duplicate_check_in_target": 0,
            "source_minus_target_count": 0,
            "target_minus_source_count": 0,
            "source_sql": "",
            "target_sql": "",
            "post_dedup_count": 0,
            "comment": "source or target load failure"
        }

    prefix = table[0].lower()
    if prefix == 'p':
        target_table_name = f"policycenter_{table}"
    elif prefix == 'b':
        target_table_name = f"billingcenter_{table}"
    elif prefix == 'c':
        target_table_name = f"claimcenter_{table}"
    else:
        target_table_name = table

    try:
        target_table = f"{target_catalog}.{target_db}.{target_table_name}"
        target_df = spark.table(target_table)
        if 'target_df' in locals():
            target_df = convert_boolean_columns(target_df)
            target_df = to_lowercase_columns(target_df)

        if "azure_load_date" in target_df.columns:
            target_df = target_df.drop("azure_load_date")

        has_createtime = "createtime" in target_df.columns
        has_updatetime = "updatetime" in target_df.columns

        if has_createtime:
            target_df = target_df.filter((col("createtime") >= filter_start) & (col("createtime") <= filter_end))
        elif has_updatetime:
            target_df = target_df.filter((col("updatetime") >= filter_start) & (col("updatetime") <= filter_end))

        target_filtered_count = target_df.count()
        target_count = target_df.count()
        target_columns = set(target_df.columns)
    except Exception:
        return {
            "input_table": table,
            "source_table": source_table,
            "target_table": "missing",
            "source_count": source_count,
            "target_count": 0,
            "target_filtered_count": 0,
            "missing_in_source": [],
            "missing_in_target": [],
            "mismatches": [],
            "mismatches_target": [],
            "duplicate_check_in_target": 0,
            "source_minus_target_count": 0,
            "target_minus_source_count": 0,
            "source_sql": f"SELECT * FROM {source_table}",
            "target_sql": "",
            "post_dedup_count": 0,
            "comment": "target table load failure"
        }

    common_columns = list(source_columns & target_columns)
    source_sql = f"SELECT {', '.join(common_columns)} FROM {source_table}"
    target_sql = f"SELECT {', '.join(common_columns)} FROM {target_table}"

    missing_in_source = list(target_columns - source_columns)
    missing_in_target = list(source_columns - target_columns)

    source_comp_df = normalize_dataframe(cast_columns_to_string(source_df, common_columns), common_columns)
    target_comp_df = normalize_dataframe(target_df.select(common_columns), common_columns)

    mismatches = [col_name for col_name in common_columns if source_comp_df.select(col_name).subtract(target_comp_df.select(col_name)).take(1)]
    mismatches_target = [col_name for col_name in common_columns if target_comp_df.select(col_name).subtract(source_comp_df.select(col_name)).take(1)]

    source_minus_target_count = source_comp_df.subtract(target_comp_df).count()
    target_minus_source_count = target_comp_df.subtract(source_comp_df).count()

    duplicate_count = target_comp_df.count() - target_comp_df.dropDuplicates().count() if has_createtime or has_updatetime else 0

    comment_parts = []
    if source_count != target_filtered_count:
        comment_parts.append("count mismatch")
    if missing_in_source:
        comment_parts.append("missing columns in source")
    if missing_in_target:
        comment_parts.append("missing columns in target")
    if mismatches or mismatches_target:
        comment_parts.append("data mismatch")
    if duplicate_count > 0:
        comment_parts.append("duplicates in target")
    comment = " | ".join(comment_parts) if comment_parts else "All checks passed"

    return {
        "input_table": table,
        "source_table": source_table,
        "target_table": target_table,
        "source_count": source_count,
        "target_count": target_count,
        "target_filtered_count": target_filtered_count,
        "missing_in_source": missing_in_source,
        "missing_in_target": missing_in_target,
        "mismatches": mismatches,
        "mismatches_target": mismatches_target,
        "duplicate_check_in_target": duplicate_count,
        "source_minus_target_count": source_minus_target_count,
        "target_minus_source_count": target_minus_source_count,
        "source_sql": source_sql,
        "target_sql": target_sql,
        "post_dedup_count": target_comp_df.dropDuplicates().count(),
        "comment": comment
    }

with ThreadPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(validate_table, table_names))

schema = StructType([
    StructField("input_table", StringType(), True),
    StructField("source_table", StringType(), True),
    StructField("target_table", StringType(), True),
    StructField("source_count", IntegerType(), True),
    StructField("target_count", IntegerType(), True),
    StructField("target_filtered_count", IntegerType(), True),
    StructField("missing_in_source", ArrayType(StringType()), True),
    StructField("missing_in_target", ArrayType(StringType()), True),
    StructField("mismatches", ArrayType(StringType()), True),
    StructField("mismatches_target", ArrayType(StringType()), True),
    StructField("duplicate_check_in_target", IntegerType(), True),
    StructField("source_minus_target_count", IntegerType(), True),
    StructField("target_minus_source_count", IntegerType(), True),
    StructField("source_sql", StringType(), True),
    StructField("target_sql", StringType(), True),
    StructField("post_dedup_count", IntegerType(), True),
    StructField("comment", StringType(), True)
])

result_rdd = spark.sparkContext.parallelize([(
    r["input_table"], r["source_table"], r["target_table"], r["source_count"], r["target_count"],
    r["target_filtered_count"], r["missing_in_source"], r["missing_in_target"], r["mismatches"],
    r["mismatches_target"], r["duplicate_check_in_target"], r["source_minus_target_count"],
    r["target_minus_source_count"], r["source_sql"], r["target_sql"], r["post_dedup_count"],
    r["comment"]
) for r in results])

result_df = spark.createDataFrame(result_rdd, schema)
result_df.write.mode("overwrite").saveAsTable("core_tst_sys9.default.validation_results")
result_df.show(truncate=False)
