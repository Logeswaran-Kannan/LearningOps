# Databricks notebook or script to fetch upstream and downstream tables for input tables
# in catalog `core_tst` and database `curated_edw` using Unity Catalog Lineage API

from pyspark.sql import SparkSession
import pandas as pd
import requests
import json

# Initialize Spark session
spark = SparkSession.builder.appName("LineageFetcher").getOrCreate()

# Input tables
input_tables = [
    "table1",
    "table2"
    # Add more table names as needed
]

# Constants for catalog and database
catalog = "core_tst"
database = "curated_edw"
workspace_url = "https://<your-databricks-workspace>"  # e.g., https://adb-1234567890123456.7.azuredatabricks.net
token = "<your-databricks-pat-token>"

headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json"
}

results = []

def fetch_lineage(fully_qualified_table_name):
    lineage_url = f"{workspace_url}/api/2.1/unity-catalog/lineage"
    payload = {
        "dataset": fully_qualified_table_name,
        "direction": "BOTH",
        "include_table_lineage": True
    }
    response = requests.post(lineage_url, headers=headers, json=payload)
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Failed to fetch lineage for {fully_qualified_table_name}: {response.text}")
        return {}

for table in input_tables:
    full_name = f"{catalog}.{database}.{table}"
    lineage = fetch_lineage(full_name)

    upstreams = lineage.get("upstreams", [])
    downstreams = lineage.get("downstreams", [])

    # Add each upstream as a separate row
    for upstream in upstreams:
        results.append({
            "table": table,
            "direction": "upstream",
            "related_table": upstream.get("dataset", {}).get("name", "")
        })

    # Add each downstream as a separate row
    for downstream in downstreams:
        results.append({
            "table": table,
            "direction": "downstream",
            "related_table": downstream.get("dataset", {}).get("name", "")
        })

# Convert to DataFrame for display only if results exist
if results:
    lineage_df = pd.DataFrame(results)
    display(spark.createDataFrame(lineage_df))
else:
    print("No lineage data retrieved. Check API responses and table names.")
