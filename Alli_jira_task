from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

# Create Spark session
spark = SparkSession.builder.getOrCreate()

# Catalog & schema
catalog_schema = "core_tst.ods_views"

# List of table names (without schema prefix)
table_list = ["orders", "users", "events"]  # replace with your tables
target_date = datetime(2025, 5, 19)  # the date to check

# Result collector
results = []

for table_name in table_list:
    full_table_name = f"{catalog_schema}.{table_name}"
    
    try:
        # Get column names
        cols = [c.name.lower() for c in spark.table(full_table_name).schema.fields]
        
        if "updatetime" in cols:
            # Filter for date range: >= target_date and < target_date + 1 day
            cnt = (
                spark.table(full_table_name)
                .filter((col("updatetime") >= lit(target_date)) & 
                        (col("updatetime") < lit(target_date + timedelta(days=1))))
                .count()
            )
            results.append((table_name, True, cnt, f"updatetime present"))
        else:
            results.append((table_name, False, 0, "updatetime not present"))
    
    except Exception as e:
        # If table doesn't exist or can't be read
        results.append((table_name, False, None, f"Error: {str(e)}"))

# Convert to DataFrame for nice output
result_df = spark.createDataFrame(results, ["table_name", "has_updatetime", "row_count", "comment"])

# Show results
result_df.show(truncate=False)
