from pyspark.sql import SparkSession

# Start Spark
spark = SparkSession.builder.getOrCreate()

# Config
catalog = "core"
database = "ods_views"
prefixes = ("vw_billingcenter", "vw_policycenter", "vw_claimcenter")

# Use catalog and DB
spark.sql(f"USE CATALOG {catalog}")
spark.sql(f"USE {database}")

# Fetch relevant tables
tables_df = spark.sql("SHOW TABLES")
tables = [row.tableName for row in tables_df.collect() if row.tableName.startswith(prefixes)]

# Collect generated SQLs
sql_entries = []

for table in tables:
    try:
        full_table = f"{catalog}.{database}.{table}"
        df = spark.table(full_table)

        # Lowercase all column names and drop AZURE_LOAD_DATE
        columns = [col.lower() for col in df.columns if col.lower() != "azure_load_date"]
        if not columns:
            continue

        struct_expr = "struct(" + ", ".join([f"`{col}`" for col in columns]) + ")"

        sql_text = f"""SELECT '{table}' AS table_name,
       '{catalog}' AS catalog,
       COUNT(*) AS total_count,
       (COUNT(*) - COUNT(DISTINCT {struct_expr})) AS duplicate_count
FROM {full_table}"""

        sql_entries.append((table, catalog, database, sql_text.strip()))
    except Exception as e:
        sql_entries.append((table, catalog, database, f"-- ERROR generating SQL: {str(e)}"))

# Convert to DataFrame
sql_df = spark.createDataFrame(sql_entries, ["table_name", "catalog", "database", "sql_text"])

# Store to Delta table
sql_df.write.mode("overwrite").format("delta").saveAsTable("core.admin_scripts.duplicate_check_sqls")
