🔍 Test Scenario ID: EH-BR-04 – Null/Corrupted Message Handling
🔹 Purpose
This test scenario validates the system's ability to gracefully handle malformed, null, or incomplete messages received from Azure Event Hub during ingestion. The goal is to ensure that such messages are either quarantined, rejected, or logged, and do not cause pipeline failure or contaminate the Bronze dataset.

🔹 Scope and Components Involved
Message Types: All MQS-originating messages (e.g., Quote Request, PreComp, Rating Request)

Edge Cases:

Empty payload ({} or null)

Missing mandatory fields (e.g., quote_id, event_type)

Malformed JSON (e.g., missing braces, truncated strings)

Invalid schema versions or unknown message types

Pipeline Layers:

EventHub → DLT Parse Layer → Bronze Table (Landed Table)

Optional: Quarantine/Error Table or Dead-letter mechanism

Reference Docs:

Technical ingestion specification (defines fallback/quarantine behavior)

DLT expectations or schema enforcement definitions

🔹 Validation Objectives
Ensure messages with corrupt or null payloads are not inserted into the primary Bronze table.

Confirm pipeline logs an error, redirects the record to a quarantine table, or triggers an alert.

Validate that ingestion continues without interruption when encountering bad records.

🧪 Testing Approach
✅ Against Ingestion Logic and Pipeline Design
From your design documentation, messages are streamed into the DLT Landed Table using predefined schema enforcement and transformation logic.

Corrupt messages must be either:

Rejected during parsing, with error logged and pipeline continuing, or

Diverted to an error-handling path (e.g., Bronze_Quarantine)

Test cases should include:

A null JSON payload

A payload missing key fields like event_type or quote_id

A syntactically invalid JSON message

Review Databricks logs for structured error entries and monitor Bronze table for unintended records.

📋 Validation Methods
Inject controlled invalid messages into EventHub.

Confirm whether:

They are excluded from bronze_landed_table

They appear in an error-handling/quarantine table

Error messages are logged in the DLT job monitoring interface

Review retry behavior or job continuation state.

✅ Success Criteria
Corrupted or null records do not appear in the main Bronze layer.

They are either:

Logged, with identifiers and failure reason

Redirected to a dedicated quarantine table (if configured)

Pipeline ingestion process continues without failure or retry loop.

No schema inference errors are triggered.

🧾 Exclusions
Validation of field presence/type in valid records is part of EH-BR-02 – Schema Conformance Check.

Deduplication and idempotency are tested in EH-BR-05/E2E-03.

Latency or performance effects from bad records are handled in OBS scenarios.

🗂 Mapped Data Quality Requirements
DQ Ref	Description
RL DQ03 – Completeness	Ensures system handles missing or incomplete data without silently failing.
RL DQ05 – Validity	Validates data conforms to expected formats and rules.
RL DQ09 – Traceability	Requires bad records to be logged with identifiers for audit/troubleshooting.

🔐 Pre-requisites & Access Required
Ability to send malformed or test messages to EventHub.

Access to:

Bronze table

Quarantine/Error table (if configured)

DLT pipeline logs or job monitoring dashboard

Knowledge of:

Field-level validations defined in schema enforcement or parsing notebook

Retry behavior of DLT in case of failure

