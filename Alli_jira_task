# This script performs a comprehensive comparison between source and target
# tables across different catalogs using Apache Spark, focusing on column normalization,
# boolean pattern handling, and data matching.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lit, lower, regexp_replace
from concurrent.futures import ThreadPoolExecutor, as_completed
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType
import matplotlib.pyplot as plt

spark = SparkSession.builder.appName("CatalogComparison").getOrCreate()

# Configuration
source_catalog = "core_tst_sys9"
source_db = "rr_source"
target_catalog = "core_tst_std001"
target_db = "ods"
table_names = ["cc_matter", "btable2", "ctable3"]
filter_start = '2025-05-12T00:00:01.0025+00:00'
filter_end = '2025-05-13T00:00:01.0025+00:00'

results = []
progress_counter = 0

def normalize_column_names(df):
    return df.select([col(c).alias(c.lower()) for c in df.columns])

def generate_target_table_name(table):
    if table.startswith("p"):
        return f"policycenter_std001_{table}"
    elif table.startswith("b"):
        return f"billingcenter_std001_{table}"
    elif table.startswith("c"):
        return f"claimcenter_std001_{table}"
    return table

def identify_boolean_pattern(df, col_name):
    values = set([r[0] for r in df.select(col_name).distinct().collect()])
    if values.issubset({"t", "f", None}):
        if "t" in values and "f" in values:
            return "tf"
        elif "t" in values and None in values:
            return "t_null"
        elif "f" in values and None in values:
            return "f_null"
        elif "t" in values:
            return "t"
        elif "f" in values:
            return "f"
        elif None in values:
            return "null_only"
    return None

def correct_boolean_cast(df, column, pattern):
    if pattern == "tf":
        return df.withColumn(column, when(lower(col(column)) == "t", lit(1)).when(lower(col(column)) == "f", lit(0)).otherwise(None).cast("int"))
    elif pattern == "t_null":
        return df.withColumn(column, when(lower(col(column)) == "t", lit(1)).otherwise(None).cast("int"))
    elif pattern == "f_null":
        return df.withColumn(column, when(lower(col(column)) == "f", lit(0)).otherwise(None).cast("int"))
    elif pattern in ["t", "f"]:
        return df.withColumn(column, when(lower(col(column)) == "t", lit(1)).when(lower(col(column)) == "f", lit(0)).otherwise(None).cast("int"))
    elif pattern == "null_only":
        return df.withColumn(column, lit(None).cast("int"))
    else:
        return df

def boolean_transform_patterned(df, table, boolean_lookup):
    bool_cols = boolean_lookup.get(table, [])
    patterns = {}
    for c in bool_cols:
        pattern = identify_boolean_pattern(df, c)
        if pattern:
            patterns[c] = pattern
    return df, bool_cols, patterns

def cast_decimal_to_double(df):
    decimal_cols = [f.name for f in df.schema.fields if f.dataType.simpleString().startswith("decimal")]
    for c in decimal_cols:
        df = df.withColumn(c, col(c).cast("double"))
    return df, decimal_cols

def remove_column(df, colname):
    return df.drop(colname) if colname in df.columns else df

def build_sql_exprs(columns, bool_cols, decimal_cols, bool_patterns, source=True):
    exprs = []
    for c in columns:
        if source and c in bool_cols:
            pattern = bool_patterns.get(c)
            if pattern in ["tf", "t_null", "f_null", "t", "f"]:
                exprs.append(f"CAST(CASE LOWER({c}) WHEN 't' THEN 1 WHEN 'f' THEN 0 ELSE NULL END AS STRING) AS {c}")
            else:
                exprs.append(f"CAST({c} AS STRING) AS {c}")
        elif c in decimal_cols:
            exprs.append(f"CAST({c} AS DOUBLE) AS {c}")
        else:
            exprs.append(f"CAST({c} AS STRING) AS {c}")
    return exprs

import pandas as pd
boolean_lookup = pd.read_csv("/dbfs/path/to/Booleancol.csv").groupby("tablename")["booleancolumn"].apply(list).to_dict()
filtercol_lookup = pd.read_csv("/dbfs/path/to/filtercol.csv").set_index("tablename")["filtercol"].to_dict()

def compare_table(table):
    global progress_counter
    source_table = f"{source_catalog}.{source_db}.{table}"
    target_table = generate_target_table_name(table)
    target_table_full = f"{target_catalog}.{target_db}.{target_table}"

    try:
        source_df = normalize_column_names(spark.read.table(source_table))
        target_df = normalize_column_names(spark.read.table(target_table_full))

        for c in source_df.columns:
            if source_df.schema[c].dataType.simpleString() == "string":
                source_df = source_df.withColumn(c, regexp_replace(col(c), "ï¿½", "-"))
                source_df = source_df.withColumn(c, regexp_replace(col(c), '""', '"'))
                source_df = source_df.withColumn(c, regexp_replace(col(c), '^"(.*)"$', '$1'))

        source_df, bool_cols, bool_patterns = boolean_transform_patterned(source_df, table, boolean_lookup)
        target_df = remove_column(target_df, "azure_load_date")
        target_df, decimal_cols = cast_decimal_to_double(target_df)

        for c in decimal_cols:
            if c in source_df.columns:
                source_df = source_df.withColumn(c, col(c).cast("double"))

        filter_col = filtercol_lookup.get(table)
        full_target_count = target_df.count()
        dedup_target_df = target_df.dropDuplicates() if not update_time_present else target_df
        post_dedup_count = dedup_target_df.count()

        if full_target_count > 3000 and filter_col and filter_col in source_df.columns and filter_col in target_df.columns:
            filtered_source_df = source_df.filter((col(filter_col) >= filter_start) & (col(filter_col) <= filter_end))
            filtered_target_df = dedup_target_df.filter((col(filter_col) >= filter_start) & (col(filter_col) <= filter_end))
        else:
            filtered_source_df = source_df
            filtered_target_df = dedup_target_df

        targetfiltercount = filtered_target_df.count()
        source_count = filtered_source_df.count()
        common_cols = list(set(filtered_source_df.columns) & set(filtered_target_df.columns))

        for c in common_cols:
            if c in bool_cols:
                filtered_source_df = correct_boolean_cast(filtered_source_df, c, bool_patterns.get(c))
            elif c in decimal_cols:
                filtered_source_df = filtered_source_df.withColumn(c, col(c).cast("double"))
            else:
                filtered_source_df = filtered_source_df.withColumn(c, col(c).cast("string"))

        for c in common_cols:
            if c in decimal_cols:
                filtered_target_df = filtered_target_df.withColumn(c, col(c).cast("double"))
            else:
                filtered_target_df = filtered_target_df.withColumn(c, col(c).cast("string"))

        source_only = filtered_source_df.select(common_cols).exceptAll(filtered_target_df.select(common_cols)).count()
        target_only = filtered_target_df.select(common_cols).exceptAll(filtered_source_df.select(common_cols)).count()

        missing_in_source = list(set(filtered_target_df.columns) - set(filtered_source_df.columns))
        missing_in_target = list(set(filtered_source_df.columns) - set(filtered_target_df.columns))
        duplicate_count = full_target_count - post_dedup_count

        comment_parts = []
        if source_count != post_dedup_count:
            comment_parts.append("count mismatch")
        if missing_in_source:
            comment_parts.append("missing columns in source")
        if missing_in_target:
            comment_parts.append("missing columns in target")
        if source_only + target_only > 0:
            comment_parts.append("data mismatch")
        if duplicate_count > 0:
            comment_parts.append("duplicates in target")

        comment = " | ".join(comment_parts) if comment_parts else "All checks passed"

        source_sql = f"SELECT {', '.join(build_sql_exprs(common_cols, bool_cols, decimal_cols, bool_patterns, True))} FROM {source_table}"
        target_sql = f"SELECT {', '.join(build_sql_exprs(common_cols, bool_cols, decimal_cols, bool_patterns, False))} FROM {target_table_full}"

        if full_target_count > 3000 and filter_col:
            source_sql += f" WHERE {filter_col} BETWEEN '{filter_start}' AND '{filter_end}'"
            target_sql += f" WHERE {filter_col} BETWEEN '{filter_start}' AND '{filter_end}'"

        casted_sql = f"%sql \n{source_sql}\n except \n{target_sql}"

        progress_counter += 1
        print(f"Progress: {progress_counter}/{len(table_names)} tables compared")

        return (table, source_table, target_table_full, source_count, full_target_count, targetfiltercount,
                post_dedup_count, duplicate_count, bool_cols, decimal_cols,
                source_only, target_only, missing_in_source, missing_in_target,
                source_only, target_only, comment, casted_sql)

    except Exception as e:
        progress_counter += 1
        print(f"Progress: {progress_counter}/{len(table_names)} tables compared")
        return (table, source_table, target_table_full, 0, 0, 0, 0, 0, [], [], 0, 0, [], [], 0, 0, str(e), "")

# Run comparisons concurrently and track progress
with ThreadPoolExecutor() as executor:
    result_futures = [executor.submit(compare_table, tbl) for tbl in table_names]
    results = [f.result() for f in as_completed(result_futures)]

# Define output schema
schema = StructType([...])

# Save and visualize results
final_df = spark.createDataFrame(results, schema=schema)
final_df.write.mode("overwrite").saveAsTable("core_tst_sys9.default.validation_results")

# Visualization: enhanced pie chart with category mapping
def categorize(comment):
    if comment.startswith("[TABLE_OR_VIEW_NOT_FOUND"):
        return "Table Not Found"
    return comment

categorized_df = final_df.withColumn("category", when(col("comment").startswith("[TABLE_OR_VIEW_NOT_FOUND"), "Table Not Found").otherwise(col("comment")))
category_summary = categorized_df.groupBy("category").count().collect()
labels = [row["category"] for row in category_summary]
sizes = [row["count"] for row in category_summary]

from collections import defaultdict

result_summary = defaultdict(int)
for row in final_df.select("comment").collect():
    if row["comment"].startswith("[TABLE_OR_VIEW_NOT_FOUND"):
        result_summary["Table Not Found"] += 1
    else:
        result_summary[row["comment"]] += 1

labels = list(result_summary.keys())
sizes = list(result_summary.values())

plt.figure(figsize=(6, 6))
plt.pie(sizes, labels=labels, autopct="%1.1f%%", startangle=140)
plt.title("Validation Results Summary")
plt.axis("equal")
plt.show()
