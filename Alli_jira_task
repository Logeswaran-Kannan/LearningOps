from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.getOrCreate()

# Define catalog and database
catalog_name = "core"
database_name = "ods"

# Define table name prefixes
prefixes = ("policy", "billing", "claim")

# Get all tables in the catalog and database
tables_df = spark.sql(f"SHOW TABLES IN {catalog_name}.{database_name}")

# Filter tables that start with specified prefixes
filtered_tables = tables_df.filter(
    tables_df.tableName.startswith(prefixes[0]) |
    tables_df.tableName.startswith(prefixes[1]) |
    tables_df.tableName.startswith(prefixes[2])
)

# Generate fully qualified table names
qualified_tables = [
    f"{catalog_name}.{database_name}.{row.tableName}"
    for row in filtered_tables.collect()
]

# Count records for each filtered table
results = []
for table in qualified_tables:
    try:
        count = spark.read.table(table).count()
        results.append((table, count))
    except Exception as e:
        results.append((table, f"Error: {e}"))

# Convert result to DataFrame and show
results_df = spark.createDataFrame(results, ["TableName", "RecordCount"])
results_df.display()
