from pyspark.sql import SparkSession, functions as F, Window

# Initialize Spark
spark = SparkSession.builder.getOrCreate()

# Step 1: Read the Delta table
df = spark.table("core_preprod.default.table_counts")

# Step 2: Filter last 2 runs based on run_timestamp
latest_runs = (
    df.select("run_timestamp")
    .distinct()
    .orderBy(F.col("run_timestamp").desc())
    .limit(2)
    .collect()
)

# Ensure at least 2 runs exist
if len(latest_runs) < 2:
    raise Exception("Not enough runs to compare (need at least two run_timestamp values).")

latest_ts = latest_runs[0][0]
previous_ts = latest_runs[1][0]

print(f"ðŸ“Š Comparing run_timestamp: {previous_ts} âŸ¶ {latest_ts}")

# Step 3: Filter and pivot
df_filtered = df.filter(F.col("run_timestamp").isin([latest_ts, previous_ts]))

df_pivot = df_filtered.groupBy("table_name").pivot("run_timestamp").agg(F.first("record_count"))

# Step 4: Rename pivoted columns for clarity
df_pivot = df_pivot.withColumnRenamed(str(previous_ts), "previous_count") \
                   .withColumnRenamed(str(latest_ts), "current_count")

# Step 5: Calculate difference
df_result = df_pivot.withColumn("count_diff", F.col("current_count") - F.col("previous_count"))

# Show and export
import ace_tools as tools; tools.display_dataframe_to_user(name="Record Count Comparison", dataframe=df_result)
