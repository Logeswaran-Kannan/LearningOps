from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lower
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType

# Initialize Spark session
spark = SparkSession.builder.appName("BooleanColumnConversion").getOrCreate()

# Catalog and database details
source_catalog = "core_tst_sys9"
source_db = "rr_source"
target_catalog = "core_tst_std001"
target_db = "ods"

# Table names to process
table_names = ["ptable1", "btable2", "ctable3"]  # Add your table names here

# Date range for filtering
filter_start = '2025-05-12T00:00:01.0025+00:00'
filter_end = '2025-05-13T00:00:01.0025+00:00'

# Output result list
results = []

# Function to convert boolean-like columns
def convert_boolean_columns(df):
    for column in df.columns:
        unique_vals = df.select(column).distinct().rdd.flatMap(lambda x: x).collect()
        if set(unique_vals).issubset({"t", "f"}):
            df = df.withColumn(column, when(col(column) == "t", 1).otherwise(0))
    return df

# Function to convert column names to lowercase
def to_lowercase_columns(df):
    for col_name in df.columns:
        df = df.withColumnRenamed(col_name, col_name.lower())
    return df

# Loop through each table and prepare them for comparison
for table in table_names:
    source_table = f"{source_catalog}.{source_db}.{table}"
    source_df = spark.table(source_table)
    source_df = convert_boolean_columns(source_df)
    source_df = to_lowercase_columns(source_df)
    source_count = source_df.count()

    # Determine target table name based on prefix
    prefix = table[0].lower()
    if prefix == 'p':
        target_table_name = f"policycenter_{table}"
    elif prefix == 'b':
        target_table_name = f"billingcenter_{table}"
    elif prefix == 'c':
        target_table_name = f"claimcenter_{table}"
    else:
        target_table_name = table  # Default fallback

    try:
        target_table = f"{target_catalog}.{target_db}.{target_table_name}"
        target_df = spark.table(target_table).drop("azure_load_date")
        target_df = convert_boolean_columns(target_df)
        target_df = to_lowercase_columns(target_df)
        target_count = target_df.count()
    except:
        target_df = None
        target_count = 0

    if source_count == target_count and target_df is not None:
        # Filter target if count > 5000
        if target_count > 5000:
            target_df = target_df.filter((col("createtime") >= filter_start) & (col("createtime") <= filter_end))

        # Get columns for comparison
        source_columns = set(source_df.columns)
        target_columns = set(target_df.columns)
        missing_in_source = list(target_columns - source_columns)
        missing_in_target = list(source_columns - target_columns)

        common_columns = list(source_columns & target_columns)
        mismatches = []
        mismatches_target = []

        for col_name in common_columns:
            source_vals = source_df.select(col_name).distinct()
            target_vals = target_df.select(col_name).distinct()
            if source_vals.subtract(target_vals).count() > 0:
                mismatches.append(col_name)
            if target_vals.subtract(source_vals).count() > 0:
                mismatches_target.append(col_name)

        # Check for duplicates in target
        duplicate_count = target_df.count() - target_df.dropDuplicates().count()
    else:
        missing_in_source = []
        missing_in_target = []
        mismatches = []
        mismatches_target = []
        duplicate_count = 0

    results.append({
        "input_table": table,
        "source_table": source_table if source_df else "missing",
        "target_table": target_table if target_df else "missing",
        "source_count": source_count,
        "target_count": target_count,
        "missing_in_source": missing_in_source,
        "missing_in_target": missing_in_target,
        "mismatches": mismatches,
        "mismatches_target": mismatches_target,
        "duplicate_check_in_target": duplicate_count
    })

# Define schema for result rows
schema = StructType([
    StructField("input_table", StringType(), True),
    StructField("source_table", StringType(), True),
    StructField("target_table", StringType(), True),
    StructField("source_count", IntegerType(), True),
    StructField("target_count", IntegerType(), True),
    StructField("missing_in_source", ArrayType(StringType()), True),
    StructField("missing_in_target", ArrayType(StringType()), True),
    StructField("mismatches", ArrayType(StringType()), True),
    StructField("mismatches_target", ArrayType(StringType()), True),
    StructField("duplicate_check_in_target", IntegerType(), True)
])

# Convert to DataFrame using defined schema
result_rdd = spark.sparkContext.parallelize([(
    r["input_table"],
    r["source_table"],
    r["target_table"],
    r["source_count"],
    r["target_count"],
    r["missing_in_source"],
    r["missing_in_target"],
    r["mismatches"],
    r["mismatches_target"],
    r["duplicate_check_in_target"]
) for r in results])

result_df = spark.createDataFrame(result_rdd, schema)
result_df.show(truncate=False)
