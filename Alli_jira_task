🔍 Test Scenario ID: EH-BR-07 – EventHub Offset Continuity
🔹 Purpose
This test scenario verifies that all messages published to Azure EventHub are read in correct sequence without gaps or duplications, and that offsets are processed in order by the DLT ingestion pipeline. This ensures full ingestion coverage, protects against data loss, and maintains message integrity through recovery and restarts.

🔹 Scope and Components Involved
Primary Focus:

Continuity of EventHub partition offsets

Alignment with DLT checkpointing and job restart logic

Relevant Components:

Azure EventHub (offsets, partitions)

Databricks Structured Streaming via DLT

Bronze table ingestion flow

Key Metadata Fields:

eventhub_offset

eventhub_partition_id

_ingest_time

Supporting Artifacts:

EventHub monitoring in Azure Portal

DLT checkpoint storage in /mnt/.../.checkpoints/...

🔹 Validation Objectives
Validate that no offsets are skipped during continuous streaming ingestion.

Confirm that checkpointing works correctly on failure and restart (no message reprocessing or data loss).

Ensure monotonic offset progression within each EventHub partition.

Detect potential issues with:

Out-of-sequence message reads

Stale checkpoint application

Faulty retries or duplicate insertions

🧪 Testing Approach
✅ Streaming Offset and Checkpoint Behavior
Run a controlled stream ingestion job and simulate the following conditions:

Regular ingestion without failure

Simulated job failure + restart

EventHub partition-based replay

Track the continuity of:

eventhub_offset within a partition

Number of records vs. expected message count

Cross-reference DLT job logs and checkpoint folders to validate recovery behavior.

📋 Validation Methods
Query Bronze records by eventhub_partition_id ordered by eventhub_offset and ensure sequential continuity.

Check for offset gaps or overlaps:

Gaps → message loss

Overlaps → possible duplicate ingestion

Review DLT checkpoint metadata after each run and after job restart.

Use EventHub diagnostics or Azure Metrics to correlate producer → consumer offset progression.

✅ Success Criteria
All offsets from EventHub are processed exactly once and in sequential order.

No messages are missed, duplicated, or delayed due to offset issues.

In case of pipeline failure or restart:

Processing resumes from last confirmed checkpoint

Duplicate records are not inserted

Ingestion flow maintains consistency across partitions.

🧾 Exclusions
Deduplication of reprocessed messages is validated in EH-BR-05.

End-to-end record count matching is handled under E2E-01.

Latency impact due to retries or restarts is measured in E2E-02.

🗂 Mapped Data Quality Requirements
DQ Ref	Description
RL DQ01 – Accuracy	Ensures complete and correct retrieval of message stream from EventHub.
RL DQ03 – Completeness	No records are skipped due to offset jumps.
RL DQ09 – Traceability	Offset metadata allows full traceability of message lifecycle.

🔐 Pre-requisites & Access Required
Access to:

EventHub namespace and partition monitoring via Azure Portal

DLT pipeline definition and checkpoint folder

Bronze table fields: eventhub_offset, partition_id, _ingest_time

Permissions to simulate job pause, restart, and controlled reprocessing

