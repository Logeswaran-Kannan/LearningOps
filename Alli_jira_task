# Import required libraries
from azure.cosmos import CosmosClient, PartitionKey
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import logging

# Set up logger
logger = logging.getLogger("cosmos_logger")
logging.basicConfig(level=logging.INFO)

# Widgets (used in Databricks to pass parameters)
dbutils.widgets.text("cosmos_account_key", "")
dbutils.widgets.text("cosmos_end_point", "")
dbutils.widgets.text("cosmos_container_name", "vehicle-area-factors")
dbutils.widgets.text("cosmos_db_name", "vehicle-area-cdb")

# Retrieve widget values
cosmos_account_key = dbutils.widgets.get("cosmos_account_key")
cosmos_end_point = dbutils.widgets.get("cosmos_end_point")
cosmos_container_name = dbutils.widgets.get("cosmos_container_name")
cosmos_db_name = dbutils.widgets.get("cosmos_db_name")

# Set Spark configuration settings for Cosmos DB
config = {
    "spark.cosmos.accountEndpoint": cosmos_end_point,
    "spark.cosmos.accountKey": cosmos_account_key,
    "spark.cosmos.container": cosmos_container_name,
    "spark.cosmos.database": cosmos_db_name,
    "spark.cosmos.read.inferSchema.enabled": "true"
}

# Configure Catalog API
spark.conf.set("spark.sql.catalog.cosmosCatalog", "com.azure.cosmos.spark.CosmosCatalog")
spark.conf.set("spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint", config["spark.cosmos.accountEndpoint"])
spark.conf.set("spark.sql.catalog.cosmosCatalog.spark.cosmos.accountKey", config["spark.cosmos.accountKey"])
logger.info("Cosmos configuration settings loaded")

# Define a method to read data from Cosmos DB
def read_data_from_cosmos():
    try:
        cosmos_df = spark.read.format("cosmos.oltp").options(**config).load()
        display(cosmos_df)
        return cosmos_df
    except Exception as e:
        logger.error(f"Error reading data from Cosmos DB: {e}")
        return None

# Read and display data as table
read_data_from_cosmos()
