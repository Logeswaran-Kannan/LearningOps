from pyspark.sql import SparkSession
from datetime import datetime

# Initialize Spark session
spark = SparkSession.builder.getOrCreate()

# Catalog and databases to check
catalog_name = "core_tst"
databases_to_check = [
    "ods",
    "ods_views",
    "ods_stg_copy",
    "curated_edw",
    "curated_edw_insert_views",
    "curated_idw"
]

# Output list
results = []

# Current timestamp
timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

for db in databases_to_check:
    print(f"Checking database: {db}")
    try:
        tables = spark.catalog.listTables(f"{catalog_name}.{db}")
        for table in tables:
            table_name = table.name
            full_table_name = f"{catalog_name}.{db}.{table_name}"
            try:
                count = spark.table(full_table_name).count()
                print(f"  Table: {table_name} - Count: {count}")
                results.append((db, table_name, count, timestamp))
            except Exception as e:
                print(f"  Failed to count {full_table_name}: {e}")
                results.append((db, table_name, -1, timestamp))  # -1 indicates error
    except Exception as e:
        print(f"Failed to list tables in database {db}: {e}")

# Convert to DataFrame
results_df = spark.createDataFrame(results, ["database", "table_name", "row_count", "count_time"])

# Define output table location
output_table = "core_tst_sys9.default.release_validation_counts"

# Write the results to the output table (overwrite or append as needed)
results_df.write.mode("append").saveAsTable(output_table)

print(f"Counts stored successfully in {output_table}")
