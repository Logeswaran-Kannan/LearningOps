from pyspark.sql.functions import col
from pyspark.sql.utils import AnalysisException

# Input
input_table = "table_xyz"  # Table to search for
schemas_to_search = ["schema1", "schema2", "schema3"]  # List of schemas to scan
catalog = "your_catalog"  # Set your catalog name here

# Result holder
usage_results = []

for schema in schemas_to_search:
    try:
        # Get all tables/views in schema
        objects = spark.sql(f"SHOW TABLES IN {catalog}.{schema}").collect()

        for obj in objects:
            table_name = obj.tableName
            full_table_name = f"{catalog}.{schema}.{table_name}"

            try:
                # Get the DDL for each table/view
                ddl_df = spark.sql(f"SHOW CREATE TABLE {full_table_name}")
                ddl_text = ddl_df.collect()[0][0]

                # Check if input_table name appears in DDL
                if input_table.lower() in ddl_text.lower():
                    usage_results.append((full_table_name, "Referenced"))
            except AnalysisException as e:
                # Skip if the object doesn't support SHOW CREATE TABLE
                continue

    except AnalysisException as e:
        print(f"Could not access schema {schema}: {e}")

# Show results
if usage_results:
    result_df = spark.createDataFrame(usage_results, ["Object", "Status"])
    display(result_df)
else:
    print(f"No downstream usage found for table '{input_table}' in specified schemas.")
