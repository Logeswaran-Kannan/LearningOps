from pyspark.sql import functions as F
from pyspark.sql.utils import AnalysisException

# Configuration
catalog_name = "core"
db_name = "ods"
target_baseline = "core_preprod.default.ddl_benchmark_baseline"
target_diff = "core_preprod.default.ddl_benchmark_diff"

# Table name patterns
prefixes = ["policy", "billing", "claim"]

# Step 1: Pull current DDL from information_schema
info_schema = spark.read.table(f"{catalog_name}.information_schema.columns")

current_ddl = (
    info_schema
    .filter(
        (F.col("table_schema") == db_name) &
        (F.lower(F.col("table_name")).rlike(f"^({'|'.join(prefixes)})"))
    )
    .select(
        F.lit(catalog_name).alias("catalog_name"),
        F.col("table_schema").alias("db_name"),
        F.col("table_name"),
        F.col("column_name"),
        F.col("data_type"),
        F.col("numeric_precision"),
        F.col("numeric_scale"),
        F.current_timestamp().alias("ddl_pull_ts")
    )
    .cache()
)

current_ddl.count()  # Force execution for performance monitoring

# Step 2: Save baseline on first run
try:
    baseline = spark.read.table(target_baseline)
    is_first_run = False
except AnalysisException:
    current_ddl.write.mode("overwrite").saveAsTable(target_baseline)
    print("✅ First run: baseline created.")
    is_first_run = True

# Step 3: Compare with baseline and save diff
if not is_first_run:
    join_keys = ["catalog_name", "db_name", "table_name", "column_name"]

    joined = current_ddl.alias("current").join(
        baseline.alias("base"),
        on=join_keys,
        how="outer"
    )

    diff = joined.select(
        F.coalesce(F.col("current.catalog_name"), F.col("base.catalog_name")).alias("catalog_name"),
        F.coalesce(F.col("current.db_name"), F.col("base.db_name")).alias("db_name"),
        F.coalesce(F.col("current.table_name"), F.col("base.table_name")).alias("table_name"),
        F.coalesce(F.col("current.column_name"), F.col("base.column_name")).alias("column_name"),
        F.col("base.data_type").alias("old_data_type"),
        F.col("current.data_type").alias("new_data_type"),
        F.col("base.numeric_precision").alias("old_precision"),
        F.col("current.numeric_precision").alias("new_precision"),
        F.col("base.numeric_scale").alias("old_scale"),
        F.col("current.numeric_scale").alias("new_scale"),
        F.when(F.col("base.column_name").isNull(), "added")
         .when(F.col("current.column_name").isNull(), "removed")
         .otherwise("modified").alias("change_type"),
        F.current_timestamp().alias("change_ts")
    )

    # Only capture rows with actual schema differences
    filtered_diff = diff.filter(
        (F.col("change_type") != "modified") |
        (F.col("old_data_type") != F.col("new_data_type")) |
        (F.col("old_precision") != F.col("new_precision")) |
        (F.col("old_scale") != F.col("new_scale"))
    )

    filtered_diff.write.mode("append").saveAsTable(target_diff)
    print("✅ Changes written to diff table.")
