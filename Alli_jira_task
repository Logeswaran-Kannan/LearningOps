from pyspark.sql import SparkSession
from delta.tables import DeltaTable
import concurrent.futures
import threading

# Initialize Spark session
spark = SparkSession.builder.getOrCreate()

# Config
catalog = "core"
database = "ods"
target_output_table = "core_preprod.default.delta_table_counts"
prefixes = ("policy", "billing", "claim")

# Lock for thread-safe progress updates
progress_lock = threading.Lock()
progress_counter = {"completed": 0, "total": 0}

# Step 1: Get filtered table list
tables_df = spark.sql(f"SHOW TABLES IN {catalog}.{database}")
filtered_tables = tables_df.filter(
    tables_df.tableName.startswith(prefixes[0]) |
    tables_df.tableName.startswith(prefixes[1]) |
    tables_df.tableName.startswith(prefixes[2])
)

table_names = [f"{catalog}.{database}.{row.tableName}" for row in filtered_tables.collect()]
progress_counter["total"] = len(table_names)

# Step 2: Function to get record count
def get_record_count(full_table_name):
    try:
        # Try metadata-based count
        details = spark.sql(f"DESCRIBE DETAIL {full_table_name}").collect()[0]
        count = details.get("numRecords", None)

        # Fallback to full scan count if metadata is missing or None
        if count is None or count == 0:
            count = spark.read.table(full_table_name).count()

    except Exception as e:
        count = f"Error: {str(e)}"

    # Update progress
    with progress_lock:
        progress_counter["completed"] += 1
        print(f"[{progress_counter['completed']}/{progress_counter['total']}] Counted: {full_table_name}")

    return (full_table_name, count)

# Step 3: Parallel processing with ThreadPoolExecutor
results = []
with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(get_record_count, table) for table in table_names]
    for future in concurrent.futures.as_completed(futures):
        results.append(future.result())

# Step 4: Save results to Delta table
result_df = spark.createDataFrame(results, ["TableName", "RecordCount"])
result_df.write.mode("overwrite").format("delta").saveAsTable(target_output_table)

# Show final output
print("\nâœ… All table counts completed.")
result_df.display()
