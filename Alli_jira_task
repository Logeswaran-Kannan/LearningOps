from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, when
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

spark = SparkSession.builder.appName("SchemaValidation").getOrCreate()

# Configuration
table_names = ["ptable1", "btable2", "ctable3"]
source_db = "core_tst_sys9.rr_source"
target_db = "core_tst_std001.ods"
date_filter_start = "2025-05-12T00:00:01.0025+00:00"
date_filter_end = "2025-05-13T00:00:01.0025+00:00"

# Prefix logic
def get_target_table_name(table):
    if table.startswith("p"):
        return f"policycenter_std001_{table}"
    elif table.startswith("b"):
        return f"billingcenter_std001_{table}"
    elif table.startswith("c"):
        return f"claimcenter_std001_{table}"
    return table

def to_lowercase_columns(df):
    return df.select([col(c).alias(c.lower()) for c in df.columns])

def convert_boolean_columns(df):
    converted = []
    for c in df.columns:
        distinct_vals = [r[c] for r in df.select(c).distinct().collect() if r[c] is not None]
        if set(map(str.lower, map(str, distinct_vals))) <= {'t', 'f'}:
            df = df.withColumn(c, when(lower(col(c)) == 't', 1).otherwise(0))
            converted.append(c)
    return df, converted

def normalize_target_decimal_to_double(df):
    decimal_columns = [f.name for f in df.schema.fields if f.dataType.simpleString().startswith("decimal")]
    for c in decimal_columns:
        df = df.withColumn(c, col(c).cast("double"))
    return df, decimal_columns

def cast_columns_to_common_type(source_df, target_df, columns):
    for c in columns:
        src_type = source_df.schema[c].dataType
        tgt_type = target_df.schema[c].dataType
        common_type = str(tgt_type) if str(src_type) != str(tgt_type) else str(src_type)
        source_df = source_df.withColumn(c, col(c).cast(common_type))
        target_df = target_df.withColumn(c, col(c).cast(common_type))
    return source_df, target_df

def compare_dataframes(src, tgt, columns):
    src_only = src.select(columns).subtract(tgt.select(columns))
    tgt_only = tgt.select(columns).subtract(src.select(columns))
    mismatches = {}
    for c in columns:
        diff = src.select(c).subtract(tgt.select(c))
        if diff.count() > 0:
            mismatches[c] = "source to target mismatch"
        diff = tgt.select(c).subtract(src.select(c))
        if diff.count() > 0:
            mismatches[c] = mismatches.get(c, "") + ", target to source mismatch"
    return src_only.count(), tgt_only.count(), mismatches

def generate_casted_comparison_sql(table, columns, boolean_columns, decimal_columns):
    def cast_expr(c):
        if c in boolean_columns:
            return f"CASE WHEN LOWER({c}) = 't' THEN 1 ELSE 0 END AS {c}"
        elif c in decimal_columns:
            return f"CAST({c} AS DOUBLE) AS {c}"
        else:
            return f"CAST({c} AS STRING) AS {c}"

    casted = [cast_expr(c) for c in columns]
    source_sql = f"SELECT {', '.join(casted)} FROM {source_db}.{table}"
    target_sql = f"SELECT {', '.join(casted)} FROM {target_db}.{get_target_table_name(table)}"
    return source_sql + " ||||| " + target_sql

def validate_table(table):
    result = {"table": table, "comment": ""}
    try:
        src_df = spark.table(f"{source_db}.{table}")
        tgt_table = get_target_table_name(table)
        tgt_df = spark.table(f"{target_db}.{tgt_table}")

        src_df = to_lowercase_columns(src_df)
        tgt_df = to_lowercase_columns(tgt_df)

        src_df, boolean_columns = convert_boolean_columns(src_df)
        tgt_df, _ = convert_boolean_columns(tgt_df)

        tgt_df, decimal_columns = normalize_target_decimal_to_double(tgt_df)

        common_cols = list(set(src_df.columns).intersection(set(tgt_df.columns)))
        missing_src = list(set(tgt_df.columns) - set(src_df.columns))
        missing_tgt = list(set(src_df.columns) - set(tgt_df.columns))

        src_df, tgt_df = cast_columns_to_common_type(src_df, tgt_df, common_cols)

        if 'createtime' in tgt_df.columns and tgt_df.count() > 5000:
            tgt_df = tgt_df.filter((col('createtime') >= date_filter_start) & (col('createtime') <= date_filter_end))
            if 'createtime' in src_df.columns:
                src_df = src_df.filter((col('createtime') >= date_filter_start) & (col('createtime') <= date_filter_end))
        else:
            src_df = src_df.dropDuplicates([c for c in src_df.columns if c != 'azure_load_date'])
            tgt_df = tgt_df.dropDuplicates([c for c in tgt_df.columns if c != 'azure_load_date'])

        src_only_count, tgt_only_count, mismatches = compare_dataframes(src_df, tgt_df, common_cols)

        result.update({
            "converted_boolean_columns": boolean_columns,
            "decimal_columns": decimal_columns,
            "source_only_count": src_only_count,
            "target_only_count": tgt_only_count,
            "missing_in_source": missing_src,
            "missing_in_target": missing_tgt,
            "mismatches": list(mismatches.keys()),
            "casted_comparison_sql": generate_casted_comparison_sql(table, common_cols, boolean_columns, decimal_columns)
        })
    except Exception as e:
        result.update({
            "source_table": "missing",
            "target_table": "missing",
            "comment": str(e)
        })
    return result

# Run validations in parallel
with ThreadPoolExecutor() as executor:
    results = list(executor.map(validate_table, table_names))

# Output as Spark DataFrame
results_df = spark.createDataFrame(results)
results_df.write.mode("overwrite").saveAsTable("core_tst_std001.validation_results")
