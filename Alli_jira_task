


decimal_columns = [col_name for col_name, dtype in spark.table(target_table).dtypes if dtype.startswith("decimal")]




casted_comparison_sql = generate_casted_comparison_sql(
    common_columns,
    source_table,
    target_table,
    converted_boolean_columns,
    decimal_columns,
    has_createtime,
    source_count,
    target_count
)

def generate_casted_comparison_sql(common_columns, source_table, target_table, converted_boolean_columns, decimal_columns, has_createtime, source_count, target_count):
    cast_exprs = []
    for c in common_columns:
        if c in converted_boolean_columns:
            cast_exprs.append(f"CAST(CASE LOWER({c}) WHEN 't' THEN '1' WHEN 'f' THEN '0' ELSE {c} END AS STRING) AS {c}")
        elif c in decimal_columns:
            cast_exprs.append(f"CAST(CAST({c} AS DOUBLE) AS STRING) AS {c}")
        else:
            cast_exprs.append(f"CAST({c} AS STRING) AS {c}")

    casted_source_sql = f"SELECT {', '.join(cast_exprs)} FROM {source_table}"
    casted_target_sql = f"SELECT {', '.join(cast_exprs)} FROM {target_table}"
    if has_createtime and source_count > 5000:
        casted_source_sql += f" WHERE createtime >= '{filter_start}' AND createtime <= '{filter_end}'"
    if has_createtime and target_count > 5000:
        casted_target_sql += f" WHERE createtime >= '{filter_start}' AND createtime <= '{filter_end}'"
    return casted_source_sql + " ||||| " + casted_target_sql
