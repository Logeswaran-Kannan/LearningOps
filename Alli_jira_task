from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("DDLComparison").getOrCreate()

# Define input parameters
source_catalog = "core_tst_sys9"
source_db = "rr_source"
target_catalog = "core_tst_sys9"
target_db = "ods"
input_tables = ["ptable1", "btable2", "ctable3"]

# Function to resolve target table names based on prefix
def resolve_target_table_name(table_name):
    prefix = table_name[0].lower()
    if prefix == 'p':
        return f"policycentre_{table_name}"
    elif prefix == 'b':
        return f"billingcentre_{table_name}"
    elif prefix == 'c':
        return f"claimcentre_{table_name}"
    return table_name

# Function to fetch table schema
def get_table_columns(catalog, db, table):
    try:
        df = spark.table(f"{catalog}.{db}.{table}")
        return set(df.columns)
    except Exception:
        return None

# Store results
results = []

for table in input_tables:
    source_columns = get_table_columns(source_catalog, source_db, table)
    target_table = resolve_target_table_name(table)
    target_columns = get_table_columns(target_catalog, target_db, target_table)

    table_status = "Present" if source_columns and target_columns else "Missing"
    columns_status = []

    if source_columns and target_columns:
        for col in source_columns:
            if col in target_columns:
                columns_status.append((col, "Present"))
            else:
                columns_status.append((col, "Missing"))

    results.append({
        "Table": table,
        "Target Table": target_table,
        "Table Status": table_status,
        "Columns": columns_status
    })

# Display results
for result in results:
    print(f"Table: {result['Table']} => Target Table: {result['Target Table']}")
    print(f"  Table Status: {result['Table Status']}")
    for col, status in result['Columns']:
        print(f"    Column: {col} - {status}")
    print("\n")
