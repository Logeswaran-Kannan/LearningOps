from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType
from datetime import datetime
import uuid

# Initialize Spark session
spark = SparkSession.builder.appName("TableCountCheck").getOrCreate()

# Constants
DATABASE_NAME = "curated_edw"
OUTPUT_TABLE = "core_tst_sys9.default"

# Input: List of tables to check
table_list = [
    "table1",
    "table2",
    "table3"
]

# Generate unique run_id for this batch
now = datetime.now()
run_id_value = uuid.uuid4().int >> 96

# Collect counts or note missing tables
results = []
for table in table_list:
    full_table_name = f"{DATABASE_NAME}.{table}"
    try:
        df = spark.read.table(full_table_name)
        count = df.count()
        comment = "table present"
    except Exception:
        count = None
        comment = "table not present"

    results.append((run_id_value, table, count, now, comment))

# Define schema explicitly
schema = StructType([
    StructField("run_id", LongType(), False),
    StructField("table_name", StringType(), False),
    StructField("record_count", IntegerType(), True),
    StructField("timestamp", TimestampType(), False),
    StructField("comment", StringType(), True)
])

# Create final result DataFrame
df = spark.createDataFrame(results, schema=schema)

# Create output table if it does not exist
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {OUTPUT_TABLE} (
        run_id BIGINT,
        table_name STRING,
        record_count INT,
        timestamp TIMESTAMP,
        comment STRING
    )
    USING DELTA
""")

# Write new results to output table
df.write.mode("append").saveAsTable(OUTPUT_TABLE)
