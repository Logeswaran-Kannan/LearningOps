🔍 Test Scenario ID: E2E-06 – Schema Drift Handling
🔹 Purpose
This test ensures the ingestion pipeline can detect, tolerate, and log schema drift — unplanned or undocumented changes in the structure of incoming data — without causing pipeline crashes or data corruption. It also validates that schema drift handling does not compromise the data lineage, reliability, or downstream consumption in the Bronze and Silver layers.

🔹 Scope and Components Involved
Message Source: EventHub payloads with evolved, missing, or extra fields

Bronze Layer: Ingests raw data, optionally captures unknown fields

Silver Layer: Applies strict transformations and may need fallback logic

Monitoring Tools: Databricks logs, _rescued_data, schema inference audit logs

Types of Drift Handled:

Additional unexpected fields

Missing fields (not critical to transformations)

Data type changes (e.g., integer → string)

🔹 Validation Objectives
Verify pipeline can ingest messages with drifted schema

Ensure drift does not halt processing or lead to silent data loss

Validate logging or capturing of unknown fields for downstream analysis

Confirm Silver logic can handle absence or transformation of new/modified fields

Trace drift events via metadata, logs, or lineage tags

🧪 Testing Approach
✅ Inject Schema Drift into Test Messages
Generate a valid EventHub message with baseline schema

Send subsequent messages with:

One or more new fields (e.g., test_field, new_dimension)

Removed optional field (e.g., omit customer_note)

Modified field type (e.g., send string instead of int for rating)

Validate:

Bronze still ingests without failure

Unknown fields are captured in _rescued_data or ignored per config

Silver layer transformations do not fail and log field inconsistencies

Monitor for any alerts, schema mismatch warnings, or drift logs

📋 Validation Methods
Inspect _rescued_data in Bronze for new or unexpected fields

Analyze Spark logs or DLT alerts for schema mismatch events

Verify transformation logic uses safe parsing, default values, or try_cast

Check that row counts and record lineage remain consistent post-drift

✅ Success Criteria
Bronze job does not crash on schema drift

Extra/missing fields handled without data loss

Schema drift is recorded in logs or a metadata tracking table

Silver jobs run successfully, applying fallbacks where needed

No impact on downstream data quality or consumption

🧾 Exclusions
Version-controlled schema management is addressed in SCHEMA-02

Optional field handling by design is covered in SCHEMA-01

🗂 Mapped Data Quality Requirements
DQ Ref	Description
RL DQ01 – Validity	Data should align with structural expectations
RL DQ04 – Flexibility	Pipeline must tolerate drift within boundaries
RL DQ09 – Traceability	Schema anomalies must be logged or tagged
RL DQ06 – Reliability	Pipeline should not fail unexpectedly due to schema variance

🔐 Pre-requisites & Access Required
Access to:

EventHub ingestion stream (for injecting messages)

Bronze tables with _rescued_data enabled

DLT job logs and Silver transformation notebooks

Ability to:

Modify and send JSON payloads with simulated drift

Observe and analyze ingestion logs, data lineage tags, or alerts
