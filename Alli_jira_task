🔍 Test Scenario ID: EH-BR-05 – Duplication Handling
🔹 Purpose
This scenario verifies that the ingestion pipeline properly identifies and handles duplicate messages received from EventHub to prevent redundant or repeated records in the Bronze Layer. The objective is to ensure either deduplication, quarantine, or logging of duplicates based on defined uniqueness logic, while maintaining the integrity of downstream processing.

🔹 Scope and Components Involved
Potential Duplication Points:

Source system retries (e.g., Athena or MQS re-sending same message)

MuleSoft re-delivery logic

EventHub message replay due to offset reset or checkpoint rollback

Pipeline Layers:

Azure Event Hub

DLT streaming pipeline (checkpoint and offset logic)

Bronze Landed Table

Duplication Criteria (defined in ingestion spec or entity mapping):

Repeated message_id or quote_id

Identical payload hash across multiple records

Same event_type and timestamp combination (depending on message type)

🔹 Validation Objectives
Ensure that duplicate messages do not create redundant rows in the Bronze table.

Confirm that duplicates are either:

Dropped silently (if idempotency logic is enforced),

Logged/quarantined for traceability, or

Marked with a duplicate indicator (e.g., is_duplicate = true)

Verify correct checkpoint management to avoid offset reprocessing in case of pipeline failures.

🧪 Testing Approach
✅ Against Ingestion Behavior & DLT Checkpoint Logic
Inject a test message with a known quote_id or message_id multiple times into EventHub:

Within seconds (simulated retry)

After a checkpoint reset

Monitor how the DLT job processes the reappeared record.

Check for:

Record count difference in Bronze

Fields like uuid, eventhub_offset, _ingest_time for new insertions

Any deduplication logic applied at Bronze level (e.g., Spark dropDuplicates, hash filters)

📋 Validation Methods
Send N identical messages from Mule to EventHub.

Confirm whether:

Only 1 record appears in the Bronze table.

Additional records are flagged or captured in a duplicate log table.

Inspect _ingest_time, eventhub_offset, or any hash key to detect redundancy.

✅ Success Criteria
No duplicate records exist in Bronze for the same logical message.

Records with matching payloads are:

Filtered out at Bronze ingestion time, OR

Clearly marked/logged with duplication info

Pipeline maintains stability and does not reprocess offsets during restart or retries.

No impact to downstream Silver or Gold pipelines due to redundant Bronze records.

🧾 Exclusions
Deduplication during downstream transformation (Silver Layer) is separately covered under BR-SL-01 and BR-SL-05.

Offset continuity and recovery validation is handled in EH-BR-07.

🗂 Mapped Data Quality Requirements
DQ Ref	Description
RL DQ06 – Uniqueness	Ensures duplicate records are identified and managed.
RL DQ09 – Traceability	If duplicates are retained or logged, message lineage must remain clear.
RL DQ03 – Completeness	Reprocessing must not result in partial data due to duplicate discards.

🔐 Pre-requisites & Access Required
Access to:

EventHub (to send known test messages repeatedly)

Bronze table (to inspect message presence, ingest metadata, and UUID fields)

Quarantine/duplicate log table (if applicable)

DLT notebook or streaming pipeline logic (to confirm deduplication logic)

Understand how DLT handles:

Checkpoint tracking (.checkpointLocation)

Streaming deduplication (e.g., via Spark dropDuplicates or UUID hash)
