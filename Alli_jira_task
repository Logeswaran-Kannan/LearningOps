# Databricks notebook script for data validation between managed tables, views, and source SQL

from pyspark.sql import SparkSession
import pandas as pd
import re
from pyspark.sql.functions import lit, when

# Initialize Spark session
spark = SparkSession.builder.getOrCreate()

# Load the CSV with metadata details
csv_path = "/dbfs/path/to/your_metadata_file.csv"  # Update this path
metadata_df = pd.read_csv(csv_path)

# Filter for scope = 'Y'
scoped_df = metadata_df[metadata_df['Scope'] == 'Y']

# Function to extract appropriate SQL
def extract_sql(sql):
    sql_clean = sql.replace('\n', ' ').replace('\r', ' ').strip()
    if re.match(r'(?i)^create\s+(view|function)', sql_clean):
        match = re.search(r'(?i)as\s+(select\s+.+)', sql_clean, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        else:
            raise ValueError("Could not extract SELECT statement from CREATE FUNCTION/VIEW SQL.")
    else:
        return sql_clean

# Results list
validation_results = []

# Loop through each scoped entry and perform validation
for index, row in scoped_df.iterrows():
    table_fqn = f"{row['table_catalog']}.{row['table_schema']}.{row['table_name']}"
    view_fqn = f"{row['view_catalog']}.{row['view_schema']}.{row['view_name']}"

    # Extract final SQL to be executed
    try:
        final_sql = extract_sql(row['SQL'])
    except ValueError as ve:
        validation_results.append({
            "view": view_fqn,
            "table": table_fqn,
            "script_sql": "ERROR extracting SQL",
            "error": str(ve),
            "status": "Failed"
        })
        continue

    # Get counts from table, view, and SQL
    try:
        table_count = spark.sql(f"SELECT COUNT(*) FROM {table_fqn}").collect()[0][0]
        view_count = spark.sql(f"SELECT COUNT(*) FROM {view_fqn}").collect()[0][0]
        script_count = spark.sql(final_sql).count()

        status = "Match"
        if len(set([table_count, view_count, script_count])) > 1:
            status = "Mismatch"

        validation_results.append({
            "view": view_fqn,
            "table": table_fqn,
            "table_count": table_count,
            "view_count": view_count,
            "script_count": script_count,
            "script_sql": final_sql,
            "status": status
        })

    except Exception as e:
        validation_results.append({
            "view": view_fqn,
            "table": table_fqn,
            "script_sql": final_sql,
            "error": str(e),
            "status": "Failed"
        })

# Convert result to DataFrame
results_df = pd.DataFrame(validation_results)
spark_df = spark.createDataFrame(results_df)

# Add color coding (note: display() in notebooks handles this visually with styling tools)
spark_df = spark_df.withColumn("status_color", when(spark_df.status == "Match", lit("✅ Match")).otherwise(lit("❌ Mismatch/Failed")))

# Display results
display(spark_df.select("view", "table", "table_count", "view_count", "script_count", "status", "status_color", "script_sql"))

# Optional: Save results to a managed table or CSV
# results_df.to_csv("/dbfs/path/to/output/validation_results.csv", index=False)
