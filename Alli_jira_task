from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, lit
from concurrent.futures import ThreadPoolExecutor, as_completed
import re

# Initialize Spark session
spark = SparkSession.builder.getOrCreate()

# Configuration
catalog = "core"
database = "ods_views"
prefixes = ["vw_billingcenter", "vw_policycenter", "vw_claimcenter"]
max_threads = 10  # Adjust based on your cluster capacity

# Fetch tables
spark.sql(f"USE CATALOG {catalog}")
spark.sql(f"USE {database}")
tables_df = spark.sql("SHOW TABLES")

# Filter relevant tables
pattern = re.compile(r"^(vw_billingcenter|vw_policycenter|vw_claimcenter).*")
filtered_tables = [row.tableName for row in tables_df.collect() if pattern.match(row.tableName)]

# Shared list to collect results
results = []

# Function to process a table
def check_duplicates(table_name):
    try:
        full_table_name = f"{catalog}.{database}.{table_name}"
        df = spark.table(full_table_name)
        
        if "AZURE_LOAD_DATE" in df.columns:
            df = df.drop("AZURE_LOAD_DATE")

        dup_count = df.groupBy(df.columns) \
                      .count() \
                      .filter("count > 1") \
                      .count()

        return (table_name, dup_count)
    except Exception as e:
        return (table_name, f"Error: {str(e)}")

# Parallel processing with progress tracking
progress = 0
total = len(filtered_tables)

with ThreadPoolExecutor(max_workers=max_threads) as executor:
    future_to_table = {executor.submit(check_duplicates, table): table for table in filtered_tables}
    
    for future in as_completed(future_to_table):
        table = future_to_table[future]
        try:
            result = future.result()
            results.append(result)
        except Exception as exc:
            results.append((table, f"Error: {str(exc)}"))
        
        progress += 1
        print(f"Checked {progress}/{total} tables...")

# Create final DataFrame with results
results_df = spark.createDataFrame(results, ["table_name", "duplicate_count"])
results_df.show(truncate=False)

# Optionally save to a Delta table
results_df.write.mode("overwrite").format("delta").saveAsTable("core.validation_results.duplicate_counts")
