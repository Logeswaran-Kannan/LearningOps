from pyspark.sql import SparkSession, functions as F

# Initialize Spark
spark = SparkSession.builder.getOrCreate()

# Read the delta table
df = spark.table("core_preprod.default.table_counts")

# Get the two latest run_timestamps
latest_runs = (
    df.select("run_timestamp")
    .distinct()
    .orderBy(F.col("run_timestamp").desc())
    .limit(2)
    .collect()
)

# Ensure there are at least two runs
if len(latest_runs) < 2:
    raise Exception("â— Not enough runs to compare.")

latest_ts = latest_runs[0][0]
previous_ts = latest_runs[1][0]

# Format timestamps for column headers
latest_label = f"current_count_{str(latest_ts)[:19].replace(':', '-')}"
previous_label = f"previous_count_{str(previous_ts)[:19].replace(':', '-')}"
print(f"ğŸ” Comparing: {previous_label} â†’ {latest_label}")

# Filter only those two runs
df_filtered = df.filter(F.col("run_timestamp").isin([latest_ts, previous_ts]))

# Pivot by run_timestamp
df_pivot = df_filtered.groupBy("table_name").pivot("run_timestamp").agg(F.first("record_count"))

# Rename columns with timestamp labels
df_pivot = df_pivot.withColumnRenamed(str(previous_ts), previous_label) \
                   .withColumnRenamed(str(latest_ts), latest_label)

# Compute difference
df_result = df_pivot.withColumn(
    "count_diff",
    F.col(latest_label) - F.col(previous_label)
)

# Add icon column
df_result = df_result.withColumn(
    "change_icon",
    F.when(F.col("count_diff") > 0, "ğŸŸ¢â¬†ï¸")  # Increase
     .when(F.col("count_diff") < 0, "ğŸ”´â¬‡ï¸")  # Decrease
     .otherwise("ğŸŸ¡â¸ï¸")                      # No change
)

# Display the results
df_result.display()
