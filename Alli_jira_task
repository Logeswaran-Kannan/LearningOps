from pyspark.sql import SparkSession, functions as F

# Initialize Spark
spark = SparkSession.builder.getOrCreate()

# Read the delta table
df = spark.table("core_preprod.default.table_counts")

# Step 1: Filter only relevant table_name prefixes
df = df.filter(
    (F.col("table_name").startswith("vw_billingcenter_current_")) |
    (F.col("table_name").startswith("vw_policycenter_current_")) |
    (F.col("table_name").startswith("vw_claimcenter_current_"))
)

# Step 2: Get the two most recent run_timestamps
latest_runs = (
    df.select("run_timestamp")
    .distinct()
    .orderBy(F.col("run_timestamp").desc())
    .limit(2)
    .collect()
)

if len(latest_runs) < 2:
    raise Exception("â— Not enough runs to compare (need at least two).")

latest_ts = latest_runs[0][0]
previous_ts = latest_runs[1][0]

# Format timestamps for column headers
latest_label = f"current_count_{str(latest_ts)[:19].replace(':', '-')}"
previous_label = f"previous_count_{str(previous_ts)[:19].replace(':', '-')}"
print(f"ğŸ” Comparing: {previous_label} â†’ {latest_label}")

# Step 3: Filter for those two runs
df_filtered = df.filter(F.col("run_timestamp").isin([latest_ts, previous_ts]))

# Step 4: Pivot table
df_pivot = df_filtered.groupBy("table_name").pivot("run_timestamp").agg(F.first("record_count"))

# Step 5: Rename columns
df_pivot = df_pivot.withColumnRenamed(str(previous_ts), previous_label) \
                   .withColumnRenamed(str(latest_ts), latest_label)

# Step 6: Add count_diff column
df_result = df_pivot.withColumn(
    "count_diff",
    F.col(latest_label) - F.col(previous_label)
)

# Step 7: Add icon column based on change
df_result = df_result.withColumn(
    "change_icon",
    F.when(F.col("count_diff") > 0, "ğŸŸ¢â¬†ï¸")       # Increase
     .when(F.col("count_diff") < 0, "ğŸ”´â¬‡ï¸")       # Decrease
     .when(F.col("count_diff") == 0, "ğŸŸ¡â¸ï¸")      # No change
     .when(F.col("count_diff").isNull(), "ğŸŸ§â“")   # Missing value
)

# Step 8: Display results
df_result.display()
