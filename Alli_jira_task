ğŸ” Test Scenario ID: EH-BR-02 â€“ Schema Conformance Check
ğŸ”¹ Purpose
To ensure that every message landed in the Bronze layer strictly matches the expected JSON schema structure as defined in the MQS message contract (RAML/interface spec) and entity mapping documents. This validation guarantees structural data integrity and prevents downstream transformation or query failures.

ğŸ”¹ Scope and Components Involved
Source Schema Reference: MQS Interface Documents, RAML Specs, and Mapping Sheets (e.g., for Quote Request, PreComp Response, Rating Request)

Source Systems: Athena â†’ MQS â†’ EventHub

Landing Layer: Bronze tables (ODS Raw Layer in DLT)

The messages include:

Embedded JSON with nested or semi-structured fields

Key payload elements like: quote_id, event_type, cache_id, product_id, timestamp, and nested objects such as core_risk, raw_enrichments, etc.

ğŸ”¹ Validation Objectives
Each JSON message must adhere to the expected structure as documented.

All required fields must be present and correctly typed.

Nested fields must conform to data model rules (e.g., core_risk.driver.age should be integer).

No unexpected fields or schema drift should break the ingestion pipeline.

Any discrepancies should be logged and flagged (either quarantined or rejected).

ğŸ§ª Testing Approach
âœ… Against Reference Mapping Document
Compare Bronze schema with the Mapping Document provided by Mike and MQS Interface Table (from your screenshot).

Use each MQS message type (e.g., MQS Quote Request, PreComp Request/Response, Rating Request) and:

Cross-validate fields and types against Bronze Delta Table definition.

Confirm complex/nested JSON flattening or preservation as per expectation.

Example elements to validate (schema sample â€“ Quote Request):

quote_id: string (required)

event_type: string (required, e.g., "RatingRequest")

core_risk: object (required)

raw_enrichments: object (optional/required as per use case)

_ingest_time, message_id: metadata fields from ingestion

ğŸ“‹ Validation Methods
Use automated data profiling (e.g., DLT expectations or schema enforcement rules).

Validate nullability, data types, structure depth, array handling.

Set up alerts/logs for:

Missing mandatory fields

Type mismatches (e.g., string instead of int)

Unexpected schema fields (indicative of schema drift)

âœ… Success Criteria
All records conform to the expected schema version.

No schema-related ingestion failures (e.g., due to Spark schema inference mismatch).

Error messages from validation failures are logged and quarantined properly.

Transformation layer (Silver) is not impacted by unexpected structure variations.

ğŸ§¾ Exclusions
Type casting validation (string-to-timestamp etc.) is part of BR-SL-04.

Backward compatibility with schema changes is tested under SCHEMA-01.

ğŸ—‚ Mapped Data Quality Requirements
DQ Ref	Description
RL DQ01 â€“ Accuracy	Ensures the payload accurately represents expected entities/fields.
RL DQ05 â€“ Validity	Applies validation rules and standards for acceptable formats/types.
RL DQ03 â€“ Completeness	Mandatory fields are checked as part of the schema conformance.

ğŸ” Pre-requisites & Access Required
Access to:

The latest data mapping document and RAML contracts.

Bronze Layer Table Definitions (Delta schemas).

Databricks or DLT logs for schema mismatch errors.

Test Message Samples: Available in interface doc (Sample MQS Quote Request JSON.json, PreComp_Response_Sample.json, etc.)

