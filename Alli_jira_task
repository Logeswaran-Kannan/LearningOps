from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType
from datetime import datetime
import uuid

# Initialize Spark session
spark = SparkSession.builder.appName("TableCountChecker").getOrCreate()

# Input list of tables in 'database.table' format
tables = [
    "db1.table1",
    "db2.table2",
    "nonexistent_db.tableX"
]

# Constants
OUTPUT_TABLE = "core_tst_sys9.default.table_count_audit"
now = datetime.now()
run_id_value = uuid.uuid4().int >> 96

# Results collection
results = []

for full_table_name in tables:
    try:
        df = spark.read.table(full_table_name)
        count = df.count()
        comment = "table present"
    except Exception:
        count = None
        comment = "table not present"

    results.append((run_id_value, full_table_name, count, now, comment))

# Define schema
schema = StructType([
    StructField("run_id", LongType(), False),
    StructField("table_name", StringType(), False),
    StructField("row_count", IntegerType(), True),
    StructField("timestamp", TimestampType(), False),
    StructField("comment", StringType(), True)
])

# Create DataFrame
df = spark.createDataFrame(results, schema=schema)

# Create output table if it does not exist
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {OUTPUT_TABLE} (
        run_id BIGINT,
        table_name STRING,
        row_count INT,
        timestamp TIMESTAMP,
        comment STRING
    )
    USING DELTA
""")

# Write to output table
df.write.mode("append").saveAsTable(OUTPUT_TABLE)
