from pyspark.sql.functions import col, explode, from_json, schema_of_json
from pyspark.sql import SparkSession

# Start Spark session
spark = SparkSession.builder.getOrCreate()

# Read the table
df = spark.table("mqs.dlt_quote_request")

# Infer schema from a sample JSON to help parse
sample_json = df.select("Payload").filter("Payload IS NOT NULL").limit(1).collect()[0][0]
json_schema = schema_of_json(sample_json)

# Parse the JSON string into a structured column
parsed_df = df.withColumn("parsed", from_json(col("Payload"), json_schema))

# Navigate nested structure: coreRisk -> drivers[] -> incidents -> claims[]
exploded_df = parsed_df \
    .withColumn("driver", explode("parsed.coreRisk.drivers")) \
    .withColumn("claim", explode("driver.incidents.claims"))

# Extract the required fields if present
result_df = exploded_df.select(
    col("claim.claim_PRN").alias("claim_PRN"),
    col("claim.claim_ClaimType").alias("claim_ClaimType"),
    col("claim.claim_Date").alias("claim_Date"),
    col("claim.claim_DriverAtFaultInd").alias("claim_DriverAtFaultInd"),
    col("claim.claim_NcdLostInd").alias("claim_NcdLostInd")
)

# Show or export results
result_df.show(truncate=False)
