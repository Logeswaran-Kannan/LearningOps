from pyspark.sql import SparkSession
import os

# Initialize Spark session
spark = SparkSession.builder.appName("ImportTXTtoCatalogTables").getOrCreate()

# Path in DBFS (without /dbfs/ prefix when using Spark APIs)
txt_path = "dbfs:/path/to/files/"
catalog = "core_tst_sys9"
database = "rr_source"

# Set the current catalog and database
spark.sql(f"USE CATALOG {catalog}")
spark.sql(f"USE DATABASE {database}")

# List all .txt files in the directory
import os
files = [f for f in os.listdir("/dbfs/path/to/files/") if f.endswith(".txt")]

for file_name in files:
    file_path = f"{txt_path}{file_name}"
    table_name = os.path.splitext(file_name)[0].lower()

    # Read the TXT file as DataFrame
    df = spark.read.option("header", "true") \
                   .option("delimiter", "\t") \
                   .option("inferSchema", "true") \
                   .csv(file_path)

    # Write to catalog table, overwrite if exists
    df.write.mode("overwrite") \
          .option("overwriteSchema", "true") \
          .saveAsTable(f"{catalog}.{database}.{table_name}")

    print(f"Table {catalog}.{database}.{table_name} created from {file_name}")
