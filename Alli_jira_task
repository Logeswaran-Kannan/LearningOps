from pyspark.sql.utils import AnalysisException

# List of fully qualified table names (e.g., 'catalog.schema.table')
table_list = [
    "your_catalog.your_schema.table1",
    "your_catalog.your_schema.table2",
    "your_catalog.your_schema.non_existing_table"
]

# Store results
results = []

# Iterate through each table
for table_name in table_list:
    try:
        # Try to read the table
        df = spark.table(table_name)
        row_count = df.count()
        results.append((table_name, row_count))
    except AnalysisException:
        # If table not found
        results.append((table_name, "No table found"))

# Convert results to DataFrame for display
result_df = spark.createDataFrame(results, ["Table Name", "Record Count"])
display(result_df)
