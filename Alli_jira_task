from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, count, when, current_timestamp, regexp_extract, length, isnan
from pyspark.sql.types import *
import datetime
import time

spark = SparkSession.builder.appName("SchemaAndDataValidation").enableHiveSupport().getOrCreate()

EXCLUDED_COLUMNS = ["DWH_FROM_DATE", "DWH_UNTIL_DATE", "DWH_CURRENT_FLAG", "AZURE_LOAD_DATE"]
SOURCE_DB = "ods_stg_copy"
TARGET_DB = "ods"
CATALOG = "core_tst_std001"
OUTPUT_TABLE = f"{CATALOG}.validation_results"
RUN_ID = int(time.time())
VALIDATION_TIMESTAMP = datetime.datetime.now()

# Define schema
schema = StructType([
    StructField("run_id", IntegerType(), False),
    StructField("table_name", StringType(), False),
    StructField("src_database", StringType(), False),
    StructField("tgt_database", StringType(), False),
    StructField("src_vs_tgt_missing_columns", StringType(), True),
    StructField("tgt_vs_src_missing_columns", StringType(), True),
    StructField("src_count", IntegerType(), True),
    StructField("tgt_count", IntegerType(), True),
    StructField("src_duplicate_count", IntegerType(), True),
    StructField("tgt_duplicate_count", IntegerType(), True),
    StructField("src_vs_tgt_data_mismatch_count", IntegerType(), True),
    StructField("tgt_vs_src_data_mismatch_count", IntegerType(), True),
    StructField("null_diff_columns", StringType(), True),
    StructField("data_mismatch_columns", StringType(), True),
    StructField("junk_values_columns", StringType(), True),
    StructField("status", StringType(), False),
    StructField("comment", StringType(), True),
    StructField("time_taken_seconds", DoubleType(), True),
    StructField("source_sql", StringType(), True),
    StructField("target_sql", StringType(), True),
    StructField("validation_timestamp", TimestampType(), True),
    StructField("test_case_detail", StringType(), True),
])

# Housekeeping
spark.sql(f"DELETE FROM {OUTPUT_TABLE} WHERE validation_timestamp < DATE_SUB(CURRENT_DATE(), 7)")

# Get list of tables
tables = [row.tableName for row in spark.catalog.listTables(SOURCE_DB)]
results = []

for idx, table in enumerate(tables):
    start_time = time.time()
    try:
        src_df = spark.table(f"{SOURCE_DB}.{table}").drop(*EXCLUDED_COLUMNS)
        tgt_df = spark.table(f"{TARGET_DB}.{table}").drop(*EXCLUDED_COLUMNS)

        src_cols = set(src_df.columns)
        tgt_cols = set(tgt_df.columns)

        src_only = src_cols - tgt_cols
        tgt_only = tgt_cols - src_cols
        common_cols = list(src_cols & tgt_cols)

        src_count = src_df.count()
        tgt_count = tgt_df.count()

        src_dup = src_df.groupBy(common_cols).count().filter("count > 1").count()
        tgt_dup = tgt_df.groupBy(common_cols).count().filter("count > 1").count()

        mismatch_columns = []
        for col_name in common_cols:
            mismatches = src_df.select(col_name).subtract(tgt_df.select(col_name)).union(tgt_df.select(col_name).subtract(src_df.select(col_name)))
            if mismatches.count() > 0:
                mismatch_columns.append(col_name)

        mismatch_count = len(mismatch_columns)

        null_diff_cols = [col for col in common_cols if src_df.filter(col(col).isNull()).count() == src_count and tgt_df.filter(col(col).isNull()).count() != tgt_count]

        junk_columns = []
        junk_pattern = r"[^a-zA-Z0-9\s\.,;:@\-]"
        for col_name in common_cols:
            if src_df.filter(regexp_extract(col(col_name).cast("string"), junk_pattern, 0) != "").count() > 0:
                junk_columns.append(col_name)

        comment_list = []
        if src_only or tgt_only:
            comment_list.append("schema mismatch")
        if src_count != tgt_count:
            comment_list.append("count issue")
        if src_dup > 0 or tgt_dup > 0:
            comment_list.append("duplicate issue")
        if mismatch_columns:
            comment_list.append("data issue")
        if junk_columns:
            comment_list.append("junk values issue")

        comment = " | ".join(comment_list) if comment_list else "All checks passed"

        results.append((
            RUN_ID, table, SOURCE_DB, TARGET_DB,
            ",".join(src_only), ",".join(tgt_only),
            src_count, tgt_count,
            src_dup, tgt_dup,
            mismatch_count, mismatch_count,
            ",".join(null_diff_cols), ",".join(mismatch_columns), ",".join(junk_columns),
            "Success" if not comment_list else "Failure",
            comment,
            time.time() - start_time,
            f"SELECT * FROM {SOURCE_DB}.{table}",
            f"SELECT * FROM {TARGET_DB}.{table}",
            VALIDATION_TIMESTAMP,
            ""
        ))
        print(f"Completed validation for table: {table} | Remaining: {len(tables) - idx - 1} | Time: {time.time() - start_time:.2f}s")

    except Exception as e:
        results.append((
            RUN_ID, table, SOURCE_DB, TARGET_DB,
            "", "", 0, 0, 0, 0, 0, 0, "", "", "", "Error", str(e),
            time.time() - start_time,
            "", "", VALIDATION_TIMESTAMP, ""
        ))
        print(f"Error validating table: {table} | {str(e)}")

# Save result
spark.createDataFrame(results, schema).write.format("delta").mode("overwrite").saveAsTable(OUTPUT_TABLE)
