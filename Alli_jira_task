import requests
import pandas as pd
import sqlalchemy

# Token and URL Setup
token = "test"
url = "https://adb-67779643136828772.12.azuredatabricks.net/api/2.0/lineage-tracking/column-lineage"
headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json"
}

# Set up a connection to the information schema (adjust the connection string as needed)
engine = sqlalchemy.create_engine('databricks+connector://<username>:<password>@<hostname>/<catalog>/<schema>')

# Query the information schema to get tables and columns
query = """
SELECT table_name, column_name 
FROM information_schema.columns 
WHERE table_schema = 'ods_views'
"""
table_info = pd.read_sql(query, engine)

# Iterate through each table and column
all_lineage_data = []

for _, row in table_info.iterrows():
    params = {
        "table_name": f"core_tst_std001.ods_views.{row['table_name']}",
        "column_name": row['column_name'],
        "include_entity_lineage": True
    }

    response = requests.get(url, headers=headers, params=params)

    if response.status_code == 200:
        data = response.json()
        upstream = pd.json_normalize(data.get('upstream_cols', []))
        downstream = pd.json_normalize(data.get('downstream_cols', []))

        upstream["direction"] = "upstream"
        downstream["direction"] = "downstream"
        lineage_df = pd.concat([upstream, downstream], ignore_index=True)
        lineage_df["source_table"] = row['table_name']
        lineage_df["source_column"] = row['column_name']

        all_lineage_data.append(lineage_df)
    else:
        print(f"Failed for {row['table_name']}.{row['column_name']}: {response.status_code}")

# Combine all collected lineage data
if all_lineage_data:
    final_df = pd.concat(all_lineage_data, ignore_index=True)
    display(final_df)
else:
    print("No lineage data collected.")
