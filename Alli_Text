from pyspark.sql import functions as F

target_table = "core_bld.default.monitoring_mqs_interaction_sta"

# -----------------------------
# 1. Source: FLOW stream
# -----------------------------
flow_df = (
    spark.table("core.flow.ehubstream")
    .filter(F.col("eHubQueuedDatestamp") == F.current_date())
    .select(
        F.to_date(F.get_json_object("payload", "$.timestamp")).alias("run_date"),
        F.to_timestamp(F.get_json_object("payload", "$.timestamp")).alias("event_ts"),
        F.get_json_object("payload", "$.interaction.status").alias("status"),
        F.get_json_object("payload", "$.interaction.mqsQuoteId").alias("mqs_quote_id"),
        F.get_json_object("payload", "$.interaction.id").alias("interaction_id"),
    )
)

# -----------------------------
# 2. Source: MTR stream
# -----------------------------
mtr_df = (
    spark.table("core.mqs_atnh_mtr.ehubstream")
    .filter(F.to_date("enqueuedTime") == F.current_date())
    .select(
        F.col("headers")["mqsQuoteId"].alias("mqs_quote_id"),
        F.col("headers")["externalId"].alias("interaction_id"),
    )
    .dropDuplicates()
)

# -----------------------------
# 3. Join (fast, multi-column)
# -----------------------------
joined_df = (
    flow_df.join(
        mtr_df,
        on=["mqs_quote_id", "interaction_id"],
        how="inner"
    )
    .select(
        "run_date",
        F.from_unixtime(
            (F.unix_timestamp("event_ts") / 1800).cast("long") * 1800
        ).alias("interval_start"),
        "status",
        "mqs_quote_id",
    )
)

# -----------------------------
# 4. Aggregate + SLA flag
# -----------------------------
final_df = (
    joined_df
    .groupBy("run_date", "interval_start", "status")
    .agg(
        F.count("mqs_quote_id").alias("total_count")
    )
    .withColumn(
        "is_sla_breach",
        F.when(F.col("total_count") < 3, F.lit(1)).otherwise(F.lit(0))
    )
)

# -----------------------------
# 5. Create table if not exists
# -----------------------------
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {target_table} (
    run_date DATE,
    interval_start TIMESTAMP,
    status STRING,
    total_count BIGINT,
    is_sla_breach INT
)
USING DELTA
PARTITIONED BY (run_date)
""")

# -----------------------------
# 6. FULL REFRESH (overwrite)
# -----------------------------
(
    final_df
    .write
    .format("delta")
    .mode("overwrite")
    .option("overwriteSchema", "true")
    .saveAsTable(target_table)
)
