# ================================================================
# FULL PIPELINE: VARIANT → BASE64 → SNAPPY → JSON → FLATTEN
# ================================================================

import base64
import snappy
import json

from pyspark.sql.functions import *
from pyspark.sql.types import *

# ---------------------------------------------------------------
# 1. Decode UDF: Base64 → Snappy → JSON string
# ---------------------------------------------------------------
def decode_snappy_base64(b64_str):
    if b64_str is None:
        return None
    try:
        raw = base64.b64decode(b64_str)
        decompressed = snappy.decompress(raw)
        return decompressed.decode("utf-8")
    except Exception:
        return None

decode_udf = udf(decode_snappy_base64, StringType())

# ---------------------------------------------------------------
# 2. Load source table (contains VARIANT Payload)
# ---------------------------------------------------------------
table_name = "core_tst.mqs_athn_mtr.dlt_raw_enrichments"
df = spark.table(table_name)

# ---------------------------------------------------------------
# 3. Convert VARIANT → STRING and extract the .data field
# ---------------------------------------------------------------
df1 = df.withColumn("payload_str", col("Payload").cast("string"))

df2 = df1.withColumn(
    "payload_data_base64",
    get_json_object(col("payload_str"), "$.data")
)

# ---------------------------------------------------------------
# 4. Decode Snappy Base64 into raw JSON string
# ---------------------------------------------------------------
df3 = df2.withColumn(
    "payload_json",
    decode_udf(col("payload_data_base64"))
)

# ---------------------------------------------------------------
# 5. Infer schema dynamically from decoded JSON
# ---------------------------------------------------------------
sample = df3.select("payload_json") \
            .dropna() \
            .limit(1) \
            .collect()[0][0]

json_schema = spark.read.json(
    spark.sparkContext.parallelize([sample])
).schema

# ---------------------------------------------------------------
# 6. Convert JSON string → struct
# ---------------------------------------------------------------
df_struct = df3.withColumn(
    "payload_struct",
    from_json(col("payload_json"), json_schema)
)

# ---------------------------------------------------------------
# 7. Deep recursive flatten function
# ---------------------------------------------------------------
def flatten_recursive(df):
    complex_fields = True

    while complex_fields:

        # find complex fields
        complex_fields = [
            (field.name, field.dataType)
            for field in df.schema.fields
            if isinstance(field.dataType, (StructType, ArrayType, MapType))
        ]

        if not complex_fields:
            break

        col_name, col_type = complex_fields[0]

        # ----- Struct -----
        if isinstance(col_type, StructType):
            expanded_cols = [
                col(f"{col_name}.{f.name}").alias(f"{col_name}_{f.name}")
                for f in col_type.fields
            ]
            df = df.select("*", *expanded_cols).drop(col_name)

        # ----- Array -----
        elif isinstance(col_type, ArrayType):
            df = df.withColumn(col_name, explode_outer(col(col_name)))

        # ----- Map -----
        elif isinstance(col_type, MapType):
            map_keys_list = (
                df.select(map_keys(col(col_name)))
                  .dropna()
                  .limit(1)
                  .collect()[0][0]
            )

            expanded_map_cols = [
                col(col_name).getItem(k).alias(f"{col_name}_{k}")
                for k in map_keys_list
            ]

            df = df.select("*", *expanded_map_cols).drop(col_name)

    return df

# ---------------------------------------------------------------
# 8. Apply deep flattening
# ---------------------------------------------------------------
df_flat = flatten_recursive(
    df_struct.drop("payload_json", "payload_data_base64", "payload_str")
)

# ---------------------------------------------------------------
# 9. Clean column names
# ---------------------------------------------------------------
for c in df_flat.columns:
    df_flat = df_flat.withColumnRenamed(
        c,
        c.replace(".", "_").replace(" ", "_")
    )

# ---------------------------------------------------------------
# 10. Final output
# ---------------------------------------------------------------
df_flat.display()

# ---------------------------------------------------------------
# OPTIONAL: Save to table
# ---------------------------------------------------------------
# df_flat.write.format("delta").mode("overwrite") \
#        .saveAsTable("core_tst.mqs_athn_mtr.enrichments_flattened")

# ================================================================
# END OF SCRIPT
# ================================================================
