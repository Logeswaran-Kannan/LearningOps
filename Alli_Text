Thanks for raising this point — it’s a valid question and happy to clarify the current design and controls in place.

Yes, this is intentionally designed as a single job, and the key reason is that data integrity is enforced before anything is loaded into Cosmos DB or made active.

A few important points to clarify:

The job will not load any data into the database if it fails data quality checks.

Data Quality (DQ) rules are configurable and customisable, and can be made as strong as required.

Only when all configured DQ validations pass, the job proceeds to:

Load the data into Cosmos DB

Mark the file as active/passive as part of the same controlled flow

If any data quality or integrity issue is detected, the job fails early, and:

Data is not loaded

The file is not activated

Downstream quote processing is therefore protected

Because of this, the risk scenario mentioned (data with issues getting loaded and quotes failing immediately) should not occur, as the activation step is dependent on successful DQ validation.

While splitting the job into two (load + activate) can offer flexibility, the current approach achieves the same safety by:

Enforcing strict pre-load validation

Preventing partial or inconsistent states

Keeping activation tightly coupled to validated data only

We believe this design gives sufficient control, provided the DQ rules are defined strictly enough to catch issues upfront, which is the recommended approach.

Happy to discuss further or refine DQ rules if UAT has specific integrity checks in mind.
