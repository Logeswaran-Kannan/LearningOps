# Databricks / PySpark: Keep ONE ROW per original record
# - Filters by LOAD_DT between 2025-10-25 and 2025-10-30 23:59:59 (inclusive)
# - Parses JSON in column `CVR`
# - Recursively FLATTENS only STRUCTS into top-level columns
# - DOES NOT explode arrays; arrays/maps (including arrays-of-structs) are serialized to JSON strings
# - Creates temp view `flattened_json_out`

from pyspark.sql import functions as F
from pyspark.sql import types as T
from pyspark.sql import DataFrame

# ---------------------------------------------------------------------------
# Configuration — set your source table
# ---------------------------------------------------------------------------
source_table = "your_catalog.your_schema.your_table"  # <-- change this
json_col = "CVR"
start_date = "2025-10-25"
end_date = "2025-10-30 23:59:59"

# ---------------------------------------------------------------------------
# Load & filter by LOAD_DT
# ---------------------------------------------------------------------------
df = spark.table(source_table)
df = df.withColumn("LOAD_DT_TS", F.to_timestamp(F.col("LOAD_DT")))
df = df.where((F.col("LOAD_DT_TS") >= F.to_timestamp(F.lit(start_date))) &
              (F.col("LOAD_DT_TS") <= F.to_timestamp(F.lit(end_date))))

# ---------------------------------------------------------------------------
# Parse CVR JSON into a struct (works for STRING/STRUCT/MAP/ARRAY inputs)
# ---------------------------------------------------------------------------

def infer_schema_from_sample(sample_df: DataFrame, json_col_name: str, limit_rows: int = 20000) -> T.StructType:
    sample = (sample_df
              .select(F.col(json_col_name).cast("string").alias("_json"))
              .where(F.col("_json").isNotNull())
              .limit(limit_rows))
    if sample.rdd.isEmpty():
        return T.StructType([])
    return spark.read.json(sample.rdd.map(lambda r: r[0])).schema

src_field = next((f for f in df.schema.fields if f.name == json_col), None)
if src_field is None:
    raise ValueError(f"Column '{json_col}' not found in {source_table}.")

parsed_col = "__parsed_json__"

if isinstance(src_field.dataType, T.StringType):
    inferred_schema = infer_schema_from_sample(df, json_col)
    work = df.withColumn(parsed_col, F.from_json(F.col(json_col), inferred_schema))
elif isinstance(src_field.dataType, T.StructType):
    work = df.withColumn(parsed_col, F.col(json_col))
elif isinstance(src_field.dataType, (T.MapType, T.ArrayType)):
    json_str_df = df.withColumn("__json_str__", F.to_json(F.col(json_col)))
    inferred_schema = infer_schema_from_sample(json_str_df, "__json_str__")
    work = json_str_df.drop("__json_str__").withColumn(parsed_col, F.from_json(F.to_json(F.col(json_col)), inferred_schema))
else:
    json_str_df = df.withColumn("__json_str__", F.to_json(F.col(json_col)))
    inferred_schema = infer_schema_from_sample(json_str_df, "__json_str__")
    work = json_str_df.drop("__json_str__").withColumn(parsed_col, F.from_json(F.to_json(F.col(json_col)), inferred_schema))

# ---------------------------------------------------------------------------
# Deep FLATTEN (NO EXPLODE):
# - Repeatedly expand Struct columns into top-level columns with underscores
# - Convert any Array/Map columns to JSON strings to keep one row
# ---------------------------------------------------------------------------

def expand_struct_once(df_in: DataFrame) -> DataFrame:
    struct_cols = [f.name for f in df_in.schema.fields if isinstance(f.dataType, T.StructType)]
    if not struct_cols:
        return df_in
    # Expand the FIRST struct column found (iterate by calling repeatedly)
    target = struct_cols[0]
    # Keep all non-target columns
    base_cols = [F.col(c) for c in df_in.columns if c != target]
    # Add children as top-level columns with underscore names
    t_schema = next(f.dataType for f in df_in.schema.fields if f.name == target)
    expanded = [F.col(f"{target}.{fld.name}").alias(f"{target}_{fld.name}") for fld in t_schema.fields]
    return df_in.select(*base_cols, *expanded)

flat = work
# Keep expanding until no StructType columns remain
while any(isinstance(f.dataType, T.StructType) for f in flat.schema.fields):
    flat = expand_struct_once(flat)

# Now serialize any Array/Map columns (including arrays-of-structs) to JSON strings
final_cols = []
for f in flat.schema.fields:
    if isinstance(f.dataType, (T.ArrayType, T.MapType)):
        final_cols.append(F.to_json(F.col(f.name)).alias(f.name))
    else:
        final_cols.append(F.col(f.name))

flat = flat.select(*final_cols)

# Clean up any dots in names (defensive) — should already be underscores from expansion
for c in flat.columns:
    if "." in c:
        flat = flat.withColumnRenamed(c, c.replace(".", "_"))

# Drop helper parsed column if it remains primitive/empty
if parsed_col in flat.columns:
    try:
        flat = flat.drop(parsed_col)
    except Exception:
        pass

flat.createOrReplaceTempView("flattened_json_out")
print("Temp view 'flattened_json_out' created — one row per original record (no explode).")

# Quick peek
try:
    display(flat.limit(50))
except NameError:
    pass
