from pyspark.sql import functions as F
from pyspark.sql.types import StringType
import snappy
import json


# 1️⃣ UDF to decompress Snappy
@F.udf(returnType=StringType())
def snappy_decompress(value):
if value is None:
return None
try:
return snappy.uncompress(value).decode("utf-8")
except Exception as e:
return None


# 2️⃣ Read table with VARIANT column
source_df = spark.table("core_tst.mqs_athn_mtr.dlt_raw_enrichments")


# 3️⃣ Extract compressed binary from variant (Payload.rawEnrichment.data)
# Databricks stores VARIANT as a struct → cast to BINARY
extracted_df = (
source_df
.withColumn("compressed_bin", F.col("Payload.rawEnrichment.data").cast("BINARY"))
)


# 4️⃣ Decompress using UDF
with_json_df = extracted_df.withColumn(
"decompressed_json", snappy_decompress(F.col("compressed_bin"))
)


# 5️⃣ Parse JSON into struct
parsed_df = with_json_df.withColumn(
"payload_struct", F.from_json(F.col("decompressed_json"), "MAP<STRING,STRING>")
)


# 6️⃣ Select flattened output
final_df = parsed_df.select(
"TimeEnqueued",
"eHubQueuedDatestamp",
"decompressed_json",
"payload_struct.*"
)


final_df.display()
