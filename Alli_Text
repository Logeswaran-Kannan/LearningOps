from pyspark.sql import functions as F

SLA_THRESHOLD = 100
TARGET_TABLE = "core_bld.default.monitoring_ehub_30min_metrics"

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TARGET_TABLE} (
  interval_start TIMESTAMP,
  interval_end   TIMESTAMP,
  message_type   STRING,
  message_count  BIGINT,
  interval_time  STRING,
  run_date       DATE,
  is_sla_breach  INT
)
USING DELTA
PARTITIONED BY (run_date)
""")

run_ts = F.current_timestamp()
run_date = F.current_date()
day_start = F.date_trunc("day", run_ts)

today_exists = spark.sql(f"""
SELECT 1
FROM {TARGET_TABLE}
WHERE run_date = current_date()
LIMIT 1
""").count() > 0

if today_exists:
    interval_start_expr = (
        day_start +
        F.expr(
            "INTERVAL 1 SECOND * (CAST((unix_timestamp(current_timestamp()) - unix_timestamp(date_trunc('day', current_timestamp()))) / 1800 AS INT) * 1800)"
        )
    )

    intervals_df = (
        spark.range(1)
        .select(interval_start_expr.alias("interval_start"))
        .withColumn("interval_end", F.col("interval_start") + F.expr("INTERVAL 30 MINUTES"))
        .withColumn("interval_time", F.date_format("interval_start", "HH:mm"))
    )

    src_df = (
        spark.table("mqs_athn_mtr.ehubstream")
        .filter(
            (F.col("TimeEnqueued") >= F.col("interval_start")) &
            (F.col("TimeEnqueued") < F.col("interval_end"))
        )
        .select(
            F.col("TimeEnqueued"),
            F.col("headers.messageType").alias("message_type")
        )
    )

else:
    intervals_df = (
        spark.range(0, 48)
        .select(
            (day_start + F.expr("INTERVAL 1 SECOND") * (F.col("id") * 1800)).alias("interval_start")
        )
        .withColumn("interval_end", F.col("interval_start") + F.expr("INTERVAL 30 MINUTES"))
        .withColumn("interval_time", F.date_format("interval_start", "HH:mm"))
        .filter(F.col("interval_start") <= run_ts)
    )

    src_df = (
        spark.table("mqs_athn_mtr.ehubstream")
        .filter(
            (F.col("TimeEnqueued") >= day_start) &
            (F.col("TimeEnqueued") < run_ts)
        )
        .select(
            F.col("TimeEnqueued"),
            F.col("headers.messageType").alias("message_type")
        )
    )

bucketed_df = (
    src_df
    .withColumn(
        "interval_start",
        day_start + F.expr(
            "INTERVAL 1 SECOND * (CAST((unix_timestamp(TimeEnqueued) - unix_timestamp(date_trunc('day', TimeEnqueued))) / 1800 AS INT) * 1800)"
        )
    )
)

agg_by_type_df = (
    bucketed_df
    .groupBy("interval_start", "message_type")
    .agg(F.count("*").alias("message_count"))
)

agg_all_df = (
    bucketed_df
    .groupBy("interval_start")
    .agg(F.count("*").alias("message_count"))
    .withColumn("message_type", F.lit("ALL"))
)

agg_df = agg_by_type_df.unionByName(agg_all_df)

final_df = (
    intervals_df
    .join(agg_df, on="interval_start", how="left")
    .fillna({"message_count": 0})
    .withColumn("run_date", run_date)
    .withColumn(
        "is_sla_breach",
        F.when(F.col("message_count") < SLA_THRESHOLD, F.lit(1)).otherwise(F.lit(0))
    )
)

(
    final_df
    .select(
        "interval_start",
        "interval_end",
        "message_type",
        "message_count",
        "interval_time",
        "run_date",
        "is_sla_breach"
    )
    .write
    .format("delta")
    .mode("append")
    .saveAsTable(TARGET_TABLE)
)

spark.sql(f"""
DELETE FROM {TARGET_TABLE}
WHERE run_date < current_date() - INTERVAL 2 DAYS
""")
