from pyspark.sql import SparkSession

catalog = "core_tst"
schema = "mqs"
schema_full = f"{catalog}.{schema}"

spark.sql(f"USE CATALOG {catalog}")
spark.sql(f"USE SCHEMA {schema}")

# Get all tables in the schema
tables_df = spark.sql(f"SHOW TABLES IN {schema_full}")
tables = [row.tableName for row in tables_df.collect()]

results = []

for table in tables:
    full_table_name = f"{schema_full}.{table}"
    print(f"Processing table: {full_table_name}")

    df = spark.table(full_table_name)
    row_count = df.count()

    for col in df.columns:
        populated_count = df.filter(df[col].isNotNull()).count()
        is_populated = populated_count > 0
        
        results.append((table, col, row_count, populated_count, is_populated))

# Convert results to DataFrame
result_df = spark.createDataFrame(
    results,
    ["table_name", "column_name", "row_count", "non_null_count", "is_column_populated"]
)

display(result_df)
