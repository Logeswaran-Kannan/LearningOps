from pyspark.sql import functions as F

# Read the table
df = spark.table("core_rl_test.flow.ehubstream")

# Calculate payload size in MB
# length() gives character count; convert to MB
df_with_size = df.withColumn(
    "payload_size_mb",
    (F.length(F.col("payload")) / (1024 * 1024)).cast("double")
)

# Extract year-month from enduedate
df_with_month = df_with_size.withColumn(
    "year_month",
    F.date_format(F.col("enduedate"), "yyyy-MM")
)

# Aggregate min and max payload size per month
payload_size_stats = df_with_month.groupBy("year_month").agg(
    F.round(F.min("payload_size_mb"), 4).alias("min_payload_size_mb"),
    F.round(F.max("payload_size_mb"), 4).alias("max_payload_size_mb"),
    F.count("*").alias("record_count")
).orderBy("year_month")

# Display result
display(payload_size_stats)


large_payloads = df_with_month.filter(F.col("payload_size_mb") > 1)

display(large_payloads.select(
    "enduedate",
    "payload_size_mb",
    "payload"
))
