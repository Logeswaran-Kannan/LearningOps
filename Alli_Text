from pyspark.sql import functions as F

SLA_THRESHOLD = 100
TARGET_TABLE = "core_bld.default.monitoring_ehub_30min_metrics"
SOURCE_TABLE = "mqs_athn_mtr.ehubstream"

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TARGET_TABLE} (
  interval_start TIMESTAMP,
  interval_end   TIMESTAMP,
  message_type   STRING,
  message_count  BIGINT,
  interval_time  STRING,
  run_date       DATE,
  is_sla_breach  INT
)
USING DELTA
PARTITIONED BY (run_date)
""")

now_ts = F.current_timestamp()
start_ts = now_ts - F.expr("INTERVAL 2 DAYS")
day_start = F.date_trunc("day", start_ts)

intervals_df = (
    spark.range(0, 144)   # 3 days * 48 intervals
    .select(
        (day_start + F.expr("INTERVAL 1 SECOND") * (F.col("id") * 1800)).alias("interval_start")
    )
    .withColumn("interval_end", F.col("interval_start") + F.expr("INTERVAL 30 MINUTES"))
    .withColumn("interval_time", F.date_format("interval_start", "HH:mm"))
    .withColumn("run_date", F.to_date("interval_start"))
    .filter(F.col("interval_start") <= now_ts)
)

src_df = (
    spark.table(SOURCE_TABLE)
    .filter(
        (F.col("TimeEnqueued") >= day_start) &
        (F.col("TimeEnqueued") < now_ts)
    )
    .select(
        F.col("TimeEnqueued"),
        F.col("headers.messageType").alias("message_type")
    )
    .withColumn(
        "interval_start",
        day_start + F.expr(
            "INTERVAL 1 SECOND * (CAST((unix_timestamp(TimeEnqueued) - unix_timestamp(date_trunc('day', TimeEnqueued))) / 1800 AS INT) * 1800)"
        )
    )
)

agg_by_type_df = (
    src_df
    .groupBy("interval_start", "message_type")
    .agg(F.count("*").alias("message_count"))
)

agg_all_df = (
    src_df
    .groupBy("interval_start")
    .agg(F.count("*").alias("message_count"))
    .withColumn("message_type", F.lit("ALL"))
)

agg_df = agg_by_type_df.unionByName(agg_all_df)

final_df = (
    intervals_df
    .join(agg_df, on="interval_start", how="left")
    .fillna({"message_count": 0})
    .withColumn(
        "is_sla_breach",
        F.when(F.col("message_count") < SLA_THRESHOLD, F.lit(1)).otherwise(F.lit(0))
    )
    .select(
        "interval_start",
        "interval_end",
        "message_type",
        "message_count",
        "interval_time",
        "run_date",
        "is_sla_breach"
    )
)

(
    final_df
    .write
    .format("delta")
    .mode("overwrite")
    .option("replaceWhere", "run_date >= current_date() - INTERVAL 2 DAYS")
    .saveAsTable(TARGET_TABLE)
)
