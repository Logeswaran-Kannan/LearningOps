from pyspark.sql import functions as F
from datetime import date, timedelta

SOURCE_SCHEMA = "core.mqs_athn_mtr"
TARGET_TABLE  = "core_bld.default.monitoring_mqs_30min"

TABLES = [
    "dit_stop_quote",
    "dit_quote_request",
    "dit_precomp_response",
    "dit_rating_response",
    "dit_rating_request",
    "dit_raw_enrichments",
    "dit_cache_id",
    "dit_rating_request_dvla_removed"
]

RETENTION_DAYS = 3
SLA_THRESHOLD = 1   # set N here (e.g. <3, <5 etc.)

today = date.today()
start_date = today - timedelta(days=RETENTION_DAYS - 1)

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TARGET_TABLE} (
    run_date DATE,
    interval_start TIMESTAMP,
    interval_end TIMESTAMP,
    message_type STRING,
    message_count BIGINT,
    is_sla_flag INT
)
USING DELTA
""")

# Retention cleanup (always keep last 3 days only)
spark.sql(f"""
DELETE FROM {TARGET_TABLE}
WHERE run_date < date_sub(current_date(), {RETENTION_DAYS - 1})
""")

# Detect first run (table empty)
is_first_run = spark.table(TARGET_TABLE).limit(1).count() == 0

if is_first_run:
    interval_start_expr = f"timestamp('{start_date} 00:00:00')"
    date_filter_expr = f"eHubQueuedDatestamp >= date_sub(current_date(), {RETENTION_DAYS - 1})"
else:
    spark.sql(f"DELETE FROM {TARGET_TABLE} WHERE run_date = current_date()")
    interval_start_expr = "timestamp(current_date())"
    date_filter_expr = "eHubQueuedDatestamp = current_date()"

intervals_df = (
    spark.sql(f"""
        SELECT explode(
            sequence(
                {interval_start_expr},
                current_timestamp(),
                interval 30 minutes
            )
        ) AS interval_start
    """)
    .withColumn("interval_end", F.expr("interval_start + interval 30 minutes"))
    .withColumn("run_date", F.to_date("interval_start"))
)

final_df = None

for table in TABLES:
    src_df = (
        spark.table(f"{SOURCE_SCHEMA}.{table}")
        .filter(date_filter_expr)
        .select("enqueuedTime")
    )

    agg_df = (
        intervals_df
        .join(
            src_df,
            (src_df.enqueuedTime >= intervals_df.interval_start) &
            (src_df.enqueuedTime < intervals_df.interval_end),
            "left"
        )
        .groupBy("run_date", "interval_start", "interval_end")
        .agg(F.count("enqueuedTime").alias("message_count"))
        .withColumn("message_type", F.lit(table))
        .withColumn(
            "is_sla_flag",
            F.when(F.col("message_count") < SLA_THRESHOLD, 1).otherwise(0)
        )
    )

    final_df = agg_df if final_df is None else final_df.unionByName(agg_df)

(
    final_df
    .write
    .mode("append")
    .format("delta")
    .saveAsTable(TARGET_TABLE)
)
