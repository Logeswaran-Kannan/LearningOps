from pyspark.sql import functions as F

# =========================
# Parameters
# =========================
SLA_THRESHOLD = 100
SOURCE_FLOW_TABLE = "core.flow.ehubstream"
SOURCE_MQS_TABLE  = "core.mqs_athn_mtr.ehubstream"
TARGET_TABLE      = "core_bld.default.monitoring_mqs_status_30min"

# =========================
# Create target table
# =========================
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TARGET_TABLE} (
  run_date        DATE,
  time_slice      STRING,
  status          STRING,
  mqs_quote_cnt   BIGINT,
  is_sla_breach   INT
)
USING DELTA
PARTITIONED BY (run_date)
""")

# =========================
# Define rolling 3-day window
# =========================
run_ts = F.current_timestamp()

window_start_ts = (
    F.date_trunc("day", run_ts) - F.expr("INTERVAL 2 DAYS")
)

# =========================
# Build valid interaction keys (SQL-equivalent logic)
# =========================
valid_ids_df = (
    spark.table(SOURCE_MQS_TABLE)
    .filter(F.col("eHubQueuedDatestamp") >= F.to_date(window_start_ts))
    .select(
        F.coalesce(
            F.col("headers.MQSQuoteID"),
            F.col("headers.externalID")
        ).alias("join_key")
    )
    .where(F.col("join_key").isNotNull())
    .distinct()
)

# =========================
# Read flow table (partition pruning via HubQueuedDatestamp)
# =========================
flow_df = (
    spark.table(SOURCE_FLOW_TABLE)
    .filter(
        (F.col("HubQueuedDatestamp") >= F.to_date(window_start_ts)) &
        (F.col("TimeEnqueued") <= run_ts)
    )
    .select(
        F.to_timestamp(F.get_json_object("payload", "$.timestamp")).alias("event_ts"),
        F.get_json_object("payload", "$.interaction.status").alias("status"),
        F.coalesce(
            F.get_json_object("payload", "$.interaction.mqsQuoteId"),
            F.get_json_object("payload", "$.interaction.id")
        ).alias("join_key")
    )
    .filter(F.col("event_ts").isNotNull())
)

# =========================
# Join (fast, SQL-correct)
# =========================
joined_df = flow_df.join(
    valid_ids_df,
    on="join_key",
    how="inner"
)

# =========================
# Fixed 30-minute bucketing from day start
# =========================
bucketed_df = (
    joined_df
    .withColumn("run_date", F.to_date("event_ts"))
    .withColumn(
        "time_slice",
        F.date_format(
            F.date_trunc("day", F.col("event_ts")) +
            F.expr(
                "INTERVAL 1 SECOND * (CAST((unix_timestamp(event_ts) - unix_timestamp(date_trunc('day', event_ts))) / 300 AS INT) * 300)"
            ),
            "HH:mm"
        )
    )
)

# =========================
# Aggregate + SLA
# =========================
final_df = (
    bucketed_df
    .groupBy("run_date", "time_slice", "status")
    .agg(F.count("*").alias("mqs_quote_cnt"))
    .withColumn(
        "is_sla_breach",
        F.when(F.col("mqs_quote_cnt") < F.lit(SLA_THRESHOLD), F.lit(1)).otherwise(F.lit(0))
    )
)

# =========================
# Overwrite only last 3 days
# =========================
(
    final_df
    .write
    .format("delta")
    .mode("overwrite")
    .option(
        "replaceWhere",
        "run_date >= current_date() - INTERVAL 2 DAYS"
    )
    .saveAsTable(TARGET_TABLE)
)
