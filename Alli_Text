from pyspark.sql import functions as F

# =========================
# Parameters
# =========================
SLA_THRESHOLD = 100   # <-- change SLA here
SOURCE_FLOW_TABLE = "core.flow.ehubstream"
SOURCE_MQS_TABLE  = "core.mqs_athn_mtr.ehubstream"
TARGET_TABLE      = "core_bld.default.monitoring_mqs_status_30min"

# =========================
# Create target table
# =========================
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TARGET_TABLE} (
  run_date        DATE,
  time_slice      STRING,
  status          STRING,
  mqs_quote_cnt   BIGINT,
  is_sla_breach   INT
)
USING DELTA
PARTITIONED BY (run_date)
""")

# =========================
# Detect first run
# =========================
is_first_run = spark.sql(
    f"SELECT COUNT(*) c FROM {TARGET_TABLE}"
).first()["c"] == 0

run_ts = F.current_timestamp()
today_start = F.date_trunc("day", run_ts)

# =========================
# Decide refresh window
# =========================
if is_first_run:
    refresh_start = today_start - F.expr("INTERVAL 2 DAYS")
    replace_where = "run_date >= current_date() - INTERVAL 2 DAYS"
else:
    refresh_start = today_start
    replace_where = "run_date = current_date()"

# =========================
# Read valid interaction IDs (JOIN instead of IN)
# =========================
valid_ids_df = (
    spark.table(SOURCE_MQS_TABLE)
    .filter(F.col("eHubQueuedDatestamp") >= F.to_date(refresh_start))
    .select(
        F.coalesce(
            F.col("headers.externalID"),
            F.col("headers.MQSQuoteID")
        ).alias("interaction_id")
    )
    .distinct()
)

# ===================
