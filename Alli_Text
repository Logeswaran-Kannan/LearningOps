# Databricks / PySpark: Flatten JSON column (CVR) into individual columns
# and filter rows by LOAD_DT between 2025-10-25 and 2025-10-30 (inclusive)

from pyspark.sql import functions as F
from pyspark.sql import types as T

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------
source_table = "your_catalog.your_schema.your_table"  # Replace with your actual table name
json_col = "CVR"
start_date = "2025-10-25"
end_date = "2025-10-30 23:59:59"

# ---------------------------------------------------------------------------
# Load source table and filter by LOAD_DT
# ---------------------------------------------------------------------------
df = spark.table(source_table)

# Ensure LOAD_DT is a timestamp and filter between date range
df = df.withColumn("LOAD_DT_TS", F.to_timestamp(F.col("LOAD_DT")))
df_filtered = df.where(
    (F.col("LOAD_DT_TS") >= F.to_timestamp(F.lit(start_date))) &
    (F.col("LOAD_DT_TS") <= F.to_timestamp(F.lit(end_date)))
)

# ---------------------------------------------------------------------------
# Parse JSON column CVR and flatten into individual columns
# ---------------------------------------------------------------------------
def infer_schema_from_sample(sample_df, json_col_name, limit_rows=10000):
    sample = (sample_df
              .select(F.col(json_col_name).cast("string").alias("_json"))
              .where(F.col("_json").isNotNull())
              .limit(limit_rows))
    if sample.rdd.isEmpty():
        return T.StructType([])
    return spark.read.json(sample.rdd.map(lambda r: r[0])).schema

parsed_col_name = "__parsed_json__"

src_field = next((f for f in df_filtered.schema.fields if f.name == json_col), None)
if src_field is None:
    raise ValueError(f"Column '{json_col}' not found in {source_table}.")

if isinstance(src_field.dataType, T.StringType):
    inferred_schema = infer_schema_from_sample(df_filtered, json_col)
    parsed_df = df_filtered.withColumn(parsed_col_name, F.from_json(F.col(json_col), inferred_schema))
elif isinstance(src_field.dataType, T.StructType):
    parsed_df = df_filtered.withColumn(parsed_col_name, F.col(json_col))
elif isinstance(src_field.dataType, (T.MapType, T.ArrayType)):
    json_str_df = df_filtered.withColumn("__json_str__", F.to_json(F.col(json_col)))
    inferred_schema = infer_schema_from_sample(json_str_df, "__json_str__")
    parsed_df = json_str_df.drop("__json_str__").withColumn(parsed_col_name, F.from_json(F.to_json(F.col(json_col)), inferred_schema))
else:
    json_str_df = df_filtered.withColumn("__json_str__", F.to_json(F.col(json_col)))
    inferred_schema = infer_schema_from_sample(json_str_df, "__json_str__")
    parsed_df = json_str_df.drop("__json_str__").withColumn(parsed_col_name, F.from_json(F.to_json(F.col(json_col)), inferred_schema))

# ---------------------------------------------------------------------------
# Flatten struct fields
# ---------------------------------------------------------------------------
def flatten_struct_columns(df_in, root_col):
    struct_field = next((f for f in df_in.schema.fields if f.name == root_col), None)
    if not struct_field or not isinstance(struct_field.dataType, T.StructType):
        return df_in

    def _expand(schema: T.StructType, prefix: str):
        out_cols = []
        for f in schema.fields:
            full_path = f"{prefix}{f.name}"
            alias = full_path.replace(".", "_")
            dt = f.dataType
            c = F.col(full_path)
            if isinstance(dt, T.StructType):
                out_cols.extend(_expand(dt, full_path + "."))
            elif isinstance(dt, (T.ArrayType, T.MapType)):
                out_cols.append(F.to_json(c).alias(alias))
            else:
                out_cols.append(c.alias(alias))
        return out_cols

    expanded_cols = _expand(struct_field.dataType, root_col + ".")
    base_cols = [F.col(c) for c in df_in.columns if c != root_col]
    return df_in.select(*base_cols, *expanded_cols)

flattened_df = flatten_struct_columns(parsed_df, parsed_col_name)
flattened_df = flattened_df.drop(parsed_col_name)

# ---------------------------------------------------------------------------
# Create a temp view for inspection
# ---------------------------------------------------------------------------
flattened_df.createOrReplaceTempView("flattened_json_out")

print("Temp view 'flattened_json_out' created with flattened CVR JSON column.")

# Optional: Display a few rows
try:
    display(flattened_df.limit(20))
except NameError:
    pass
