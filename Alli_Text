from pyspark.sql import functions as F

SLA_THRESHOLD = 100
TARGET_TABLE = "core_bld.default.monitoring_ehub_30min_metrics"
SOURCE_TABLE = "mqs_athn_mtr.ehubstream"

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TARGET_TABLE} (
  interval_start TIMESTAMP,
  interval_end   TIMESTAMP,
  message_type   STRING,
  message_count  BIGINT,
  interval_time  STRING,
  run_date       DATE,
  is_sla_breach  INT
)
USING DELTA
PARTITIONED BY (run_date)
""")

run_ts = F.current_timestamp()
start_ts = F.date_trunc("day", run_ts) - F.expr("INTERVAL 2 DAYS")

# ---------- Generate 30-min intervals for last 3 days (SMALL DF) ----------
intervals_df = (
    spark.range(0, 48 * 3)
    .select(
        (start_ts + F.expr("INTERVAL 1 SECOND") * (F.col("id") * 1800)).alias("interval_start")
    )
    .filter(F.col("interval_start") <= run_ts)
    .withColumn("interval_end", F.col("interval_start") + F.expr("INTERVAL 30 MINUTES"))
    .withColumn("interval_time", F.date_format("interval_start", "HH:mm"))
    .withColumn("run_date", F.to_date("interval_start"))
)

# ---------- Read streaming table as batch (ONLY last 3 days) ----------
src_df = (
    spark.table(SOURCE_TABLE)
    .filter(
        (F.col("TimeEnqueued") >= start_ts) &
        (F.col("TimeEnqueued") < run_ts)
    )
    .select(
        F.col("TimeEnqueued"),
        F.col("headers.messageType").alias("message_type")
    )
)

# ---------- Bucket once (cheap arithmetic) ----------
bucketed_df = (
    src_df
    .withColumn(
        "interval_start",
        F.date_trunc("day", F.col("TimeEnqueued")) +
        F.expr(
            "INTERVAL 1 SECOND * (CAST((unix_timestamp(TimeEnqueued) - unix_timestamp(date_trunc('day', TimeEnqueued))) / 1800 AS INT) * 1800)"
        )
    )
)

# ---------- Aggregate ----------
agg_by_type_df = (
    bucketed_df
    .groupBy("interval_start", "message_type")
    .agg(F.count("*").alias("message_count"))
)

agg_all_df = (
    bucketed_df
    .groupBy("interval_start")
    .agg(F.count("*").alias("message_count"))
    .withColumn("message_type", F.lit("ALL"))
)

agg_df = agg_by_type_df.unionByName(agg_all_df)

# ---------- Join intervals (no gaps) ----------
final_df = (
    intervals_df
    .join(agg_df, on="interval_start", how="left")
    .fillna({"message_count": 0})
    .withColumn(
        "is_sla_breach",
        F.when(F.col("message_count") < SLA_THRESHOLD, F.lit(1)).otherwise(F.lit(0))
    )
    .select(
        "interval_start",
        "interval_end",
        "message_type",
        "message_count",
        "interval_time",
        "run_date",
        "is_sla_breach"
    )
)

# ---------- Overwrite only last 3 days ----------
(
    final_df
    .write
    .format("delta")
    .mode("overwrite")
    .option("replaceWhere", "run_date >= current_date() - INTERVAL 2 DAYS")
    .saveAsTable(TARGET_TABLE)
)
