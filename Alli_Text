from pyspark.sql import functions as F

# Read the table
df = spark.table("core_rl_test.flow.ehubstream")

# Calculate payload size in bytes
# length() = number of characters
# If payload is UTF-8 JSON, this is usually acceptable for size comparison
df_with_size = df.withColumn(
    "payload_size_bytes",
    F.length(F.col("payload"))
)

# Derive year-month from enduedate
df_with_month = df_with_size.withColumn(
    "year_month",
    F.date_format(F.col("enduedate"), "yyyy-MM")
)

# Aggregate min and max payload size per month
payload_size_stats = df_with_month.groupBy("year_month").agg(
    F.min("payload_size_bytes").alias("min_payload_size_bytes"),
    F.max("payload_size_bytes").alias("max_payload_size_bytes"),
    F.count("*").alias("record_count")
).orderBy("year_month")

# Display results
display(payload_size_stats)
