# ------------------------------------------------
# 1. Load table
# ------------------------------------------------
df = spark.table("core_tst.mqs_athn_mtr.dlt_raw_enrichments")

from pyspark.sql.functions import *
from pyspark.sql.types import *
import base64, snappy

# ------------------------------------------------
# 2. Convert VARIANT → JSON string
# ------------------------------------------------
df1 = df.withColumn("payload_json", col("Payload").cast("STRING"))

# ------------------------------------------------
# 3. Extract rawEnrichments JSON array from VARIANT
# ------------------------------------------------
df2 = df1.withColumn(
    "enrichments_json",
    get_json_object(col("payload_json"), "$.rawEnrichments")
)

# ------------------------------------------------
# 4. Convert rawEnrichments (string) → Array<STRING>
#     (we don't know schema, so read as string array)
# ------------------------------------------------
df3 = df2.withColumn(
    "enrichments",
    from_json(col("enrichments_json"), ArrayType(StringType()))
)

# ------------------------------------------------
# 5. Explode enrichment items (still JSON strings)
# ------------------------------------------------
df4 = df3.withColumn("enrich_raw", explode_outer(col("enrichments")))

# ------------------------------------------------
# 6. Extract compression + data fields from JSON string WITHOUT schema
# ------------------------------------------------
df5 = df4.withColumn("compression", get_json_object(col("enrich_raw"), "$.compression")) \
         .withColumn("data",        get_json_object(col("enrich_raw"), "$.data"))

# ------------------------------------------------
# 7. Snappy decompress UDF (blindly, no schema)
# ------------------------------------------------
@udf(StringType())
def snappy_decompress(b64_string):
    if not b64_string:
        return None
    try:
        raw = base64.b64decode(b64_string)
        out = snappy.uncompress(raw)
        return out.decode("utf-8")
    except:
        return None

df6 = df5.withColumn(
    "decompressed_json",
    when(col("compression") == "snappy", snappy_decompress(col("data")))
    .otherwise(None)
)

# ------------------------------------------------
# 8. Infer schema dynamically from decompressed JSON
# ------------------------------------------------
sample = df6.select("decompressed_json").where("decompressed_json IS NOT NULL").limit(1).collect()

if sample and sample[0].decompressed_json:
    inferred_schema = spark.read.json(
        spark.sparkContext.parallelize([sample[0].decompressed_json])
    ).schema
else:
    inferred_schema = None

print("=== Inferred Schema ===")
print(inferred_schema)

# ------------------------------------------------
# 9. Parse decompressed JSON using inferred schema
# ------------------------------------------------
if inferred_schema:
    df7 = df6.withColumn(
        "parsed",
        from_json(col("decompressed_json"), inferred_schema)
    )
else:
    df7 = df6

# ------------------------------------------------
# 10. Flatten final output
# ------------------------------------------------
final_df = df7.select(
    "TimeEnqueued",
    "eHubQueuedDatestamp",
    "compression",
    "data",
    "decompressed_json",
    col("parsed.*")
)

final_df.display()
