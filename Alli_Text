from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType, BinaryType
import base64
import snappy
import json

# 1. Load the table
df = spark.table("core_tst.mqs_athn_mtr.dlt_raw_enrichments")

# 2. Extract the base64-snappy encoded payload string
df_extracted = df.selectExpr("Payload.rawEnrichment.data as compressed_data")

# 3. UDF to decode and decompress the snappy data
def decode_and_decompress(base64_str):
    try:
        decoded = base64.b64decode(base64_str)
        decompressed = snappy.uncompress(decoded)
        return decompressed.decode('utf-8')
    except Exception as e:
        return None

decode_udf = udf(decode_and_decompress, StringType())

df_decompressed = df_extracted.withColumn("json_string", decode_udf(col("compressed_data")))

# 4. Filter out failed decompression (if any)
df_valid = df_decompressed.filter(col("json_string").isNotNull())

# 5. Infer schema dynamically from JSON string
json_df = spark.read.json(df_valid.rdd.map(lambda row: row['json_string']))

# Optional: cache if large
json_df.cache()

# 6. Flatten nested structure (adjust depth as needed)
from pyspark.sql.functions import explode_outer

flat_df = json_df.select("*")  # start with top-level
flat_df.display()
