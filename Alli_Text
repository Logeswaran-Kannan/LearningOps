import base64
import snappy
import json
from pyspark.sql.functions import *
from pyspark.sql.types import *

# ----------------------------------------------------
# 1. UDF – Decode Base64 → Snappy → JSON string
# ----------------------------------------------------
def decode_snappy_base64(b64_str):
    if b64_str is None:
        return None
    try:
        raw = base64.b64decode(b64_str)
        decompressed = snappy.decompress(raw)
        return decompressed.decode("utf-8")
    except:
        return None

decode_udf = udf(decode_snappy_base64, StringType())


# ----------------------------------------------------
# 2. Load Your Table
# ----------------------------------------------------
table_name = "core_tst.mqs_athn_mtr.dlt_raw_enrichments"
df = spark.table(table_name)


# ----------------------------------------------------
# 3. Decode Payload.data into JSON string
# ----------------------------------------------------
df_decoded = df.withColumn(
    "payload_json",
    decode_udf(col("Payload.data"))
)


# ----------------------------------------------------
# 4. Infer schema dynamically from JSON sample
# ----------------------------------------------------
sample_json = df_decoded.select("payload_json").dropna().limit(1).collect()[0][0]
json_schema = spark.read.json(spark.sparkContext.parallelize([sample_json])).schema


# ----------------------------------------------------
# 5. Convert JSON string → struct using inferred schema
# ----------------------------------------------------
df_struct = df_decoded.withColumn(
    "payload_struct",
    from_json(col("payload_json"), json_schema)
)


# ----------------------------------------------------
# 6. Deep flatten function (recursive)
# ----------------------------------------------------
def flatten_recursive(df):
    # Find top-level columns that need flattening
    complex_cols = [
        (c.name, c.dataType) for c in df.schema.fields
        if isinstance(c.dataType, (StructType, ArrayType, MapType))
    ]

    while complex_cols:
        col_name, col_type = complex_cols.pop(0)

        # ---- StructType: expand into separate columns
        if isinstance(col_type, StructType):
            expanded = [col(f"{col_name}.{f.name}").alias(f"{col_name}_{f.name}") for f in col_type.fields]
            df = df.select("*", *expanded).drop(col_name)

        # ---- ArrayType: explode into rows (if needed)
        elif isinstance(col_type, ArrayType):
            df = df.withColumn(col_name, explode_outer(col(col_name)))

        # ---- MapType: explode keys into columns
        elif isinstance(col_type, MapType):
            df = df.select("*", *[
                col(col_name).getItem(k).alias(f"{col_name}_{k}")
                for k in df.select(map_keys(col(col_name))).dropna().limit(1).collect()[0][0]
            ]).drop(col_name)

        # Recompute fields list
        complex_cols = [
            (c.name, c.dataType) for c in df.schema.fields
            if isinstance(c.dataType, (StructType, ArrayType, MapType))
        ]

    return df


# ----------------------------------------------------
# 7. Apply deep flattening on payload_struct
# ----------------------------------------------------
df_flat = flatten_recursive(df_struct.drop("payload_json"))


# ----------------------------------------------------
# 8. Clean column names (remove dots, spaces)
# ----------------------------------------------------
for c in df_flat.columns:
    df_flat = df_flat.withColumnRenamed(c, c.replace(".", "_").replace(" ", "_"))


# ----------------------------------------------------
# 9. Show result
# ----------------------------------------------------
df_flat.display()
