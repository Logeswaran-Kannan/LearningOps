from pyspark.sql import functions as F
from pyspark.sql.types import StringType, MapType
import base64
import snappy

# ------------------------------------------------------------
# UDF: Base64 decode + Snappy decompress
# ------------------------------------------------------------
def decompress_snappy(b64):
    if b64 is None:
        return None
    try:
        decoded = base64.b64decode(b64)
        uncompressed = snappy.decompress(decoded)
        return uncompressed.decode("utf-8")
    except Exception as e:
        return f"ERROR: {str(e)}"

decompress_udf = F.udf(decompress_snappy, StringType())

# ------------------------------------------------------------
# LOAD TABLE WITH VARIANT
# ------------------------------------------------------------
df = spark.table("core_tst.mqs_athn_mtr.dlt_raw_enrichments")

# ------------------------------------------------------------
# Extract snappy Base64 from VARIANT
# Variant → struct → field access works directly
# ------------------------------------------------------------
df2 = df.withColumn(
    "snappy_data",
    F.col("Payload.rawEnrichment.data")     # accessing VARIANT field
).withColumn(
    "decompressed_json",
    decompress_udf("snappy_data")           # apply decoder
)

df2.select("TimeEnqueued", "eHubQueuedDatestamp", "decompressed_json").show(truncate=False)

# ------------------------------------------------------------
# Convert decompressed JSON text to struct/map
# ------------------------------------------------------------
df3 = df2.withColumn(
    "json_map",
    F.from_json("decompressed_json", MapType(StringType(), StringType()))
)

# ------------------------------------------------------------
# Flatten JSON into columns (if JSON is a simple map)
# For nested JSON we use json_tuple or schema infer later
# ------------------------------------------------------------
df3.select(
    "TimeEnqueued",
    "eHubQueuedDatestamp",
    "json_map.*"
).show(truncate=False)
