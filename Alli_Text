from pyspark.sql import functions as F

# ==========================================================
# 1. Load Tables
# ==========================================================

A = spark.table("core_bld_radar_live.dlt_bronze_recon_athena_source") \
        .withColumn("requestDate", F.to_date("requestDate"))

B = spark.table("core_bld_radar_live.dlt_bronze_recon_athena_stream")

C = spark.table("core_bld_radar_live.dlt_bronze_recon_mqs")


# ==========================================================
# 2. Filter by Date Range
# ==========================================================

start_date = "2025-12-02"
end_date   = "2025-12-09"

A = A.filter((F.col("requestDate") >= start_date) & (F.col("requestDate") <= end_date))
B = B.filter(F.col("athenaStreamQuoteDateTime").isNotNull())
C = C.filter(F.col("mQSDateTime").isNotNull())


# ==========================================================
# 3. Normalise Keys
# ==========================================================

A_norm = A.withColumn("qid", F.lower(F.trim("mqsQuoteId"))) \
          .withColumn("iid", F.lower(F.trim("interactionId")))

B_norm = B.withColumn("qid", F.lower(F.trim("athenaStreamMqsQuoteId"))) \
          .withColumn("iid", F.lower(F.trim("athenaStreamInteractionId"))) \
          .withColumn("b_ts", F.col("athenaStreamQuoteDateTime"))

C_norm = C.withColumn("qid", F.lower(F.trim("mqsQuoteId"))) \
          .withColumn("iid", F.lower(F.trim("mQSInteractionId"))) \
          .withColumn("c_ts", F.col("mQSDateTime"))


# ==========================================================
# 4. A → B Matching (48 hours)
# ==========================================================

A_B = (
    A_norm.alias("a")
    .join(B_norm.alias("b"), on=["qid", "iid"], how="left")
    .withColumn(
        "diff_hours_B",
        F.abs(F.unix_timestamp("b.b_ts") - F.unix_timestamp("a.requestDateTime")) / 3600
    )
    .withColumn("hit_B", F.when(F.col("diff_hours_B") <= 48, 1).otherwise(0))
    .select(
        "a.qid",
        "a.product",
        "a.mqsQuoteId",
        "a.interactionId",
        "a.requestDate",
        "a.requestDateTime",
        "diff_hours_B",
        "hit_B"
    )
)


# ==========================================================
# 5. A → C Matching (48 hours)
# ==========================================================

A_C = (
    A_norm.alias("a")
    .join(C_norm.alias("c"), on=["qid", "iid"], how="left")
    .withColumn(
        "diff_hours_C",
        F.abs(F.unix_timestamp("c.c_ts") - F.unix_timestamp("a.requestDateTime")) / 3600
    )
    .withColumn("hit_C", F.when(F.col("diff_hours_C") <= 48, 1).otherwise(0))
    .select(
        "a.qid",
        "a.product",
        "a.mqsQuoteId",
        "a.interactionId",
        "a.requestDate",
        "a.requestDateTime",
        "diff_hours_C",
        "hit_C"
    )
)

# ==========================================================
# 6. KPI Aggregations (Date + Product Level)
# ==========================================================


# ----------------------------
# A KPIs (per date + product)
# ----------------------------
A_kpi = A_norm.groupBy("requestDate", "product").agg(
    F.countDistinct("mqsQuoteId").alias("A_total"),
    F.sum(F.when(F.col("mqsQuoteId").isNull(), 1).otherwise(0)).alias("A_null_count"),
    F.sum(F.when(~F.col("mqsQuoteId").startsWith("MQS"), 1).otherwise(0)).alias("A_non_mqs_prefix")
)


# ----------------------------
# B KPIs (per date + product)
# ----------------------------
B_kpi = B_norm.withColumn("b_date", F.to_date("b_ts")) \
    .join(A_norm.select("product", "qid").dropDuplicates(), on="qid", how="left") \
    .groupBy("b_date", "product") \
    .agg(
        F.countDistinct("qid").alias("B_total"),
        F.sum(F.when(F.col("athenaStreamMqsQuoteId").isNull(), 1).otherwise(0)).alias("B_null_count"),
        F.sum(F.when(~F.col("athenaStreamMqsQuoteId").startsWith("MQS"), 1).otherwise(0)).alias("B_non_mqs_prefix")
    ).withColumnRenamed("b_date", "requestDate")


# ----------------------------
# C KPIs (per date + product)
# ----------------------------
C_kpi = C_norm.withColumn("c_date", F.to_date("c_ts")) \
    .join(A_norm.select("product", "qid").dropDuplicates(), on="qid", how="left") \
    .groupBy("c_date", "product") \
    .agg(
        F.countDistinct("qid").alias("C_total"),
        F.sum(F.when(F.col("mqsQuoteId").isNull(), 1).otherwise(0)).alias("C_null_count"),
        F.sum(F.when(~F.col("mqsQuoteId").startsWith("MQS"), 1).otherwise(0)).alias("C_non_mqs_prefix")
    ).withColumnRenamed("c_date", "requestDate")


# ----------------------------
# A → B Hit KPIs
# ----------------------------
A_hit_B_kpi = A_B.groupBy("requestDate", "product").agg(
    F.countDistinct(F.when(F.col("hit_B") == 1, F.col("mqsQuoteId"))).alias("A_hit_B")
)

# ----------------------------
# A → C Hit KPIs
# ----------------------------
A_hit_C_kpi = A_C.groupBy("requestDate", "product").agg(
    F.countDistinct(F.when(F.col("hit_C") == 1, F.col("mqsQuoteId"))).alias("A_hit_C")
)

# ----------------------------
# A → Both B and C
# ----------------------------
A_both = (
    A_B.select("qid", "product", "requestDate", "hit_B")
    .join(A_C.select("qid", "product", "requestDate", "hit_C"), on=["qid", "product", "requestDate"])
    .filter((F.col("hit_B") == 1) & (F.col("hit_C") == 1))
    .groupBy("requestDate", "product")
    .agg(F.countDistinct("qid").alias("A_hit_both"))
)


# ==========================================================
# 7. Combine All KPIs + Reconciliation Score
# ==========================================================

final_kpi = (
    A_kpi
    .join(B_kpi, ["requestDate", "product"], "left")
    .join(C_kpi, ["requestDate", "product"], "left")
    .join(A_hit_B_kpi, ["requestDate", "product"], "left")
    .join(A_hit_C_kpi, ["requestDate", "product"], "left")
    .join(A_both, ["requestDate", "product"], "left")
    .withColumn("Reconciliation_Score_Percent",
                F.round((F.col("A_hit_both") / F.col("A_total")) * 100, 2))
    .orderBy("requestDate", "product")
)

display(final_kpi)
