from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType
import base64
import snappy
import json

# STEP 1 — Load your table
df = spark.table("core_tst.mqs_athn_mtr.dlt_raw_enrichments")

# STEP 2 — Convert entire VARIANT column to string
df_json = df.withColumn("payload_str", col("Payload").cast("string"))

# STEP 3 — Extract the rawEnrichment.data using get_json_object
from pyspark.sql.functions import get_json_object

df_extracted = df_json.withColumn(
    "compressed_data",
    get_json_object("payload_str", "$.rawEnrichment.data")
)

# STEP 4 — Define UDF to decode base64 and decompress Snappy
def decode_snappy(base64_str):
    try:
        decoded = base64.b64decode(base64_str)
        uncompressed = snappy.uncompress(decoded)
        return uncompressed.decode("utf-8")
    except Exception as e:
        return None

decode_udf = udf(decode_snappy, StringType())

# STEP 5 — Apply decompression
df_decompressed = df_extracted.withColumn(
    "json_string",
    decode_udf(col("compressed_data"))
)

# STEP 6 — Filter rows where decompression succeeded
df_valid = df_decompressed.filter(col("json_string").isNotNull())

# STEP 7 — Dynamically infer schema of JSON and flatten
json_df = spark.read.json(df_valid.rdd.map(lambda row: row["json_string"]))

# STEP 8 — Show result
json_df.display()
