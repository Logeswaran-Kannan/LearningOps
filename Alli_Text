from pyspark.sql import functions as F

SLA_THRESHOLD = 100
TARGET_TABLE = "core_bld.default.monitoring_ehub_30min_metrics"
SOURCE_TABLE = "mqs_athn_mtr.ehubstream"

# ---------- Create table if not exists ----------
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {TARGET_TABLE} (
  interval_start TIMESTAMP,
  interval_end   TIMESTAMP,
  message_type   STRING,
  message_count  BIGINT,
  interval_time  STRING,
  run_date       DATE,
  is_sla_breach  INT
)
USING DELTA
PARTITIONED BY (run_date)
""")

# ---------- Check if table already has data ----------
is_first_run = spark.sql(f"SELECT COUNT(*) c FROM {TARGET_TABLE}").first()["c"] == 0

run_ts = F.current_timestamp()
today_start = F.date_trunc("day", run_ts)

# ---------- Decide refresh window ----------
if is_first_run:
    refresh_start = today_start - F.expr("INTERVAL 2 DAYS")
    replace_where = "run_date >= current_date() - INTERVAL 2 DAYS"
else:
    refresh_start = today_start
    replace_where = "run_date = current_date()"

# ---------- Generate 30-min intervals ----------
intervals_df = (
    spark.range(0, 48 * 3)
    .select(
        (refresh_start + F.expr("INTERVAL 1 SECOND") * (F.col("id") * 1800)).alias("interval_start")
    )
    .filter(F.col("interval_start") <= run_ts)
    .withColumn("interval_end", F.col("interval_start") + F.expr("INTERVAL 30 MINUTES"))
    .withColumn("interval_time", F.date_format("interval_start", "HH:mm"))
    .withColumn("run_date", F.to_date("interval_start"))
    .filter(F.col("run_date") >= F.to_date(refresh_start))
)

# ---------- Read streaming source (minimal scan) ----------
src_df = (
    spark.table(SOURCE_TABLE)
    .filter(
        (F.col("TimeEnqueued") >= refresh_start) &
        (F.col("TimeEnqueued") < run_ts)
    )
    .select(
        F.col("TimeEnqueued"),
        F.col("headers.messageType").alias("message_type")
    )
)

# ---------- Bucket events into 30-min slots ----------
bucketed_df = (
    src_df
    .withColumn(
        "interval_start",
        F.date_trunc("day", F.col("TimeEnqueued")) +
        F.expr(
            "INTERVAL 1 SECOND * (CAST((unix_timestamp(TimeEnqueued) - unix_timestamp(date_trunc('day', TimeEnqueued))) / 1800 AS INT) * 1800)"
        )
    )
)

# ---------- Aggregations ----------
agg_by_type_df = (
    bucketed_df
    .groupBy("interval_start", "message_type")
    .agg(F.count("*").alias("message_count"))
)

agg_all_df = (
    bucketed_df
    .groupBy("interval_start")
    .agg(F.count("*").alias("message_count"))
    .withColumn("message_type", F.lit("ALL"))
)

agg_df = agg_by_type_df.unionByName(agg_all_df)

# ---------- Join with intervals (no gaps) ----------
final_df = (
    intervals_df
    .join(agg_df, on="interval_start", how="left")
    .fillna({"message_count": 0})
    .withColumn(
        "is_sla_breach",
        F.when(F.col("message_count") < SLA_THRESHOLD, F.lit(1)).otherwise(F.lit(0))
    )
    .select(
        "interval_start",
        "interval_end",
        "message_type",
        "message_count",
        "interval_time",
        "run_date",
        "is_sla_breach"
    )
)

# ---------- Overwrite only affected days ----------
(
    final_df
    .write
    .format("delta")
    .mode("overwrite")
    .option("replaceWhere", replace_where)
    .saveAsTable(TARGET_TABLE)
)
