from pyspark.sql import functions as F

target_table = "core_bld.default.monitoring_mqs_interaction_sta"

# ----------------------------------------
# 1. Date boundaries (last 3 days)
# ----------------------------------------
start_date = F.date_sub(F.current_date(), 2)

# ----------------------------------------
# 2. FLOW stream (last 3 days)
# ----------------------------------------
flow_df = (
    spark.table("core.flow.ehubstream")
    .filter(
        (F.col("eHubQueuedDatestamp") >= start_date) &
        (F.col("eHubQueuedDatestamp") <= F.current_date())
    )
    .select(
        F.to_date(F.get_json_object("payload", "$.timestamp")).alias("run_date"),
        F.to_timestamp(F.get_json_object("payload", "$.timestamp")).alias("event_ts"),
        F.get_json_object("payload", "$.interaction.status").alias("status"),
        F.get_json_object("payload", "$.interaction.mqsQuoteId").alias("mqs_quote_id"),
        F.get_json_object("payload", "$.interaction.id").alias("interaction_id"),
    )
    .filter(
        F.col("event_ts") >= F.date_trunc("day", F.date_sub(F.current_date(), 2))
    )
)

# ----------------------------------------
# 3. MTR stream (last 3 days)
# ----------------------------------------
mtr_df = (
    spark.table("core.mqs_atnh_mtr.ehubstream")
    .filter(
        F.to_date("enqueuedTime") >= start_date
    )
    .select(
        F.col("headers")["mqsQuoteId"].alias("mqs_quote_id"),
        F.col("headers")["externalId"].alias("interaction_id"),
    )
    .dropDuplicates()
)

# ----------------------------------------
# 4. Join (multi-column, fastest)
# ----------------------------------------
joined_df = (
    flow_df.join(
        mtr_df,
        on=["mqs_quote_id", "interaction_id"],
        how="inner"
    )
    .select(
        "run_date",
        F.from_unixtime(
            (F.unix_timestamp("event_ts") / 1800).cast("long") * 1800
        ).alias("interval_start"),
        "status",
        "mqs_quote_id",
    )
)

# ----------------------------------------
# 5. Aggregate + SLA breach flag
# ----------------------------------------
final_df = (
    joined_df
    .groupBy("run_date", "interval_start", "status")
    .agg(
        F.count("mqs_quote_id").alias("total_count")
    )
    .withColumn(
        "is_sla_breach",
        F.when(F.col("total_count") < 3, F.lit(1)).otherwise(F.lit(0))
    )
)

# ----------------------------------------
# 6. Create target table if not exists
# ----------------------------------------
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {target_table} (
    run_date DATE,
    interval_start TIMESTAMP,
    status STRING,
    total_count BIGINT,
    is_sla_breach INT
)
USING DELTA
PARTITIONED BY (run_date)
""")

# ----------------------------------------
# 7. FULL REFRESH (overwrite all 3 days)
# ----------------------------------------
(
    final_df
    .write
    .format("delta")
    .mode("overwrite")
    .option("overwriteSchema", "true")
    .saveAsTable(target_table)
)
