from pyspark.sql import functions as F

# --- Configuration ---
table_name = "catalog.schema.my_table"   # Replace with your Unity Catalog path
column_name = "payload"                  # Column to search
search_pattern = "error_404"             # Text or regex pattern to search for

# --- Load Table Lazily ---
df = spark.table(table_name)

# --- Filter for Matching Rows ---
# ✅ Option 1: Faster substring match
filtered_df = df.filter(F.col(column_name).contains(search_pattern))

# ✅ Option 2: Regex pattern match (slower, but flexible)
# filtered_df = df.filter(F.col(column_name).rlike(search_pattern))

# --- ✅ Cache or Persist the DataFrame if Reusing ---
# Improves performance when you plan to query, count, or display multiple times
filtered_df.cache()
filtered_df.count()  # Action to trigger caching

# --- Display Matching Rows ---
display(filtered_df)

# --- (Optional) Count for quick check ---
match_count = filtered_df.count()
print(f"✅ Found {match_count} rows containing '{search_pattern}' in column '{column_name}'")
