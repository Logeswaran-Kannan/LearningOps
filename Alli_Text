from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, schema_of_json, explode_outer
from pyspark.sql.types import StructType

# Start Spark session
spark = SparkSession.builder.getOrCreate()

# Load table
df = spark.table("core_tst_mqs_athn_mtr_ehubstream")

# Step 1: Filter by messageType = 'precomp-response' (case-insensitive)
df_filtered = df.filter(col("headers")["messageType"].rlike("(?i)^precomp-response$"))

# Step 2: Get dynamic schema of Payload using a sample
sample_payload = df_filtered.select("Payload").filter(col("Payload").isNotNull()).limit(1).collect()[0]["Payload"]
payload_schema = schema_of_json(sample_payload)

# Step 3: Parse Payload JSON into struct
df_parsed = df_filtered.withColumn("payload_json", from_json(col("Payload"), payload_schema))

# Step 4: Recursive flattening function
def flatten_df(nested_df, prefix=""):
    flat_cols = []
    nested_cols = []
    
    for field in nested_df.schema.fields:
        field_name = field.name
        field_type = field.dataType
        full_name = f"{prefix}.{field_name}" if prefix else field_name

        if isinstance(field_type, StructType):
            nested_cols.append((full_name, field_type))
        elif "Array" in str(field_type):  # Handle arrays with explode_outer
            nested_df = nested_df.withColumn(full_name.replace(".", "_"), explode_outer(col(full_name)))
            flat_cols.append(full_name.replace(".", "_"))
        else:
            flat_cols.append(full_name)

    flat_df = nested_df.select([col(c).alias(c.replace(".", "_")) for c in flat_cols])
    
    for path, typ in nested_cols:
        flat_df = flatten_df(flat_df.withColumn(path.replace(".", "_"), col(path)), prefix="")

    return flat_df

# Step 5: Flatten the payload_json column
df_with_payload = df_parsed.select("headers", "TimeEnqueued", "payload_json")
flattened_payload_df = flatten_df(df_with_payload.select("payload_json"))
final_df = df_with_payload.select("headers", "TimeEnqueued").join(flattened_payload_df)

# Step 6: Display or write output
final_df.show(truncate=False)
# final_df.write.mode("overwrite").saveAsTable("flattened_precomp_response_payload")
