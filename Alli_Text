from pyspark.sql import functions as F
from pyspark.sql.types import StructType, ArrayType

# 1. Load the base table
df = spark.table("core_tst_mqs_athn_mtr_ehubstream")

# 2. Filter on headers.messageType == 'precomp-response'
filtered_df = df.filter(F.lower(F.col("headers")["messageType"]) == "precomp-response")

# 3. Infer schema from sample record
sample_payload = filtered_df.select("Payload").filter(F.col("Payload").isNotNull()).limit(1).collect()[0]["Payload"]
inferred_schema = spark.read.json(spark.createDataset([sample_payload])).schema

# 4. Parse the Payload JSON
parsed_df = filtered_df.withColumn("parsed_payload", F.from_json("Payload", inferred_schema))

# -------------------------------
# 5. Recursive flattening function
# -------------------------------
def flatten_df(nested_df, parent_name=""):
    flat_cols = []
    for field in nested_df.schema.fields:
        field_name = field.name
        col_name = f"{parent_name}_{field_name}" if parent_name else field_name
        dtype = field.dataType

        if isinstance(dtype, StructType):
            # Recurse into Struct
            flat_df = nested_df.withColumn(field_name, F.col(field_name))
            nested = flatten_df(flat_df.select(F.col(field_name + ".*")), col_name)
            flat_cols.extend(nested.schema.fields)
            nested_df = nested_df.select("*", *[nested[col.name] for col in nested.schema.fields]).drop(field_name)

        elif isinstance(dtype, ArrayType) and isinstance(dtype.elementType, StructType):
            # Explode and flatten Array of Struct
            exploded = nested_df.withColumn(field_name, F.explode_outer(field_name))
            nested = flatten_df(exploded.select(F.col(field_name + ".*")), col_name)
            flat_cols.extend(nested.schema.fields)
            nested_df = exploded.select("*", *[nested[col.name] for col in nested.schema.fields]).drop(field_name)

        else:
            nested_df = nested_df.withColumnRenamed(field_name, col_name)
    return nested_df

# 6. Flatten the parsed payload
flattened_payload_df = flatten_df(parsed_df.select("parsed_payload"))

# 7. Combine with original metadata (TimeEnqueued, headers if needed)
final_df = parsed_df.select("TimeEnqueued", "headers").join(flattened_payload_df)

# 8. Show result
final_df.show(truncate=False)
# Optionally save:
# final_df.write.mode("overwrite").saveAsTable("flattened_payload_dynamic")
