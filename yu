from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("JSONProcessor").getOrCreate()

# Sample DataFrame with nested JSON in the 'body' column
data = [
    (1, '{"id":1, "name":"Parent", "children":[{"id":11, "name":"Child1"},{"id":12, "name":"Child2"}]}'),
    # Add more rows as needed
]

schema = ["id", "body"]
df = spark.createDataFrame(data, schema=schema)

# Extract the 'children' array using getField
children_df = df.select("id", col("body").getField("children").alias("children"))

# Explode the 'children' array to create separate rows for each child
children_df = children_df.select("id", explode_outer("children").alias("child"))

# Extract specific fields from the 'children' structure
children_df = children_df.select(
    "id",
    "child.id".alias("child_id"),
    "child.name".alias("child_name")
)

# Show the content of the 'children' DataFrame
children_df.show(truncate=False)

# Stop the Spark session
spark.stop()
