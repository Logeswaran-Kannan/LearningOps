from pyspark.sql import SparkSession

# Define Azure Synapse SQL pool connection details
synapse_server = "your_synapse_server"
database = "your_database"
user = "your_username"
password = "your_password"
jdbc_url = f"jdbc:sqlserver://{synapse_server}:1433;database={database};user={user};password={password};"

# Define the list of tables to copy
input_tables = ["table1", "table2", "table3"]  # Replace with your actual table names

for synapse_table in input_tables:
    # Extract table name without the prefix and add '_VC' suffix
    databricks_table = synapse_table.replace("mrt_owner.", "") + "_VC"
    
    synapse_df = spark.read.format("jdbc").option("url", jdbc_url).option("dbtable", synapse_table).load()
    
    # Write to Databricks default schema with the modified table name and overwrite mode
    synapse_df.write.format("delta").mode("overwrite").saveAsTable(databricks_table)

# Stop the Spark session
spark.stop()
