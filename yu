from pyspark.sql import SparkSession
from pyspark.sql.functions import col

def get_table_counts(database_name):
    # Assuming 'spark' is your SparkSession
    # Replace 'your_table' with the actual table name
    table_counts = spark.sql(f"SELECT COUNT(*) AS count FROM {database_name}.your_table")
    return table_counts

def filter_tables_with_conditions(database_name):
    # Assuming 'spark' is your SparkSession
    # Replace 'your_table' with the actual table name
    tables = spark.sql(f"SHOW TABLES IN {database_name}").filter(~col("tableName").startswith("--") & ~col("tableName").endswith("temp"))
    return tables

def main(database_list):
    for database_name in database_list:
        tables = filter_tables_with_conditions(database_name)
        
        for table_row in tables.collect():
            table_name = table_row['tableName']
            
            table_counts = get_table_counts(f"{database_name}.{table_name}")
            count = table_counts.collect()[0]['count']
            
            # Assuming 'comments' is the column in the table with comments
            comments = spark.sql(f"DESCRIBE {database_name}.{table_name} comments").collect()[0]['comments']
            
            if count > 0 or comments != "no data":
                # Print or store the output in your preferred format (HTML in this case)
                print(f"<tr><td>{database_name}</td><td>{table_name}</td><td>{count}</td><td>{comments}</td></tr>")

# Example usage
database_list = ['database1', 'database2']
main(database_list)
