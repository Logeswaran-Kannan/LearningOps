from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("DuplicateCheck").getOrCreate()

# Read CSV sheet with tablename and grain_column_list
csv_path = "/path/to/your/csv/sheet.csv"  # Replace with the actual path
csv_df = spark.read.option("header", "true").csv(csv_path)

# Iterate through each row in the CSV
for row in csv_df.rdd.collect():
    tablename = row["tablename"]
    grain_column_list = row["grain_column_list"].split(",")  # Assuming grain_column_list is a comma-separated string

    # Read data from the table in MRT database with filter condition
    table_df = spark.read.format("delta").table(f"MRT.{tablename}").filter(col("endat").isNull())

    # Check for duplicates based on the grain_column_list
    duplicate_count = table_df.groupBy(grain_column_list).count().filter("count > 1").count()

    # Display results
    print(f"Duplicate check for table {tablename} with grain columns {grain_column_list}: {duplicate_count} duplicate rows found.")

# Stop Spark session
spark.stop()
