from pyspark.sql import SparkSession
from pyspark.sql.functions import col

def count_tables(spark, database_name):
    # Switch to the specified database
    spark.sql(f"USE {database_name}")

    # Get the list of tables in the current database
    tables = spark.sql("SHOW TABLES").select("tableName").rdd.flatMap(lambda x: x).collect()

    # Filter tables based on criteria (exclude table names)
    filtered_tables = [table for table in tables if not table.startswith('--') and not table.endswith('temp') and not table.startswith('aren')]

    # Initialize an empty list to store results
    results = []

    # Iterate through the filtered tables and count the rows
    for table in filtered_tables:
        count = spark.sql(f"SELECT COUNT(*) as count FROM {table}").first()["count"]
        comments = spark.sql(f"DESCRIBE EXTENDED {table}").filter("col_name = 'Comment'").select("data_type").first()["data_type"]
        results.append((database_name, table, count, comments))

    return results

if __name__ == "__main__":
    # Initialize Spark session
    spark = SparkSession.builder.appName("TableCounter").getOrCreate()

    # Specify the list of databases
    databases = ["database1", "database2"]  # Add your database names here

    # Initialize an empty list to store overall results
    all_results = []

    # Iterate through each database and count tables
    for database in databases:
        results = count_tables(spark, database)
        all_results.extend(results)

    # Create a DataFrame from the results
    schema = ["Database", "TableName", "Count", "Comments"]
    df = spark.createDataFrame(all_results, schema=schema)

    # Write the results to the default database as countbck
    df.write.mode("overwrite").saveAsTable("countbck")

    # Stop the Spark session
    spark.stop()
