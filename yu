from pyspark.sql import SparkSession

def create_tables(source_database, target_database, table_names):
    # Initialize Spark session
    spark = SparkSession.builder.appName("CreateTables").getOrCreate()

    # Iterate through the specified table names
    for table_name in table_names:
        # Read data from the source table
        source_table = spark.table(f"{source_database}.{table_name}")

        # Get the column name to extract values
        column_name = source_table.columns[0]

        # Extract values from the specified column
        values = source_table.select(column_name).distinct()

        # Create a new DataFrame with the extracted values
        new_table = spark.createDataFrame(values.rdd, source_table.schema)

        # Create the target table name with "bck" added
        target_table_name = f"{table_name}_bck"

        # Save the new table to the target database
        new_table.write.saveAsTable(f"{target_database}.{target_table_name}")

    # Stop the Spark session
    spark.stop()

# Example usage:
source_db_name = "source_database"
target_db_name = "target_database"
table_names_input = ["table1", "table2", "table3"]

# Call the function to create tables in the target database
create_tables(source_db_name, target_db_name, table_names_input)
