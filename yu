from pyspark.sql import SparkSession
import os

# Create a Spark session
spark = SparkSession.builder.appName("CountFilesAndSize").getOrCreate()

# Define the remote mount folder path
remote_mount_folder_path = "/mnt/your_remote_mount_path"  # Replace with your actual remote mount folder path

# Function to get file size in MB
def get_file_size(file_path):
    return os.path.getsize(file_path) / (1024 * 1024)  # Convert bytes to megabytes

# Get a list of all files in the remote mount folder
file_paths = [os.path.join(remote_mount_folder_path, file) for file in os.listdir(remote_mount_folder_path)]

# Create a Spark DataFrame with file paths
df = spark.createDataFrame(file_paths, "path STRING")

# Register a user-defined function (UDF) to calculate file size
spark.udf.register("get_file_size", get_file_size)

# Calculate file size for each file using the UDF
result = spark.sql("SELECT path, get_file_size(path) AS size_MB FROM __TABLE__")

# Count the number of files
file_count = result.count()

# Show the result
print(f"Total number of files: {file_count}")
result.show(truncate=False)

# Stop the Spark session
spark.stop()
