# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Define DBFS path to the input CSV file
dbfs_csv_path = "/dbfs/path/to/input.csv"  # Replace with your actual DBFS path

# Create Spark session
spark = SparkSession.builder.appName("DuplicateCheck").getOrCreate()

# Read CSV file into DataFrame
csv_df = spark.read.option("header", "true").csv(dbfs_csv_path)

# Iterate over rows in the DataFrame
for row in csv_df.rdd.collect():
    table_name = row['tablename']
    grain_column_list = row['grain_column_list'].split(',')

    # Read data from the specified table in MRT database
    mrt_table_df = spark.table(f"MRT.{table_name}")

    # Apply duplicate check with filter condition (endat is null)
    duplicate_rows = mrt_table_df.groupBy(*grain_column_list).count().filter(col("count") > 1).filter(col("endat").isNull())

    # Show the duplicate rows
    print(f"Duplicate rows in {table_name} with filter condition (endat is null):")
    duplicate_rows.show(truncate=False)

# Stop the Spark session
spark.stop()
