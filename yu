from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Define the DBFS path for the input CSV file
csv_path = "/dbfs/path/to/input.csv"  # Replace with your actual path

# Create Spark session
spark = SparkSession.builder.appName("DuplicateCheck").getOrCreate()

# Read CSV file into a DataFrame
csv_df = spark.read.option("header", "true").csv(csv_path)

# Iterate through rows in the CSV DataFrame
for row in csv_df.rdd.collect():
    tablename = row['tablename']
    grain_columns = row['grain_column_list'].split(',')

    # Read data from the MRT table
    mrt_df = spark.read.format("delta").table(f"MRT.{tablename}").filter(col("_end_at").isNull())

    # Perform duplicate check based on grain columns
    duplicate_rows = mrt_df.groupBy(grain_columns).count().filter(col("count") > 1)

    # Display results
    print(f"Duplicates in {tablename} for grain columns {grain_columns}:")
    duplicate_rows.show(truncate=False)

# Stop the Spark session
spark.stop()
