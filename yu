from pyspark.sql import SparkSession
import os

# Create a Spark session
spark = SparkSession.builder.appName("FileSize").getOrCreate()

# Define the base folder path
base_folder_path = "/mnt/your_folder_path"  # Replace with your actual folder path

# Function to get file size in MB
def get_file_size(file_path):
    return os.path.getsize(file_path) / (1024 * 1024)  # Convert bytes to megabytes

# Get a list of all files in the folder and its subfolders
file_paths = [os.path.join(root, file) for root, dirs, files in os.walk(base_folder_path) for file in files]

# Create a Spark DataFrame with file paths
df = spark.createDataFrame(file_paths, "path STRING")

# Register a user-defined function (UDF) to calculate file size
spark.udf.register("get_file_size", get_file_size)

# Calculate file size for each file using the UDF
result = spark.sql("SELECT path, get_file_size(path) AS size_MB FROM __TABLE__")

# Show the result
result.show(truncate=False)

# Stop the Spark session
spark.stop()
