# Use PySpark to access Databricks SQL API
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Fetch Table Columns with Date") \
    .getOrCreate()

# Set the schema and database
schema = "kingfisher_core"

# Define a function to fetch table names and their columns
def fetch_table_columns_with_date(schema):
    table_columns = []

    # Query to fetch table names
    tables_query = f"SHOW TABLES IN {schema}"
    tables_df = spark.sql(tables_query)

    # Iterate through each table
    for table_row in tables_df.collect():
        table_name = table_row["tableName"]
        columns = []

        # Query to fetch columns in the table that contain 'date'
        columns_query = f"DESCRIBE {schema}.{table_name}"
        columns_df = spark.sql(columns_query)

        # Iterate through each column and check for 'date' in the column name
        for column_row in columns_df.collect():
            column_name = column_row["col_name"]
            if "date" in column_name.lower():
                columns.append(column_name)

        # If columns with 'date' are found, add to the result list
        if columns:
            table_columns.append({"Table": table_name, "Date Columns": columns})

    return table_columns

# Fetch table names and columns with 'date'
result = fetch_table_columns_with_date(schema)

# Convert the result to a DataFrame
result_df = spark.createDataFrame(result)

# Show the result
result_df.show(truncate=False)

# Stop the SparkSession
spark.stop()
