from pyspark.sql import SparkSession

# Define Azure Synapse SQL pool connection details
synapse_server = "your_synapse_server"
database = "your_database"
user = "your_username"
password = "your_password"
jdbc_url = f"jdbc:sqlserver://{synapse_server}:1433;database={database};user={user};password={password};"

# Define the list of tables to copy
input_tables = ["table1", "table2", "table3"]  # Replace with your actual table names

# Create Spark session
spark = SparkSession.builder.appName("SynapseToDatabricks").getOrCreate()

for table in input_tables:
    synapse_df = spark.read.format("jdbc").option("url", jdbc_url).option("dbtable", table).load()
    
    # Write to Databricks default schema with the same table name suffixed by "_VC" and overwrite existing data
    synapse_df.write.format("delta").mode("overwrite").saveAsTable(f"{table}_VC")

# Stop the Spark session
spark.stop()
