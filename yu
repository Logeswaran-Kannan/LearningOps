from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("LargeJSONViewer").getOrCreate()

# Set the desired configuration to avoid truncation
spark.conf.set("spark.sql.repl.eagerEval.enabled", True)
spark.conf.set("spark.sql.execution.arrow.maxRecordsPerBatch", 2000)

# Replace 'your_table' and 'column_name' with your actual table and column names
table_name = "your_table"
column_name = "column_name"

# Read the data from the table
data = spark.sql(f"SELECT {column_name} FROM {table_name}")

# Convert the column to Pandas DataFrame
df = data.select(column_name).toPandas()

# Display the Pandas DataFrame
print(df)

# Stop the Spark session
spark.stop()
