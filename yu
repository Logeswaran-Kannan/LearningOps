from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Define the DBFS path for the input CSV file
csv_path = "/dbfs/path/to/input.csv"  # Replace with your actual path

# Create Spark session
spark = SparkSession.builder.appName("DuplicateCheck").getOrCreate()

# Read CSV file into a DataFrame
csv_df = spark.read.option("header", "true").csv(csv_path)

# Iterate through rows in the CSV DataFrame
for row in csv_df.rdd.collect():
    tablename = row['tablename']
    grain_columns = row['grain_column_list'].split(',')

    # Read data from the MRT table
    mrt_df = spark.read.format("delta").table(f"MRT.{tablename}").filter(col("_end_at").isNull())

    # Perform duplicate check based on grain columns
    duplicate_rows = mrt_df.groupBy(grain_columns).count().filter(col("count") > 1)

    # Display results
    print(f"Duplicates in {tablename} for grain columns {grain_columns}:")
    
    # Check if duplicates exist
    if duplicate_rows.count() > 0:
        # Display overall duplicate count
        overall_duplicate_count = duplicate_rows.count()
        print(f"Overall duplicate count: {overall_duplicate_count}")

        # Display a sample of 20 records
        sample_records = mrt_df.join(duplicate_rows.select(grain_columns), on=grain_columns).limit(20)
        sample_records.show(truncate=False)
    else:
        print("No duplicates found, count: 0")

# Stop the Spark session
spark.stop()
