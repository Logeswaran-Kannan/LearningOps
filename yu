from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col

# Initialize Spark session
spark = SparkSession.builder.appName("JSONProcessing").getOrCreate()

# Specify your table and column names
source_table_name = "your_source_table"
source_column_name = "body"

# Read the JSON data into a Spark DataFrame
json_data = spark.sql(f"SELECT * FROM {source_table_name}")

# Parse the JSON string in the body column
json_schema = spark.read.json(json_data.rdd.map(lambda row: row.body)).schema
parsed_json_data = json_data.withColumn("parsed_body", from_json(col(source_column_name), json_schema))

# Extract fields from the parsed JSON structure
result_df = parsed_json_data.select(
    col("parsed_body.field1").alias("Field1"),
    col("parsed_body.field2").alias("Field2"),
    col("parsed_body.field3.nestedField").alias("NestedField"),
    col("parsed_body.field3.anotherNestedField").alias("AnotherNestedField")
)

# Show the result DataFrame
result_df.show(truncate=False)

# Write the result DataFrame to a new table
result_table_name = "result_table"
result_df.write.mode("overwrite").saveAsTable(result_table_name)

# Stop the Spark session
spark.stop()
