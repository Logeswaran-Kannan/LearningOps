from pyspark.sql import SparkSession
import os

# Create a Spark session
spark = SparkSession.builder.appName("FileCountAndSize").getOrCreate()

# Define the remote mount folder path
mount_folder_path = "/mnt/your_remote_mount_path"  # Replace with your actual remote mount path

# Function to get file size in MB
def get_file_size(file_path):
    return os.path.getsize(file_path) / (1024 * 1024)  # Convert bytes to megabytes

# Get a list of all files in the folder and its subfolders
file_paths = [os.path.join(root, file) for root, dirs, files in os.walk(mount_folder_path) for file in files]

# Create a Spark DataFrame with file paths
df = spark.createDataFrame(file_paths, "path STRING")

# Register a user-defined function (UDF) to calculate file size
spark.udf.register("get_file_size", get_file_size)

# Calculate file size for each file using the UDF
result = spark.sql("SELECT path, get_file_size(path) AS size_MB FROM __TABLE__")

# Count the number of files in each folder
file_count = spark.sql("SELECT COUNT(*) AS count, SUBSTRING_INDEX(path, '/', -2) AS folder FROM __TABLE__ GROUP BY folder")

# Show the result
result.show(truncate=False)
file_count.show(truncate=False)

# Stop the Spark session
spark.stop()
