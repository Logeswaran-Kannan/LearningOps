from pyspark.sql import SparkSession
from pyspark.sql.functions import col, size

# Create a Spark session
spark = SparkSession.builder.appName("FileCountAndSize").getOrCreate()

# Define the folder path
folder_path = "/mnt/your_folder_path"  # Replace with your actual folder path

# Read files from the specified folder
files_df = spark.read.option("recursiveFileLookup", "true").parquet(folder_path)

# Count the number of files
file_count = files_df.count()

# Display the count of files
print(f"Number of files in the folder: {file_count}")

# Display the size of each file
files_with_size = files_df.select("path", size("value").alias("file_size_bytes"))
files_with_size.show(truncate=False)

# Stop the Spark session
spark.stop()
