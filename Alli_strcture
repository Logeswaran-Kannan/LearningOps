from pyspark.sql import SparkSession
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import uuid
from datetime import datetime

# Initialize
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

catalog_name = "core"
database_name = "ods"
target_catalog = "core_preprod"
target_schema = "default"
target_table = "table_counts"

# Run metadata
run_id = str(uuid.uuid4())
run_timestamp = datetime.now().isoformat()

# Step 1: Get filtered table list
tables_df = spark.sql(f"SHOW TABLES IN {catalog_name}.{database_name}")
filtered_tables = tables_df.filter(
    (tables_df.tableName.startswith("policy")) |
    (tables_df.tableName.startswith("billing")) |
    (tables_df.tableName.startswith("claim"))
).select("tableName").rdd.flatMap(lambda x: x).collect()

# Step 2: Auto-tune max workers
default_parallelism = sc.defaultParallelism  # Approx total cores across executors
max_workers = min(len(filtered_tables), max(2, default_parallelism * 2 // 3))  # safe limit
print(f"ðŸ§  Auto-tuned max_workers: {max_workers} (based on {default_parallelism} cores)")

# Progress tracking
lock = threading.Lock()
progress = {"completed": 0, "total": len(filtered_tables)}

def log_progress():
    with lock:
        progress["completed"] += 1
        print(f"[{progress['completed']} of {progress['total']}] completed.")

# Step 3: Function to get count
def get_table_count(table_name):
    try:
        full_table = f"{catalog_name}.{database_name}.{table_name}"
        count = spark.read.table(full_table).count()
        log_progress()
        return (run_id, run_timestamp, table_name, count)
    except Exception as e:
        print(f"Error reading {table_name}: {e}")
        log_progress()
        return (run_id, run_timestamp, table_name, None)

# Step 4: Use parallel threads
results = []
with ThreadPoolExecutor(max_workers=max_workers) as executor:
    future_to_table = {executor.submit(get_table_count, tbl): tbl for tbl in filtered_tables}
    for future in as_completed(future_to_table):
        results.append(future.result())

# Step 5: Save output to Delta table
result_df = spark.createDataFrame(results, ["run_id", "run_timestamp", "table_name", "record_count"])
result_df.write.mode("append").saveAsTable(f"{target_catalog}.{target_schema}.{target_table}")

print(f"âœ… Count operation completed and saved to {target_catalog}.{target_schema}.{target_table}")
