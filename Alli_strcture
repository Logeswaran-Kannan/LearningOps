from pyspark.sql import SparkSession
import concurrent.futures
import threading

spark = SparkSession.builder.getOrCreate()

# Configuration
catalog = "core"
database = "ods"
prefixes = ("policy", "billing", "claim")
benchmark_table = "core_preprod.default.ddl_benchmark"
anomaly_table = "core_preprod.default.ddl_anomalies"
max_threads = 10

progress = {"done": 0, "total": 0}
lock = threading.Lock()

# Get table list
tables_df = spark.sql(f"SHOW TABLES IN {catalog}.{database}")
filtered_tables = tables_df.filter(
    tables_df.tableName.startswith(prefixes[0]) |
    tables_df.tableName.startswith(prefixes[1]) |
    tables_df.tableName.startswith(prefixes[2])
)
table_list = [f"{catalog}.{database}.{row.tableName}" for row in filtered_tables.collect()]
progress["total"] = len(table_list)

# Extract DDL details
def get_table_ddl(table_name):
    try:
        desc_df = spark.sql(f"DESCRIBE TABLE EXTENDED {table_name}").filter("col_name NOT LIKE '#%' AND col_name != ''")
        ddl_rows = desc_df.select("col_name", "data_type", "comment").collect()
        results = []
        for row in ddl_rows:
            col = row["col_name"]
            dtype = row["data_type"]
            precision, scale = None, None
            if "(" in dtype:
                try:
                    type_main, params = dtype.strip(")").split("(")
                    precision, scale = map(int, params.split(","))
                except:
                    pass
            results.append((table_name, col, dtype, precision, scale))
    except Exception as e:
        results = [(table_name, "ERROR", str(e), None, None)]

    with lock:
        progress["done"] += 1
        print(f"[{progress['done']}/{progress['total']}] Scanned: {table_name}")

    return results

# Run in parallel
all_results = []
with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
    futures = [executor.submit(get_table_ddl, t) for t in table_list]
    for f in concurrent.futures.as_completed(futures):
        all_results.extend(f.result())

# Convert to DataFrame
ddl_df = spark.createDataFrame(all_results, ["TableName", "ColumnName", "DataType", "Precision", "Scale"])

# Check if benchmark exists
benchmark_exists = spark._jsparkSession.catalog().tableExists(benchmark_table)

if not benchmark_exists:
    # First run: store benchmark
    ddl_df.write.mode("overwrite").format("delta").saveAsTable(benchmark_table)
    print("✅ Benchmark created.")
    ddl_df.display()
else:
    # Second or later run: compare against benchmark
    ddl_df.createOrReplaceTempView("current_ddl")
    spark.read.table(benchmark_table).createOrReplaceTempView("benchmark_ddl")

    comparison_query = """
    SELECT 'Missing Column' AS Issue, b.TableName, b.ColumnName, b.DataType, b.Precision, b.Scale
    FROM benchmark_ddl b
    LEFT ANTI JOIN current_ddl c
    ON b.TableName = c.TableName AND b.ColumnName = c.ColumnName

    UNION ALL

    SELECT 'New Column' AS Issue, c.TableName, c.ColumnName, c.DataType, c.Precision, c.Scale
    FROM current_ddl c
    LEFT ANTI JOIN benchmark_ddl b
    ON b.TableName = c.TableName AND b.ColumnName = c.ColumnName

    UNION ALL

    SELECT 'Changed DataType' AS Issue, b.TableName, b.ColumnName, CONCAT(b.DataType, ' -> ', c.DataType) AS DataType,
           CONCAT(b.Precision, ' -> ', c.Precision) AS Precision,
           CONCAT(b.Scale, ' -> ', c.Scale) AS Scale
    FROM benchmark_ddl b
    INNER JOIN current_ddl c
    ON b.TableName = c.TableName AND b.ColumnName = c.ColumnName
    WHERE b.DataType != c.DataType OR b.Precision != c.Precision OR b.Scale != c.Scale
    """

    anomalies_df = spark.sql(comparison_query)
    anomalies_df.write.mode("overwrite").format("delta").saveAsTable(anomaly_table)

    print("✅ Comparison complete. Anomalies saved.")
    anomalies_df.display()
