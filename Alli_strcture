from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
from datetime import datetime, timedelta
import uuid

# Initialize Spark session
spark = SparkSession.builder.appName("CountCheckValidation").getOrCreate()

# Constants
CATALOG_NAME = "core_tst_std001"
EXCLUSION_LIST_PATH = "/mnt/data/exclusion_list_std001_test.csv"
OUTPUT_TABLE = "core_tst_std001.audit_table_count_check"

# Load exclusion list CSV
exclusion_df = spark.read.option("header", True).csv(EXCLUSION_LIST_PATH)
filtered_tables_df = exclusion_df.filter(col("excludeindicator") == 'Y')

table_list = [row["tablename"] for row in filtered_tables_df.collect()]

# Databases to compare
dbs = ["ods_stg_copy", "ods", "ods_views"]
prefix_mapping = {
    "policycenter_std001_": "vw_policycenter_std001_current_",
    "billingcenter_std001_": "vw_billingcenter_std001_current_",
    "claimcenter_std001_": "vw_claimcenter_std001_current_"
}

# Current run ID
run_id = str(uuid.uuid4())
current_date = datetime.now().strftime('%Y-%m-%d')

# Helper function to apply prefix mapping for ods_views tables
def get_view_table_name(base_name):
    for prefix, view_prefix in prefix_mapping.items():
        if base_name.startswith(prefix):
            return base_name.replace(prefix, view_prefix)
    return base_name

# Collect counts with date filtering
results = []
for table in table_list:
    table_counts = {}
    for db in dbs:
        tbl_name = get_view_table_name(table) if db == "ods_views" else table
        full_table_name = f"{CATALOG_NAME}.{db}.{tbl_name}"
        try:
            df = spark.read.table(full_table_name)
            count = df.filter(col("AZURE_LOAD_DATE") == current_date).count()
            table_counts[db] = count
        except Exception as e:
            table_counts[db] = None  # log error if needed
    results.append((str(uuid.uuid4()), run_id, table, table_counts.get("ods_stg_copy"),
                    table_counts.get("ods"), table_counts.get("ods_views"), datetime.now()))

# Define schema explicitly
schema = StructType([
    StructField("id", StringType(), False),
    StructField("run_id", StringType(), False),
    StructField("table_name", StringType(), False),
    StructField("ods_stg_copy_count", IntegerType(), True),
    StructField("ods_count", IntegerType(), True),
    StructField("ods_views_count", IntegerType(), True),
    StructField("timestamp", TimestampType(), False)
])

# Create final result DataFrame
result_df = spark.createDataFrame(results, schema=schema)

# Cleanup records older than 7 days from output table
seven_days_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
spark.sql(f"DELETE FROM {OUTPUT_TABLE} WHERE timestamp < DATE('{seven_days_ago}')")

# Write new results to output table
result_df.write.mode("append").saveAsTable(OUTPUT_TABLE)
