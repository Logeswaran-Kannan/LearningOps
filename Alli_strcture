from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
from pyspark.sql.functions import *
from datetime import datetime, timedelta

spark = SparkSession.builder.getOrCreate()

# Paths to the files
schema_file_path = "/dbfs/mnt/yourmount/readreplica_schema_file_2025_04_30_13_10_04.csv"
exclusion_file_path = "/dbfs/mnt/yourmount/exclusion_list_std001_test.csv"

# Generate a unique run_id based on timestamp
run_id = int(datetime.now().strftime("%Y%m%d%H%M%S"))
current_timestamp = datetime.now()

# Read baseline schema CSV and normalize values to lowercase
schema_df = (
    spark.read.option("header", True).csv(schema_file_path)
    .withColumn("table_name", lower(col("table_name")))
    .withColumn("column_name", lower(col("column_name")))
    .withColumn("data_type", lower(col("data_type")))
)

# Read and filter exclusion list
exclusion_df = (
    spark.read.option("header", True).csv(exclusion_file_path)
    .filter(col("excludeindicator") == "Y")
    .withColumn("tablename", lower(col("tablename")))
)

# Extract list of target tables
tables_to_check = [row['tablename'] for row in exclusion_df.select("tablename").distinct().collect()]

results = []

for table in tables_to_check:
    try:
        baseline = schema_df.filter(col("table_name") == table).cache()

        try:
            target = spark.table(f"core_tst_std001.ods.{table}")
            target_schema = target.schema
            target_rows = [(table, f.name.lower(), f.dataType.simpleString().lower()) for f in target_schema]
            target_df = spark.createDataFrame(target_rows, ["table_name", "column_name", "data_type"]).cache()
        except:
            for row in baseline.collect():
                results.append((str(run_id), table, row['table_name'], row['column_name'], row['data_type'], None, None, None, "table missing in target ODS Layer", "Fail", current_timestamp))
            continue

        baseline_df = baseline.selectExpr(
            "table_name as source_table_name", "column_name as source_column_name", "data_type as source_data_type")
        target_df = target_df.selectExpr(
            "table_name as target_table_name", "column_name as target_column_name", "data_type as target_data_type")

        joined = baseline_df.join(target_df, lower(baseline_df.source_column_name) == lower(target_df.target_column_name), how="fullouter")

        for row in joined.collect():
            source_col = row['source_column_name']
            target_col = row['target_column_name']
            comment = ""
            status = "All Pass"

            if source_col is None:
                comment = "Column added from baseline version"
                status = "Fail"
            elif target_col is None:
                comment = "Column removed from baseline ddl version"
                status = "Fail"
            elif row['source_data_type'] != row['target_data_type']:
                comment = "Datatype changed from baseline version"
                status = "Fail"

            results.append((str(run_id), table, row['source_table_name'], source_col, row['source_data_type'],
                            row['target_table_name'], target_col, row['target_data_type'], comment, status, current_timestamp))
    
    except Exception as e:
        results.append((str(run_id), table, None, None, None, None, None, None, str(e), "Fail", current_timestamp))

# Define schema explicitly
output_schema = StructType([
    StructField("run_id", StringType(), True),
    StructField("tablename", StringType(), True),
    StructField("sourcetablename", StringType(), True),
    StructField("sourcecolmunname", StringType(), True),
    StructField("sourcedatatype", StringType(), True),
    StructField("targettablename", StringType(), True),
    StructField("targetcolmunname", StringType(), True),
    StructField("targetdatatype", StringType(), True),
    StructField("comment", StringType(), True),
    StructField("status", StringType(), True),
    StructField("load_timestamp", TimestampType(), True)
])

output_df = spark.createDataFrame(results, schema=output_schema)

# Output table
output_table = "core_tst_sys9.default.ddl_comparison_results"

# Create the table if it doesn't exist
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {output_table} (
        run_id STRING,
        tablename STRING,
        sourcetablename STRING,
        sourcecolmunname STRING,
        sourcedatatype STRING,
        targettablename STRING,
        targetcolmunname STRING,
        targetdatatype STRING,
        comment STRING,
        status STRING,
        load_timestamp TIMESTAMP
    )
    USING DELTA
""")

# Cleanup records older than 7 days
spark.sql(f"DELETE FROM {output_table} WHERE load_timestamp < TIMESTAMP '{(current_timestamp - timedelta(days=7)).strftime('%Y-%m-%d %H:%M:%S')}'")

# Append current run
output_df.write.format("delta").mode("append").saveAsTable(output_table)

print("DDL comparison completed and results stored.")
