from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat_ws, lit

# Initialize Spark session
spark = SparkSession.builder.appName("DataComparison").getOrCreate()

# Define paths and table names
csv_path = "dbfs:<your_dbfs_path>/data.csv"
table1_name = "<your_table1>"
table2_name = "<your_table2>"
output_table_name = "<your_output_table>"

# Load CSV into DataFrame and create first table
df1 = spark.read.option("header", "true").csv(csv_path)
df1.createOrReplaceTempView(table1_name)

# Create the second table from an existing table
table2_df = spark.sql(f"SELECT * FROM {table2_name}")
table2_df.createOrReplaceTempView(table2_name)

# Define columns for data comparison
comparison_columns = ["column1", "column2", "column3"]  # Replace with your actual column names

# Function to perform data comparison
def compare_data(row):
    key = row.policy_reference + row.dim_date_key
    status = "pass" if all(row[f"{col}_df1"] == row[f"{col}_df2"] for col in comparison_columns) else "fail"
    return (*key, *row[comparison_columns], status)

# Apply filter on concatenated columns
comparison_query = f"""
    SELECT
        {table1_name}.policy_reference,
        {table1_name}.dim_date_key,
        {', '.join([f"{table1_name}.{col} AS {col}_df1" for col in comparison_columns])},
        {', '.join([f"{table2_name}.{col} AS {col}_df2" for col in comparison_columns])}
    FROM
        {table1_name}
    JOIN
        {table2_name}
    ON
        {table1_name}.policy_reference = {table2_name}.policy_reference
        AND {table1_name}.dim_date_key = {table2_name}.dim_date_key
    WHERE
        CONCAT_WS('', {table1_name}.policy_reference, {table1_name}.dim_date_key) = 
        CONCAT_WS('', {table2_name}.policy_reference, {table2_name}.dim_date_key)
"""

# Execute comparison query
comparison_result = spark.sql(comparison_query).rdd.map(compare_data)

# Convert RDD to DataFrame
result_columns = comparison_columns + ["status"]
result_schema = table2_df.schema.add(*[col(col_name) for col_name in result_columns])
result_df = spark.createDataFrame(comparison_result, result_schema)

# Drop output table if exists
spark.sql(f"DROP TABLE IF EXISTS {output_table_name}")

# Save result DataFrame as a table
result_df.write.saveAsTable(output_table_name)

# Stop Spark session
spark.stop()
