from pyspark.sql import SparkSession
from pyspark.sql.functions import concat_ws
from pyspark.sql.types import StructType, StructField, StringType

# Initialize Spark session
spark = SparkSession.builder.appName("DataComparison").getOrCreate()

# Paths and filenames
csv_path = "dbfs:/path/to/your/csv/file.csv"
first_table_name = "first_table"
second_table_name = "second_table"
third_temp_table_name = "third_temp_table"
output_table_name = "output_table"

# Read CSV into DataFrame and create first table
schema = StructType([StructField("policy_reference", StringType(), True),
                     StructField("dim_date_key", StringType(), True),
                     # Add other columns from your CSV
                    ])
csv_df = spark.read.csv(csv_path, header=True, schema=schema)
csv_df.createOrReplaceTempView(first_table_name)

# Create second table from existing table
spark.sql(f"CREATE OR REPLACE TEMPORARY VIEW {second_table_name} AS SELECT * FROM existing_table")

# Create third temp table with distinct values from concatenated columns
spark.sql(f"""
    CREATE OR REPLACE TEMPORARY VIEW {third_temp_table_name} AS
    SELECT DISTINCT concat_ws('_', policy_reference, dim_date_key) AS concat_column
    FROM {first_table_name}
""")

# Apply filter condition to first and second tables
filter_condition = f"""
    concat_ws('_', policy_reference, dim_date_key) IN
    (SELECT concat_column FROM {third_temp_table_name})
"""
filtered_first_table = spark.sql(f"SELECT * FROM {first_table_name} WHERE {filter_condition}")
filtered_second_table = spark.sql(f"SELECT * FROM {second_table_name} WHERE {filter_condition}")

# Define list of columns for data comparison
comparison_columns = ["column1", "column2", "column3"]  # Add your column names here

# Check the count of rows in both dataframes
count_first_table = filtered_first_table.count()
count_second_table = filtered_second_table.count()

if count_first_table == count_second_table:


# Create a list to store comparison results
comparison_results = []

# Perform data comparison
   for row in filtered_first_table.orderBy("policy_reference", "dim_date_key", "days_from_inspection").collect():
        policy_reference = row.policy_reference
        dim_date_key = row.dim_date_key
        for column in comparison_columns:
            value_first = row[column]
            value_second = filtered_second_table.filter(
                (filtered_second_table.policy_reference == policy_reference) &
                (filtered_second_table.dim_date_key == dim_date_key)
            ).orderBy("policy_reference", "dim_date_key", "days_from_inspection").select(column).first()[0]
            comparison_results.append({
                "policy_reference": policy_reference,
                "dim_date_key": dim_date_key,
                "column": column,
                "value_first": value_first,
                "value_second": value_second,
                "status": "pass" if value_first == value_second else "fail"
            })

    # Convert the comparison results list into a DataFrame
    comparison_df = spark.createDataFrame(comparison_results)

    # Create or replace output table
    comparison_df.createOrReplaceTempView(output_table_name)

    # Show comparison results
    spark.sql(f"SELECT * FROM {output_table_name}").show()

else:
    # Print a message indicating that the counts do not match
    print("Row counts do not match. Data comparison cannot be performed.")
    
# Stop the Spark session
spark.stop()
In this version of the code, both the filtered_first_table and the filtered_second_table are ordered based on "policy_reference," "dim_date_key," and "days_from_inspection" columns before performing the data comparison. This ensures that the rows in both dataframes are in the same order before performing the comparison. Adjust the column names in the orderBy clauses based on your actual column names.







# Convert the comparison results list into a DataFrame
comparison_df = spark.createDataFrame(comparison_results)

# Create or replace output table
comparison_df.createOrReplaceTempView(output_table_name)

# Show comparison results
spark.sql(f"SELECT * FROM {output_table_name}").show()

# Stop the Spark session
spark.stop()
___________________________________


comparison_results.append({
    "policy_reference": policy_reference,
    "dim_date_key": dim_date_key,
    "column": column,
    "value_first": str(value_first),  # Convert to string
    "value_second": value_second,
    "status": "pass" if str(value_first) == value_second else "fail"  # Compare as strings
})
__________________________________________


# ... (previous code)

# Perform data comparison
comparison_results = []

# ... (rest of the comparison code)

# Convert the comparison results list into a DataFrame
comparison_df = spark.createDataFrame(comparison_results)

# Create or replace output table
comparison_df.createOrReplaceTempView(output_table_name)

# Calculate pass and fail counts
pass_count = comparison_df.filter(comparison_df.status == "pass").count()
fail_count = comparison_df.filter(comparison_df.status == "fail").count()

# Generate HTML code for the pie chart
import plotly.graph_objs as go

labels = ['Pass', 'Fail']
values = [pass_count, fail_count]

pie_chart = go.Figure(data=[go.Pie(labels=labels, values=values)])

# Save the HTML code to a file
html_file_path = "/dbfs/tmp/dashboard.html"
pie_chart.write_html(html_file_path)

# Display the HTML dashboard
displayHTML(open(html_file_path).read())

# Stop the Spark session
spark.stop()

------------

csv_df = spark.read.csv(csv_path, header=True, inferSchema=True, ignoreLeadingWhiteSpace=True)
