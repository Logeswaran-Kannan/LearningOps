from pyspark.sql import SparkSession
from pyspark.sql.functions import concat_ws
from pyspark.sql.types import StructType, StructField, StringType

# Initialize Spark session
spark = SparkSession.builder.appName("DataComparison").getOrCreate()

# Paths and filenames
csv_path = "dbfs:/path/to/your/csv/file.csv"
first_table_name = "first_table"
second_table_name = "second_table"
third_temp_table_name = "third_temp_table"
output_table_name = "output_table"

# Read CSV into DataFrame and create first table
schema = StructType([StructField("policy_reference", StringType(), True),
                     StructField("dim_date_key", StringType(), True),
                     # Add other columns from your CSV
                    ])
csv_df = spark.read.csv(csv_path, header=True, schema=schema)
csv_df.createOrReplaceTempView(first_table_name)

# Create second table from existing table
spark.sql(f"CREATE OR REPLACE TEMPORARY VIEW {second_table_name} AS SELECT * FROM existing_table")

# Create third temp table with distinct values from concatenated columns
spark.sql(f"""
    CREATE OR REPLACE TEMPORARY VIEW {third_temp_table_name} AS
    SELECT DISTINCT concat_ws('_', policy_reference, dim_date_key) AS concat_column
    FROM {first_table_name}
""")

# Apply filter condition to first and second tables
filter_condition = f"""
    concat_ws('_', policy_reference, dim_date_key) IN
    (SELECT concat_column FROM {third_temp_table_name})
"""
filtered_first_table = spark.sql(f"SELECT * FROM {first_table_name} WHERE {filter_condition}")
filtered_second_table = spark.sql(f"SELECT * FROM {second_table_name} WHERE {filter_condition}")

# Define list of columns for data comparison
comparison_columns = ["column1", "column2", "column3"]  # Add your column names here

# Function to perform data comparison and return results
def compare_data(row):
    policy_reference = row.policy_reference
    dim_date_key = row.dim_date_key
    comparisons = []
    for column in comparison_columns:
        value_first = row[column]
        value_second = filtered_second_table.filter(
            (filtered_second_table.policy_reference == policy_reference) &
            (filtered_second_table.dim_date_key == dim_date_key)
        ).select(column).first()[0]
        comparisons.append({
            "policy_reference": policy_reference,
            "dim_date_key": dim_date_key,
            "column": column,
            "value_first": value_first,
            "value_second": value_second,
            "status": "pass" if value_first == value_second else "fail"
        })
    return comparisons

# Perform data comparison
comparison_results = filtered_first_table.rdd.flatMap(compare_data).toDF()

# Create or replace output table
comparison_results.createOrReplaceTempView(output_table_name)

# Show comparison results
spark.sql(f"SELECT * FROM {output_table_name}").show()

# Stop the Spark session
spark.stop()
