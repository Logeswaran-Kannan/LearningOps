from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder.appName("ColumnLengthComparison").getOrCreate()

# Define the database names and table name
source_db_name = "source_database"
target_db_name = "target_database"
table_name = "your_table_name"

# Read data from source and target databases
source_df = spark.table(f"{source_db_name}.{table_name}")
target_df = spark.table(f"{target_db_name}.{table_name}")

# Define columns to be excluded
excluded_columns = ['starttmp', 'dftr']

# Filter records based on the sequence_by column
start_time = "2023-08-30T8:43:28.123+0000"
end_time = "2023-09-04T8:43:28.123+0000"
source_df = source_df.filter((col("sequence_by") >= start_time) & (col("sequence_by") <= end_time))
target_df = target_df.filter((col("sequence_by") >= start_time) & (col("sequence_by") <= end_time))

# Compare column lengths and find mismatches
mismatch_columns = []
for column in source_df.columns:
    if column not in excluded_columns:
        source_length = source_df.withColumn("source_length", length(col(column))).select("source_length")
        target_length = target_df.withColumn("target_length", length(col(column))).select("target_length")
        
        mismatch_count = source_length.join(target_length, source_length.source_length != target_length.target_length).count()
        
        if mismatch_count > 0:
            mismatch_columns.append((table_name, column, source_length.first().source_length, target_length.first().target_length))

# Display the results in tabular form
result_df = spark.createDataFrame(mismatch_columns, ["tablename", "mismatch_column", "source_length", "target_length"])
result_df.show()

# Generate SQL queries with excluded columns
source_sql = f"SELECT {','.join([col for col in source_df.columns if col not in excluded_columns])} FROM {source_db_name}.{table_name} WHERE sequence_by >= '{start_time}' AND sequence_by <= '{end_time}'"
target_sql = f"SELECT {','.join([col for col in target_df.columns if col not in excluded_columns])} FROM {target_db_name}.{table_name} WHERE sequence_by >= '{start_time}' AND sequence_by <= '{end_time}'"

print(f"Source SQL Query:\n{source_sql}")
print(f"Target SQL Query:\n{target_sql}")

# Stop the Spark session
spark.stop()
