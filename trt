from pyspark.sql.functions import col, count, countDistinct, isnan, isnull, when
from pyspark.sql.types import DoubleType, IntegerType, StringType, StructType

# Input database name
db_name = "your_database_name"

# Filter to exclude tables with names containing an underscore
table_filter = "table_name NOT LIKE '%\\_%'"

# Get all table names in the database
tables = spark.sql(f"SHOW TABLES IN {db_name}").filter(table_filter).select("tableName").collect()

# Loop through each table
for table in tables:
  table_name = table.tableName

  # Get the schema of the table
  schema = spark.table(table_name).schema

  # Initialize empty lists for storing profiling results
  column_names = []
  null_percentages = []
  data_densities = []
  valid_values = []

  # Loop through each column in the table
  for field in schema.fields:
    column_name = field.name
    column_type = field.dataType

    # Calculate null percentage
    null_percentage = spark.table(table_name).select(isnull(col(column_name)).cast(DoubleType())).agg(count(when(col("value") == 1, True))/count(col("value"))).collect()[0][0]

    # Calculate data density
    if isinstance(column_type, (IntegerType, DoubleType)):
      data_density = spark.table(table_name).select(col(column_name)).filter(col(column_name).isNotNull()).count() / spark.table(table_name).count()
    else:
      data_density = 1.0

    # Calculate valid values
    if isinstance(column_type, StringType):
      valid_values_count = spark.table(table_name).groupBy(column_name).agg(count("*").alias("count")).sort(col("count").desc())
      valid_values_list = [row[column_name] for row in valid_values_count.collect()]
    else:
      valid_values_list = []

    # Append results to lists
    column_names.append(column_name)
    null_percentages.append(null_percentage)
    data_densities.append(data_density)
    valid_values.append(valid_values_list)

  # Create a DataFrame to store the profiling results
  profiling_df = spark.createDataFrame([(column_names[i], null_percentages[i], data_densities[i], valid_values[i]) for i in range(len(column_names))], StructType([
    ("column_name", StringType()),
    ("null_percentage", DoubleType()),
    ("data_density", DoubleType()),
    ("valid_values", StringType())
  ]))

  # Create a temporary table to store the profiling results
  profiling_table_name = f"{table_name}_profiling"
  profiling_df.createOrReplaceTempView(profiling_table_name)
