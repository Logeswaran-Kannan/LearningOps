from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat_ws

# Initialize the SparkSession
spark = SparkSession.builder.appName("Data Comparison").getOrCreate()

# Step 1: Create the first table based on a CSV file in DBFS
csv_file_path = "/dbfs/path/to/your/csv/file.csv"
table_name = "first_table"

df = spark.read.option("header", "true").csv(csv_file_path)
df.createOrReplaceTempView(table_name)

# Step 2: Check if the table already exists and drop if required
overwrite_existing = True

if spark.catalog.tableExists(table_name) and overwrite_existing:
    spark.sql(f"DROP TABLE IF EXISTS {table_name}")
    df.write.saveAsTable(table_name)
elif not spark.catalog.tableExists(table_name):
    df.write.saveAsTable(table_name)

# Step 3: Create the second table with distinct policy_reference values
second_table_name = "second_table"
distinct_df = spark.sql(f"SELECT DISTINCT policy_reference FROM {table_name}")
distinct_df.createOrReplaceTempView(second_table_name)

# Define the list of columns for data comparison
columns_to_compare = ["column1", "column2", "column3"]

# Step 4: Define a function to perform data comparison for the specified columns
def compare_data(policy_reference, columns_to_compare):
    df1 = spark.sql(f"SELECT * FROM {table_name} WHERE policy_reference = '{policy_reference}'")
    df2 = spark.sql(f"SELECT * FROM {table_name} WHERE policy_reference = '{policy_reference}'")

    # Concatenate policy_reference and dim_date_key for filtering
    concat_condition = concat_ws('_', col("policy_reference"), col("dim_date_key"))

    # Filter records in both dataframes using the concatenated condition
    df1 = df1.filter(concat_condition.isin(df2.select(concat_condition).collect()))
    df2 = df2.filter(concat_condition.isin(df1.select(concat_condition).collect()))

    # Order the values before comparison
    for column in columns_to_compare:
        df1 = df1.orderBy(concat_condition, col(column))
        df2 = df2.orderBy(concat_condition, col(column))

    comparison_results = []
    for column in columns_to_compare:
        df1_values = [row[column] for row in df1.collect()]
        df2_values = [row[column] for row in df2.collect()]
        status = "Pass" if df1_values == df2_values else "Fail"
        comparison_results.append((policy_reference, column, df1_values, df2_values, status))

    return comparison_results

# Step 5: Perform data comparison and capture the results in an output DataFrame
all_comparison_results = []

# Loop through each distinct policy_reference and compare the data
for row in distinct_df.collect():
    policy_reference = row[0]
    comparison_results = compare_data(policy_reference, columns_to_compare)
    all_comparison_results.extend(comparison_results)

# Create a DataFrame from the comparison results
output_df = spark.createDataFrame(all_comparison_results, ["policy_reference", "column", "df1_values", "df2_values", "status"])

# Show the comparison results
output_df.show()

# Save the output DataFrame as a Delta table with overwrite mode
output_table_name = "comparison_output_table"

output_df.write.mode("overwrite").format("delta").saveAsTable(output_table_name)
