import os
from datetime import datetime, timedelta
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# DBFS path to analyze
dbfs_path = "/dbfs/path/to/analyze"

# Get the current date
current_date = datetime.now()

# Function to retrieve file statistics and check deletion readiness
def get_file_stats(file_path):
    file_name = os.path.basename(file_path)
    file_size = os.path.getsize(file_path)
    file_creation_date = datetime.fromtimestamp(os.path.getctime(file_path))
    file_last_update_date = datetime.fromtimestamp(os.path.getmtime(file_path))
    file_created_by = os.path.getlogin() if os.name == "posix" else None

    # Check if the file creation date is 2 years from the current date
    deletion_ready = "Yes" if (current_date - file_creation_date) >= timedelta(days=365 * 2) else "No"

    return file_name, file_size, file_path, file_creation_date, file_last_update_date, file_created_by, deletion_ready

# Recursively walk through the DBFS path and collect file statistics
file_stats = []
for root, dirs, files in os.walk(dbfs_path):
    for file in files:
        file_path = os.path.join(root, file)
        file_stat = get_file_stats(file_path)
        file_stats.append(file_stat)

# Create a DataFrame from the file statistics
columns = ["Name", "Size", "Path", "CreationDate", "LastUpdateDate", "CreatedBy", "DeletionReady"]
df = spark.createDataFrame(file_stats, columns)

# Create a temporary table
df.createOrReplaceTempView("file_stats")

# Show the resulting file statistics
df.show()
