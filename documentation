# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when
from datetime import datetime, timedelta

# Set the DBFS path
dbfs_path = "/dbfs/path/to/folder"

# Create a Spark session
spark = SparkSession.builder.getOrCreate()

# Define a recursive function to get all file stats from DBFS path and child paths
def get_file_stats(path):
    file_stats = []
    files = dbutils.fs.ls(path)
    for file in files:
        if file.isDir():
            file_stats += get_file_stats(file.path)
        else:
            file_info = {
                "name": file.name,
                "size": file.size,
                "path": file.path,
                "creation_date": datetime.fromtimestamp(file.creationTime / 1000),
                "last_update_date": datetime.fromtimestamp(file.modificationTime / 1000),
                "created_by": file.owner,
                "delete_ready": "Yes" if datetime.now() - timedelta(days=365 * 2) >= datetime.fromtimestamp(file.creationTime / 1000) else "No"
            }
            file_stats.append(file_info)
    return file_stats

# Get file stats recursively
file_stats = get_file_stats(dbfs_path)

# Create a DataFrame from the file stats
df = spark.createDataFrame(file_stats)

# Create a temporary table
df.createOrReplaceTempView("file_stats_table")

# Display the DataFrame
df.show()

# Query the temporary table
spark.sql("SELECT * FROM file_stats_table").show()
