import os
import datetime
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("DBFS File Stats") \
    .getOrCreate()

# Define the DBFS path
dbfs_path = "/dbfs/path/to/files"

# Get the current date
current_date = datetime.date.today()

# Function to retrieve file details recursively
def get_file_details(file_path):
    file_name = os.path.basename(file_path)
    file_size = os.path.getsize(file_path)
    file_creation_date = datetime.date.fromtimestamp(os.path.getctime(file_path))
    file_last_update_date = datetime.date.fromtimestamp(os.path.getmtime(file_path))
    file_created_by = os.stat(file_path).st_uid
    return file_name, file_size, file_path, file_creation_date, file_last_update_date, file_created_by

# Function to check if file is "delete ready"
def is_delete_ready(file_creation_date):
    return file_creation_date <= current_date - datetime.timedelta(days=365 * 2)

# Recursive function to get file details and create table
def get_files_and_create_table(directory):
    files = os.listdir(directory)
    for file in files:
        file_path = os.path.join(directory, file)
        if os.path.isdir(file_path):
            get_files_and_create_table(file_path)
        else:
            try:
                file_name, file_size, file_path, file_creation_date, file_last_update_date, file_created_by = get_file_details(file_path)
                delete_ready = is_delete_ready(file_creation_date)
                spark.sql(f"INSERT INTO temp_table VALUES ('{file_name}', {file_size}, '{file_path}', '{file_creation_date}', '{file_last_update_date}', '{file_created_by}', {delete_ready})")
            except OSError as e:
                print(f"Error: {e}")

# Create temporary table
spark.sql("CREATE OR REPLACE TEMPORARY VIEW temp_table (name STRING, size LONG, path STRING, creation_date DATE, last_update_date DATE, created_by STRING, delete_ready BOOLEAN)")

# Run the recursive function to get file details and populate the table
get_files_and_create_table(dbfs_path)

# Show the contents of the temporary table
spark.sql("SELECT * FROM temp_table").show()

# Stop the SparkSession
spark.stop()
