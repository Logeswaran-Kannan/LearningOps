from pyspark.sql import SparkSession
from datetime import datetime, timedelta

# Create SparkSession
spark = SparkSession.builder \
    .appName("DBFS File Stats") \
    .getOrCreate()

# DBFS path to scan
dbfs_path = "/path/to/scan"

# Get current date
current_date = datetime.now()

# Calculate date 2 years ago
two_years_ago = current_date - timedelta(days=365 * 2)

# Recursive function to retrieve file stats
def get_file_stats(path):
    stats = dbutils.fs.ls(path)
    file_stats = []
    for stat in stats:
        if stat.isDir():
            file_stats.extend(get_file_stats(stat.path))
        else:
            file_stats.append((stat.path, stat.name, stat.size, stat.creationTime, stat.modificationTime, stat.owner))
    return file_stats

# Retrieve file stats recursively
file_stats = get_file_stats(dbfs_path)

# Convert file stats to DataFrame
file_stats_df = spark.createDataFrame(file_stats, ["path", "name", "size", "creation_time", "last_update_time", "created_by"])

# Add delete ready column
file_stats_df = file_stats_df.withColumn("delete_ready", file_stats_df.creation_time < two_years_ago)

# Create temporary table
file_stats_df.createOrReplaceTempView("file_stats_table")
