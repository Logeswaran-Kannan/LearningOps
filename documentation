import os
import datetime
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.getOrCreate()

# Specify the DBFS path
dbfs_path = "/dbfs/path/to/files"

# Define a function to recursively get file information
def get_file_stats(path):
    file_stats = []
    for root, dirs, files in os.walk(path):
        for file in files:
            file_path = os.path.join(root, file)
            stat_info = os.stat(file_path)
            creation_date = datetime.datetime.fromtimestamp(stat_info.st_ctime)
            last_update_date = datetime.datetime.fromtimestamp(stat_info.st_mtime)
            current_date = datetime.datetime.now()
            is_delete_ready = "Yes" if (current_date - creation_date).days >= 730 else "No"  # 2 years = 730 days
            file_stats.append([file, stat_info.st_size, file_path, creation_date, last_update_date, is_delete_ready])
    return file_stats

# Get the file statistics
file_stats = get_file_stats(dbfs_path)

# Create a DataFrame from the file statistics
df = spark.createDataFrame(file_stats, ["Name", "Size", "Path", "CreationDate", "LastUpdateDate", "IsDeleteReady"])

# Create a temporary table with the DataFrame
df.createOrReplaceTempView("file_stats_temp")

# Display the temporary table
spark.sql("SELECT * FROM file_stats_temp").show()
