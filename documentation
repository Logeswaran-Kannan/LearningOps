from pyspark.sql import SparkSession
from pyspark.sql.functions import current_date

# Create a Spark session
spark = SparkSession.builder.appName("DBFS_File_Stats").getOrCreate()

# Set the DBFS path received from the user
dbfs_path = input("Enter the DBFS path: ")

# Get the current date
current_date_value = current_date()

# Read the DBFS files recursively
files_df = spark.sql(f"DESCRIBE EXTENDED {dbfs_path}")

# Extract the necessary columns
stats_df = files_df.select("name", "size", "path", "creationTime", "modificationTime")

# Add a column for recommendation based on last update date
stats_df = stats_df.withColumn("recommendation",
                               (current_date_value - stats_df.modificationTime) > 30)

# Create a permanent replaceable table in the default schema
table_name = "dbfs_files_stats"
stats_df.write.mode("overwrite").saveAsTable(table_name)

# Print the results
print(f"Statistics of DBFS files in '{dbfs_path}':")
stats_df.show()

# Stop the Spark session
spark.stop()
