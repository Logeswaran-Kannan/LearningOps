# Step 1: Import necessary libraries and set up Spark session
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from datetime import datetime, timedelta

# Step 2: Set up Spark session
spark = SparkSession.builder.appName("DBFS_File_Stats").getOrCreate()

# Step 3: Define a function to collect file statistics recursively
def collect_file_stats(base_path):
    file_stats = []
    fs = dbutils.fs.ls(base_path)
    for f in fs:
        if f.isDir():
            # If the entry is a directory, recursively call the function on it
            file_stats.extend(collect_file_stats(f.path))
        else:
            # If the entry is a file, collect its stats
            path = f.path
            name = path.split("/")[-1]
            size = f.size
            creation_date = datetime.fromtimestamp(f.modificationTime / 1000.0).strftime("%Y-%m-%d %H:%M:%S")
            last_update_date = datetime.fromtimestamp(f.modificationTime / 1000.0).strftime("%Y-%m-%d %H:%M:%S")
            del_status = ""
            del_date = ""
            
            # Check if the file is deletable based on last update date
            current_date = datetime.now()
            threshold_date = current_date - timedelta(days=30)  # Set a threshold of 30 days for deletability
            if datetime.fromtimestamp(f.modificationTime / 1000.0) < threshold_date:
                del_status = "Deletable"
                del_date = current_date.strftime("%Y-%m-%d %H:%M:%S")
            
            file_stats.append((name, size, path, creation_date, last_update_date, del_status, del_date))
    
    return file_stats

# Step 4: Collect file stats and create a DataFrame
base_dbfs_path = "/dbfs/path/to/your/folder"  # Input the desired DBFS path here
file_stats_data = collect_file_stats(base_dbfs_path)
file_stats_df = spark.createDataFrame(file_stats_data, ["name", "size", "path", "creation_date", "last_update_date", "del_status", "del_date"])

# Step 5: Save the DataFrame as a permanent table in the default schema
table_name = "file_stats_table"
file_stats_df.write.mode("append").saveAsTable(table_name)

# Display the table to verify the result
spark.sql(f"SELECT * FROM {table_name}").show()
