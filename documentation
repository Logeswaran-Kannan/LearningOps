import os
from datetime import datetime
from pyspark.sql import SparkSession

# Get the DBFS path from the base parameters
dbfs_path = "/dbfs/<your_dbfs_path>"

# Connect to Databricks cluster
spark = SparkSession.builder.getOrCreate()

# Function to check if a file's last update date is 2 years from the current date
def is_delete_ready(last_update_date):
    current_date = datetime.now()
    return (current_date - last_update_date).days >= 365 * 2

# Function to collect file stats
def collect_file_stats(directory):
    file_stats_list = []
    
    for root, dirs, files in dbutils.fs.ls(directory):
        for file in files:
            file_path = os.path.join(root, file.path)
            if dbutils.fs.isFile(file_path):
                file_stats = {
                    "name": file.name,
                    "size": file.size,
                    "path": file.path,
                    "creation_date": datetime.fromtimestamp(file.creationTime/1000),
                    "last_update_date": datetime.fromtimestamp(file.modificationTime/1000),
                    "delete_ready": is_delete_ready(datetime.fromtimestamp(file.modificationTime/1000))
                }
                file_stats_list.append(file_stats)
    
    return file_stats_list

# Collect file stats for the DBFS path and its child paths
file_stats_list = collect_file_stats(dbfs_path)

# Convert the list to a DataFrame
file_stats_df = spark.createDataFrame(file_stats_list)

# Save the DataFrame as a permanent replaceable table in the default schema
table_name = "file_stats"
file_stats_df.write.mode("overwrite").saveAsTable(table_name, format="parquet")
