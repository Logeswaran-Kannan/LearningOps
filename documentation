import os
from datetime import datetime, timedelta
from pyspark.sql import SparkSession

# Set up Spark session
spark = SparkSession.builder.getOrCreate()

# DBFS path to analyze
dbfs_path = "/dbfs/path/to/directory"

# Function to recursively get file stats
def get_file_stats(path):
    stats = []
    file_info = dbutils.fs.ls(path)
    for file in file_info:
        if file.isDir():
            stats.extend(get_file_stats(file.path))
        else:
            file_name = file.name
            file_size = file.size
            file_path = file.path
            creation_date = datetime.fromtimestamp(file.creationTime / 1000)
            last_update_date = datetime.fromtimestamp(file.modificationTime / 1000)
            created_by = file.name.split('/')[2]  # Assumes the third element in the path is the creator
            stats.append((file_name, file_size, file_path, creation_date, last_update_date, created_by))
    return stats

# Get file stats
file_stats = get_file_stats(dbfs_path)

# Filter files that are ready for deletion
two_years_ago = datetime.now() - timedelta(days=365*2)
deletion_ready_files = [file for file in file_stats if file[3] <= two_years_ago]

# Create DataFrame with the file stats
df = spark.createDataFrame(deletion_ready_files, ["name", "size", "path", "creation_date", "last_update_date", "created_by"])

# Register the DataFrame as a temporary table
df.createOrReplaceTempView("file_stats")

# Query the temporary table to show the results
spark.sql("SELECT * FROM file_stats").show()
