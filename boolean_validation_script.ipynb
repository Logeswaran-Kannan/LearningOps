{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql import SparkSession\n", "from pyspark.sql.functions import col, when\n", "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n", "from concurrent.futures import ThreadPoolExecutor\n", "\n", "# Initialize Spark session\n", "spark = SparkSession.builder.appName(\"AutomatedValidation\").getOrCreate()\n", "\n", "# Catalog and database details\n", "source_catalog = \"core_tst_sys9\"\n", "source_db = \"rr_source\"\n", "target_catalog = \"core_tst_std001\"\n", "target_db = \"ods\"\n", "\n", "# Table names to process\n", "table_names = [\"ptable1\", \"btable2\", \"ctable3\"]\n", "\n", "# Date filter\n", "filter_start = '2025-05-12T00:00:01.0025+00:00'\n", "filter_end = '2025-05-13T00:00:01.0025+00:00'\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def convert_boolean_columns(df):\n", "    converted_cols = []\n", "    for column in df.columns:\n", "        unique_vals = df.select(column).distinct().limit(100).rdd.flatMap(lambda x: x).collect()\n", "        string_vals = set(map(lambda x: str(x).lower() if x is not None else \"null\", unique_vals))\n", "        if string_vals.issubset({\"t\", \"f\", \"null\"}):\n", "            df = df.withColumn(column, when(col(column).isNull(), 0)\n", "                                       .when(col(column) == \"t\", 1)\n", "                                       .when(col(column) == \"f\", 0)\n", "                                       .otherwise(0))\n", "            converted_cols.append(column)\n", "    return df, converted_cols\n", "\n", "def to_lowercase_columns(df):\n", "    return df.select([col(c).alias(c.lower()) for c in df.columns])\n", "\n", "def cast_columns_to_common_type(source_df, target_df, columns):\n", "    for col_name in columns:\n", "        source_type = dict(source_df.dtypes).get(col_name)\n", "        target_type = dict(target_df.dtypes).get(col_name)\n", "        common_type = target_type if source_type != target_type else source_type\n", "        if common_type:\n", "            source_df = source_df.withColumn(col_name, col(col_name).cast(common_type))\n", "            target_df = target_df.withColumn(col_name, col(col_name).cast(common_type))\n", "    return source_df.select(columns), target_df.select(columns)\n", "\n", "def normalize_target_decimal_to_double(target_df):\n", "    for column, dtype in target_df.dtypes:\n", "        if dtype.startswith(\"decimal\"):\n", "            target_df = target_df.withColumn(column, col(column).cast(\"double\"))\n", "    return target_df\n", "\n", "def get_mismatched_columns(df1, df2, columns):\n", "    mismatched = []\n", "    for col_name in columns:\n", "        diff_count = df1.select(col_name).subtract(df2.select(col_name)).count()\n", "        reverse_diff_count = df2.select(col_name).subtract(df1.select(col_name)).count()\n", "        if diff_count > 0 or reverse_diff_count > 0:\n", "            mismatched.append(col_name)\n", "    return mismatched\n", "\n", "def compare_dataframes(source_comp_df, target_comp_df, common_columns):\n", "    source_only = source_comp_df.subtract(target_comp_df).count()\n", "    target_only = target_comp_df.subtract(source_comp_df).count()\n", "    mismatches = get_mismatched_columns(source_comp_df, target_comp_df, common_columns)\n", "    mismatches_target = get_mismatched_columns(target_comp_df, source_comp_df, common_columns)\n", "    return source_only, target_only, mismatches, mismatches_target\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}