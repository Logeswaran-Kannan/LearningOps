from pyspark.sql import SparkSession
from datetime import datetime, timedelta
import os

# Set the DBFS path
dbfs_path = "/dbfs/path/to/files"

# Set the threshold for deletable files (in days)
threshold = 30

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Get the list of files in the specified DBFS path recursively
file_list = dbutils.fs.ls(dbfs_path)

# Create an empty list to store the file statistics
file_stats = []

# Collect file statistics
for file_info in file_list:
    file_path = file_info.path
    file_name = file_info.name
    file_size = file_info.size
    file_creation_date = datetime.fromtimestamp(file_info.creationTime / 1000)
    file_last_update_date = datetime.fromtimestamp(file_info.modificationTime / 1000)
    del_status = "deletable" if (datetime.now() - file_last_update_date) > timedelta(days=threshold) else ""
    del_date = datetime.now().strftime("%Y-%m-%d")

    # Append file statistics to the list
    file_stats.append((file_name, file_size, file_path, file_creation_date, file_last_update_date, del_status, del_date))

# Create a DataFrame from the file statistics list
df = spark.createDataFrame(file_stats, ["name", "size", "path", "creation_date", "last_update_date", "del_status", "del_date"])

# Create a temporary view for the DataFrame
df.createOrReplaceTempView("file_stats")

# Create a table in the default schema using append mode
try:
    spark.sql("CREATE TABLE IF NOT EXISTS default.file_stats (name STRING, size BIGINT, path STRING, creation_date TIMESTAMP, last_update_date TIMESTAMP, del_status STRING, del_date DATE)")
except Exception as e:
    print(f"Error creating table: {e}")

# Insert the file statistics into the table, excluding duplicates
try:
    spark.sql("INSERT INTO default.file_stats SELECT * FROM file_stats EXCEPT SELECT * FROM default.file_stats")
    print("File statistics inserted successfully!")
except Exception as e:
    print(f"Error inserting file statistics: {e}")
