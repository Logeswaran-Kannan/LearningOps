import os
from datetime import datetime, timedelta
from pyspark.sql import SparkSession

# Set the DBFS path to analyze
dbfs_path = "/dbfs/path/to/analyze"

# Set the threshold for deletable files (30 days)
threshold_days = 30

# Initialize SparkSession
spark = SparkSession.builder.getOrCreate()

# Get the list of files and directories recursively
file_list = []
for root, dirs, files in os.walk(dbfs_path):
    for file_name in files:
        file_path = os.path.join(root, file_name)
        stat_info = os.stat(file_path)
        file_size = stat_info.st_size
        creation_time = datetime.fromtimestamp(stat_info.st_ctime)
        last_update_time = datetime.fromtimestamp(stat_info.st_mtime)
        deletable = (datetime.now() - last_update_time) >= timedelta(days=threshold_days)
        delete_date = datetime.now().strftime("%Y-%m-%d") if deletable else None
        file_list.append((file_name, file_size, file_path, creation_time, last_update_time, deletable, delete_date))

# Create a DataFrame from the file list
columns = ["name", "size", "path", "creation_date", "last_update_date", "del_status", "del_date"]
df = spark.createDataFrame(file_list, columns)

# Create a temporary view for the DataFrame
df.createOrReplaceTempView("file_stats")

# Create a permanent table with append mode in the default schema
table_name = "file_stats_table"
spark.sql(f"CREATE TABLE IF NOT EXISTS default.{table_name} USING DELTA LOCATION '/delta/{table_name}'")
spark.sql(f"INSERT INTO default.{table_name} SELECT * FROM file_stats")

# Show the table
spark.sql(f"SELECT * FROM default.{table_name}").show()
