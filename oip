import os
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# Function to collect file statistics recursively inside DBFS
def collect_file_stats(path, threshold_days):
    file_list = []
    for root, dirs, files in os.walk(path):
        for file in files:
            file_path = os.path.join(root, file)
            stat_info = os.stat(file_path)
            creation_date = datetime.fromtimestamp(stat_info.st_ctime)
            last_update_date = datetime.fromtimestamp(stat_info.st_mtime)
            size = stat_info.st_size
            deletable = (datetime.now() - last_update_date) > timedelta(days=threshold_days)
            del_status = "deletable" if deletable else "not deletable"
            del_date = datetime.now() if deletable else None

            file_list.append((file, size, file_path, creation_date, last_update_date, del_status, del_date))

    return file_list

# Define the DBFS path and threshold for deletable files (30 days)
dbfs_path = "/dbfs/path/to/directory"
threshold_days = 30

# Initialize Spark session
spark = SparkSession.builder.appName("FileStats").getOrCreate()

# Collect file statistics
file_stats = collect_file_stats(dbfs_path, threshold_days)

# Convert Python list to DataFrame
file_stats_df = spark.createDataFrame(file_stats, ["name", "size", "path", "creation_date", "last_update_date", "del_status", "del_date"])

# Define the primary key column name
primary_key_column = "name"

# Check if the table already exists
table_exists = spark.catalog._jcatalog.tableExists(primary_key_column)

# Insert data into the permanent table using append mode
try:
    if not table_exists:
        file_stats_df.write.saveAsTable(primary_key_column)
    else:
        file_stats_df.write.mode("append").insertInto(primary_key_column)
except Exception as e:
    # Handle duplicate rows (if any) using an exception
    print(f"Error occurred while inserting data into the table: {e}")

# Stop the Spark session
spark.stop()
