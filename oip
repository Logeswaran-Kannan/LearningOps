import os
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# Set the DBFS path to scan for files
dbfs_path = "/dbfs/path/to/files"

# Set the deletion threshold in days
deletion_threshold = 30

# Create a Spark session
spark = SparkSession.builder.getOrCreate()

# Collect file statistics
file_stats = []
for root, dirs, files in os.walk(dbfs_path):
    for file_name in files:
        file_path = os.path.join(root, file_name)
        file_size = os.path.getsize(file_path)
        creation_date = datetime.fromtimestamp(os.path.getctime(file_path))
        last_update_date = datetime.fromtimestamp(os.path.getmtime(file_path))
        del_status = "deletable" if last_update_date <= (datetime.now() - timedelta(days=deletion_threshold)) else "not deletable"
        del_date = datetime.now().strftime("%Y-%m-%d")
        
        file_stats.append((file_name, file_size, file_path, creation_date, last_update_date, del_status, del_date))

# Convert file statistics to a DataFrame
file_stats_df = spark.createDataFrame(file_stats, ["name", "size", "path", "creation_date", "last_update_date", "del_status", "del_date"])

# Append the DataFrame to a permanent table in the default schema
file_stats_df.write.mode("append").saveAsTable("default.file_stats")

# Display the file statistics table
file_stats_df.show()
