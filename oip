from pyspark.sql.functions import col, count, isnan, when
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType

def data_profile(database_name):
  # Get all table names in the database
  table_names = spark.catalog.listTables(database_name)
  
  # Filter out table names that start with '__'
  table_names = [table.name for table in table_names if not table.name.startswith('__')]
  
  # Iterate over all tables and columns
  column_profiles = []
  for table_name in table_names:
    table = spark.table(f"{database_name}.{table_name}")
    for column_name in table.columns:
      # Calculate null percentage
      null_count = table.select(count(when(isnan(col(column_name)) | col(column_name).isNull(), column_name))).collect()[0][0]
      total_count = table.count()
      null_percentage = null_count / total_count
      
      # Calculate data density
      data_density = 1 - null_percentage
      
      # Get all unique values in the column
      unique_values = [str(row[column_name]) for row in table.select(column_name).distinct().collect()]
      
      # Store the column profile in a list
      column_profile = (table_name, column_name, null_percentage, data_density, unique_values)
      column_profiles.append(column_profile)
  
  # Define the schema for the temporary table
  schema = StructType([
    StructField("table_name", StringType(), True),
    StructField("column_name", StringType(), True),
    StructField("null_percentage", DoubleType(), True),
    StructField("data_density", DoubleType(), True),
    StructField("unique_values", StringType(), True)
  ])
  
  # Create a DataFrame from the column profiles and store it in a temporary table
  column_profiles_df = spark.createDataFrame(column_profiles, schema=schema)
  column_profiles_df.createOrReplaceTempView("column_profiles")
