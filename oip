import os
from datetime import datetime, timedelta
from pyspark.sql import SparkSession

# Function to collect file statistics
def collect_file_stats(dbfs_path):
    file_stats = []
    for root, dirs, files in os.walk(dbfs_path):
        for file in files:
            file_path = os.path.join(root, file)
            stat = os.stat(file_path)
            file_name = os.path.basename(file_path)
            size = stat.st_size
            creation_date = datetime.fromtimestamp(stat.st_ctime)
            last_update_date = datetime.fromtimestamp(stat.st_mtime)
            del_status = 'deletable'
            del_date = datetime.now()
            recommend_deletable = (datetime.now() - last_update_date) > timedelta(days=30)
            file_stats.append((file_name, size, file_path, creation_date, last_update_date, del_status, del_date, recommend_deletable))
    return file_stats

# Configure SparkSession
spark = SparkSession.builder.appName("FileStats").getOrCreate()

# Define DBFS path
dbfs_input_path = "/dbfs/path/to/files"

# Collect file statistics
file_stats = collect_file_stats(dbfs_input_path)

# Convert file statistics to DataFrame
df = spark.createDataFrame(file_stats, ["name", "size", "path", "creation_date", "last_update_date", "del_status", "del_date", "recommend_deletable"])

# Append DataFrame to existing table in default schema or create a new table
table_name = "file_stats_table"
df.write.mode("append").saveAsTable(table_name)

# Check for duplicate rows and handle exceptions
try:
    spark.sql(f"ALTER TABLE {table_name} ADD COLUMNS (record_id STRING)")
except:
    pass

df.createOrReplaceTempView("temp_table")
spark.sql(f"""
    INSERT INTO TABLE {table_name}
    SELECT * FROM temp_table
    WHERE record_id NOT IN (
        SELECT record_id FROM {table_name}
    )
""")

# Print the updated table
spark.table(table_name).show()
