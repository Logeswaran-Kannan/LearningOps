import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Get the DBFS path from the dropdown widget
dbfs_path = dbutils.widgets.get("dbfs_path")

# Get the current date
current_date = datetime.datetime.now()

# Get the files metadata in the DBFS path
files_df = spark.sql(f"DESCRIBE EXTENDED {dbfs_path}")
files_df = files_df.select(
    col("name").alias("File"),
    col("size").alias("Size"),
    col("path").alias("Path"),
    col("creationTime").alias("Creation Date"),
    col("modificationTime").alias("Last Update Date")
)

# Add a new column indicating if the file is ready for deletion
two_years_ago = current_date - datetime.timedelta(days=365 * 2)
files_df = files_df.withColumn("Ready for Deletion", col("Creation Date") < two_years_ago)

# Show the file metadata
files_df.show()
---------------------------------------------------------------------------


dbutils.widgets.dropdown("dbfs_path", "/path/to/files", ["/path/to/files", "/another/path"])
----------------------------------------------------------------------------
