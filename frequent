from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.getOrCreate()

# Load the existing table with path and deletable columns
table_name = "your_table_name"
df = spark.table(table_name)

# Filter rows where deletable is set to "deletable"
deletable_rows = df.filter(df["deletable"] == "deletable")

# Iterate over each deletable row and delete the file
for row in deletable_rows.collect():
    # Get the path from the row (assuming the column name is "path")
    path = row["path"]

    # Remove "/dbfs" from the path
    path = path.replace("/dbfs", "")

    # Delete the file
    dbutils.fs.rm(path)

    # Update the "deletable" column to "deleted"
    df = df.withColumn("deletable", when(df["path"] == path, "deleted").otherwise(df["deletable"]))

# Save the updated DataFrame back to the table
df.write.mode("overwrite").saveAsTable(table_name)
