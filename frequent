from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Step 1: Initialize SparkSession
spark = SparkSession.builder.appName("DataComparison").getOrCreate()

# Step 2: Load the CSV file and create a table
csv_file_path = "dbfs:/path/to/csv_file.csv"
table_name = "temp_table"
df = spark.read.option("header", "true").csv(csv_file_path)
df.createOrReplaceTempView(table_name)

# Step 3: Check if the output table already exists and overwrite if necessary
output_table_name = "output_table"
if spark.catalog._jcatalog.tableExists(output_table_name):
    spark.sql(f"DROP TABLE IF EXISTS {output_table_name}")

# Step 4: Define the list of columns to compare
columns_to_compare = ["column1", "column2", "column3"]

# Step 5: Use policy reference to query the existing table and perform minus operation
existing_table_name = "existing_table"
policy_reference_column = "policy_reference"

query_template = """
    SELECT {columns}
    FROM {existing_table}
    WHERE {policy_reference_column} IN (SELECT DISTINCT {policy_reference_column} FROM {temp_table})
"""

columns_sql = ", ".join([f'IFNULL(t1.{col}, "") AS t1_{col}, IFNULL(t2.{col}, "") AS t2_{col}' for col in columns_to_compare])
query = query_template.format(columns=columns_sql, existing_table=existing_table_name, policy_reference_column=policy_reference_column, temp_table=table_name)

existing_data_df = spark.sql(query)
existing_data_df.createOrReplaceTempView("existing_data_view")

# Perform the "minus" operation
minus_conditions = " OR ".join([f't1.{col} <> t2.{col}' for col in columns_to_compare])
query = f"""
    SELECT t1.{policy_reference_column}, {columns_sql}
    FROM {table_name} t1
    LEFT JOIN existing_data_view t2
    ON t1.{policy_reference_column} = t2.{policy_reference_column}
    WHERE {minus_conditions}
"""

result_df = spark.sql(query)

# Step 6: Store the results in the output table
result_df.createOrReplaceTempView(output_table_name)

# Show the final result
result_df.show()

# Optionally, save the result back to DBFS or any other destination if needed.
# result_df.write.format("csv").save("dbfs:/path/to/save_results.csv")

# Stop the SparkSession
spark.stop()
