from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize SparkSession
spark = SparkSession.builder.getOrCreate()

# Load the existing table
existing_table = spark.table("your_existing_table")

# Filter the rows where del_status is "deletable"
deletable_rows = existing_table.filter(col("del_status") == "deletable")

# Iterate over the rows and delete the files
for row in deletable_rows.rdd.collect():
    file_path = row["path"].replace("/dbfs", "")
    dbutils.fs.rm(file_path, True)  # Delete the file

# Update the del_status column to "deleted" for the deleted files
existing_table = existing_table.withColumn("del_status", col("del_status").when(col("del_status") == "deletable", "deleted"))

# Save the updated table back to the same location
existing_table.write.mode("overwrite").saveAsTable("your_existing_table")
