import csv
import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import countDistinct

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Data Profiling") \
    .getOrCreate()

# Read database and table names from CSV
csv_file_path = "/path/to/csv_file.csv"
with open(csv_file_path, "r") as csv_file:
    csv_reader = csv.reader(csv_file)
    db_table_names = list(csv_reader)

# DBFS output location for profiling results
output_location = "dbfs:/path/to/output_location/"

# Perform data profiling for each table
for db_name, table_name in db_table_names:
    try:
        # Read table data
        df = spark.table(f"{db_name}.{table_name}")

        # Perform data profiling
        profiling_df = df.agg(*[
            countDistinct(col_name).alias(col_name)
            for col_name in df.columns
        ])

        # Prepare profiling output
        output_df = profiling_df \
            .selectExpr("*", f"current_date() AS profiling_date") \
            .withColumnRenamed("profiling_date", "currentdate")

        # Write profiling output to TXT file
        output_file_path = f"{output_location}/{db_name}_{table_name}_profiling.txt"
        output_df.write \
            .option("header", "true") \
            .option("sep", "\t") \
            .option("overwrite", "true") \
            .format("csv") \
            .mode("overwrite") \
            .save(output_file_path)

        print(f"Data profiling completed for table: {db_name}.{table_name}")

    except Exception as e:
        print(f"Error occurred while profiling table: {db_name}.{table_name}")
        print(f"Error message: {str(e)}")

# Stop SparkSession
spark.stop()
