import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Set the DBFS location to save the profiling output
output_location = "/dbfs/path/to/output.txt"

# Read the CSV file containing database and table names
csv_file_path = "/dbfs/path/to/input.csv"
df_csv = pd.read_csv(csv_file_path)

# Convert the pandas DataFrame to a Spark DataFrame
spark = SparkSession.builder.getOrCreate()
df_tables = spark.createDataFrame(df_csv)

# Iterate over each row in the DataFrame
for row in df_tables.collect():
    database = row['database']
    table = row['table']

    try:
        # Perform data profiling
        df = spark.table(f"{database}.{table}")
        profile_df = df.select(*[countDistinct(col(c)).alias(c) for c in df.columns])

        # Add columns for schema, tablename, and current date
        profile_df = profile_df.withColumn("schema", lit(database))
        profile_df = profile_df.withColumn("tablename", lit(table))
        profile_df = profile_df.withColumn("currentdate", current_date())

        # Calculate value contribution in %
        total_rows = df.count()
        profile_df = profile_df.withColumn("value_contribution", profile_df[profile_df.columns[1:]].sum() / total_rows * 100)

        # Save the profiling output in overwrite mode
        profile_df.write.mode("overwrite").format("csv").option("header", "true").save(output_location)

        # Move the output file to the desired DBFS location with TXT format
        output_file_path = f"{output_location}/{database}_{table}.txt"
        dbutils.fs.mv(output_location, output_file_path)

    except Exception as e:
        # Handle exceptions if table is not found
        print(f"Error profiling table {database}.{table}: {str(e)}")
