from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create SparkSession
spark = SparkSession.builder.getOrCreate()

# Read the existing table
existing_table = spark.table("your_existing_table")

# Filter rows with del_status = "deletable"
deletable_files = existing_table.filter(col("del_status") == "deletable")

# Iterate over the rows and delete files
for row in deletable_files.collect():
    file_path = row["path"].replace("/dbfs", "")  # Remove "/dbfs" from path
    dbutils.fs.rm(file_path, True)  # Delete the file recursively

# Update the original table with new del_status
updated_table = existing_table.withColumn("del_status",
                                           when(col("del_status") == "deletable", "deleted")
                                           .otherwise(col("del_status")))

# Save the updated table
updated_table.write.format("delta").mode("overwrite").saveAsTable("your_existing_table")
