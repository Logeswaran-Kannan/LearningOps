from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when

# Create SparkSession
spark = SparkSession.builder.getOrCreate()

# Read the existing table
existing_table = spark.table("your_existing_table")

# Filter rows with del_status = "deletable"
deletable_files = existing_table.filter(col("del_status") == "deletable")

# Iterate over the rows and delete files
for row in deletable_files.collect():
    file_path = row["path"].replace("/dbfs", "")  # Remove "/dbfs" from path
    dbutils.fs.rm(file_path, True)  # Delete the file recursively

# Update the original table with new del_status
updated_table = existing_table.withColumn("del_status",
                                           when(col("del_status") == "deletable", "deleted")
                                           .otherwise(col("del_status")))

# Save the updated table
updated_table.write.format("delta").mode("overwrite").saveAsTable("your_updated_table")

# Optimize the table to avoid table locking
spark.sql("OPTIMIZE your_updated_table")

# Drop the original table
spark.sql("DROP TABLE your_existing_table")

# Rename the updated table to the original table name
spark.sql("ALTER TABLE your_updated_table RENAME TO your_existing_table")
-----------------------------

ALTER TABLE your_table_name ALTER COLUMN size_col_name TYPE BIGINT;
---------------------------------

# Create SparkSession
spark = SparkSession.builder.getOrCreate()

# Alter the table and change the column data type
spark.sql("ALTER TABLE your_table_name ALTER COLUMN size TYPE Long")
