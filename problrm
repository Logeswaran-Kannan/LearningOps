import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_date, lit, when


def collect_file_stats(dbfs_path):
    # Remove "/dbfs" from the path
    dbfs_path = dbfs_path.replace("/dbfs", "")
    
    # Get file stats recursively
    file_stats = []
    for root, dirs, files in os.walk(dbfs_path):
        for file in files:
            file_path = os.path.join(root, file)
            stat = os.stat(file_path)
            name = os.path.basename(file_path)
            size = stat.st_size
            create_date = stat.st_ctime
            update_date = stat.st_mtime
            deletable = 1 if (current_date() - update_date) > 30 else 0
            del_date = current_date() if deletable else None
            
            file_stats.append((name, size, file_path, create_date, update_date, deletable, del_date))
    
    return file_stats


def insert_file_stats(spark, dbfs_path):
    # Collect file stats
    file_stats = collect_file_stats(dbfs_path)
    
    # Create DataFrame from the collected stats
    df = spark.createDataFrame(file_stats, ["name", "size", "path", "create_date", "update_date",
                                            "del_status", "del_date"])
    
    # Read the existing table
    existing_table = spark.table("your_existing_table")
    
    # Append the new stats to the existing table
    updated_table = existing_table.union(df)
    
    # Handle duplicate rows based on the path column
    updated_table = updated_table.dropDuplicates(["path"])
    
    # Write the updated table back to the existing table in append mode
    updated_table.write.mode("append").insertInto("your_existing_table")
    
    # Delete files marked as deletable and update del_status column
    deleted_files = updated_table.filter(updated_table["del_status"] == 1)
    deleted_paths = deleted_files.select("path").rdd.flatMap(lambda x: x).collect()
    
    for path in deleted_paths:
        try:
            os.remove(path)
        except Exception as e:
            print(f"Failed to delete file: {path}")
    
    # Update the original table with del_status = "deleted"
    existing_table = existing_table.withColumn("del_status", when(existing_table["path"].isin(deleted_paths), "deleted")
                                               .otherwise(existing_table["del_status"]))
    existing_table.write.mode("overwrite").insertInto("your_existing_table")


# Create SparkSession
spark = SparkSession.builder.getOrCreate()

# Call the function to insert file stats and perform deletions
insert_file_stats(spark, "/dbfs/path/to/dbfs_folder")
