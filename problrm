from pyspark.sql.functions import col
from pyspark.sql.utils import AnalysisException

# set the name of the database to be profiled
database_name = "my_database"

# get a list of table names in the database
tables = [t.name for t in spark.catalog.listTables(database_name)]

# create an empty list to store the profiling results for each table
results = []

# loop through each table and perform data profiling on its columns
for table_name in tables:
    try:
        # create a DataFrame of the table
        df = spark.table(f"{database_name}.{table_name}")

        # get a list of the column names in the DataFrame
        columns = df.columns

        # loop through each column and perform detailed data profiling
        for column_name in columns:
            # get the data type of the column
            data_type = str(df.schema[column_name].dataType)

            # perform the data profiling using Databricks' built-in functions
            column_profile = df.select(col(column_name)).summary()

            # add the profiling results to the list
            results.append((table_name, column_name, data_type, column_profile))
            
    except AnalysisException:
        # ignore tables that can't be accessed due to insufficient permissions
        continue

# save the profiling results to a file
with open("data_profiling_results.txt", "w") as f:
    for table_name, column_name, data_type, column_profile in results:
        f.write(f"Table: {table_name}, Column: {column_name}, Data Type: {data_type}\n")
        f.write(column_profile.toPandas().to_string(index=False))
        f.write("\n\n")
