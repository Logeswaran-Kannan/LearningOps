from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Define the database name
database_name = "your_database_name"

# Get the list of table names in the database
table_names = spark.catalog.listTables(database_name)

# Iterate over each table and retrieve the column list
for table in table_names:
    table_name = table.name

    # Exclude tables starting with "__app%"
    if not table_name.startswith("__app%"):
        
        # Get the DataFrame corresponding to the table
        df = spark.table(f"{database_name}.{table_name}")
        
        # Exclude "__START_AT" and "__END_AT" columns
        excluded_columns = ["__START_AT", "__END_AT"]
        columns = [col for col in df.columns if col not in excluded_columns]
        
        # Print the table name and its columns
        print(f"Table: {table_name}")
        print("Columns:")
        for col in columns:
            print(col)
        print("\n")
