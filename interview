import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, current_date
from pyspark.sql.types import StringType, IntegerType, TimestampType

# Set the DBFS path to scan
dbfs_path = "/dbfs/path/to/scan"

# Set the threshold for deletion (in days)
threshold_days = 30

# Initialize SparkSession
spark = SparkSession.builder.getOrCreate()

# Collect file statistics
file_stats = []
for root, dirs, files in os.walk(dbfs_path):
    for file in files:
        file_path = os.path.join(root, file)
        file_size = os.path.getsize(file_path)
        file_create_time = os.path.getctime(file_path)
        file_update_time = os.path.getmtime(file_path)
        file_stats.append((file, file_size, file_path, file_create_time, file_update_time))

# Create DataFrame from file statistics
df = spark.createDataFrame(file_stats, ["name", "size", "path", "create_date", "update_date"])
df = df.withColumn("del_status", lit("deletable"))
df = df.withColumn("del_date", current_date())

# Check for duplicates
existing_df = spark.table("your_existing_table")
existing_paths = existing_df.select("path").rdd.flatMap(lambda x: x).collect()
df = df.filter(~col("path").isin(existing_paths))

# Filter files based on threshold for deletion
current_date = spark.sql("SELECT current_date()").first()[0]
threshold_date = current_date.subtractExpr(f"INTERVAL {threshold_days} days")
df = df.withColumn("recommend_deletion", col("update_date") < threshold_date)

# Insert the new values into the existing table using insert mode
df.write.insertInto("your_existing_table", mode="append")

# Show the final DataFrame
df.show(truncate=False)
