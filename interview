import os
from datetime import datetime, timedelta
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder.getOrCreate()

# Function to collect file stats and check deletable status
def collect_file_stats(dbfs_path):
    stats_list = []
    today_date = datetime.now().strftime('%Y-%m-%d')
    threshold_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
    
    def process_file(path):
        stat = os.stat(path)
        name = os.path.basename(path)
        size = stat.st_size
        creation_date = datetime.fromtimestamp(stat.st_ctime).strftime('%Y-%m-%d')
        last_update_date = datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d')
        deletable = last_update_date <= threshold_date
        stats_list.append((name, size, path, creation_date, last_update_date, deletable, today_date))
    
    # Traverse the DBFS path recursively
    for root, _, files in os.walk(dbfs_path):
        for file in files:
            file_path = os.path.join(root, file)
            process_file(file_path)

    # Create a DataFrame from the collected stats list
    columns = ["name", "size", "path", "creation_date", "last_update_date", "deletable", "del_date"]
    df = spark.createDataFrame(stats_list, columns)
    
    # Insert into the existing table, handling duplicates
    df.write.mode("append").insertInto("your_existing_table", overwrite=False)

# Call the function with the desired DBFS path
dbfs_path = "/dbfs/path/to/your/folder"
collect_file_stats(dbfs_path)
