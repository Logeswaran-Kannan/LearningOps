from pyspark.sql import SparkSession
from pyspark.sql.functions import col, countDistinct, current_date

# Create a SparkSession
spark = SparkSession.builder.getOrCreate()

# Read the input CSV file containing the list of table names
table_names_df = spark.read.csv("/path/to/table_names.csv", header=True)

# Collect the table names from the DataFrame
table_names = table_names_df.select("table_name").rdd.flatMap(lambda x: x).collect()

# Iterate over the table names
for table_name in table_names:
    try:
        # Read the table data
        table_df = spark.table(table_name)

        # Perform data profiling using Spark functions
        profiling_df = table_df.select(
            col("schema"),
            col("tablename"),
            col("columnname"),
            countDistinct(col("columnname")).alias("distinct_value")
        ).withColumn(
            "value_contribution",
            col("distinct_value") / table_df.count() * 100
        ).withColumn(
            "date_of_profiling",
            current_date()
        )

        # Save the profiling output as a text file
        profiling_df.write.mode("overwrite").text(f"/path/to/output/{table_name}_profiling.txt")

    except Exception as e:
        print(f"Error processing table {table_name}: {str(e)}")
