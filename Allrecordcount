from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lit, regexp_replace

# Initialize Spark session
spark = SparkSession.builder.appName("TableCopyWithModifications").getOrCreate()

# Source table
source_table = "core_tst_sys9.default.bcr_schema_file"

# Read source table
df = spark.table(source_table)

# Filter out rows with special characters in TABLE_NAME (excluding underscores)
df_filtered = df.filter(~col("TABLE_NAME").rlike("[^a-zA-Z0-9_]") )

# Define modified datatype column
df_with_mod_type = df_filtered.withColumn(
    "MODIFIED_DATATYPE",
    when(col("DATA_TYPE") == "VARCHAR2", "STRING")
    .when(col("DATA_TYPE") == "TIMESTAMP(6)", "TIMESTAMP")
    .when(col("DATA_TYPE") == "SDO_GEOMETRY", "STRING")
    .when(
        (col("DATA_TYPE") == "NUMBER") & (col("DATA_PRECISION") > 0) & (col("DATA_SCALE") > 0),
        col("DATA_TYPE") + "(" + col("DATA_PRECISION").cast("string") + "," + col("DATA_SCALE").cast("string") + ")"
    )
    .when(col("DATA_TYPE") == "NUMBER", "DOUBLE")
    .when(col("DATA_TYPE") == "CLOB", "STRING")
    .when(col("DATA_TYPE") == "CHAR", "STRING")
    .otherwise(col("DATA_TYPE"))
)

# Add IN_SCOPE column
final_df = df_with_mod_type.withColumn("IN_SCOPE", lit(True))

# Define target table name
target_table = "core_tst_sys9.default.bcr_schema_file_modified"

# Save the new table
final_df.write.mode("overwrite").saveAsTable(target_table)

print(f"Table successfully created: {target_table}")
