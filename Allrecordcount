from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when, lit, array_except, current_timestamp
import time

# Initialize Spark Session
spark = SparkSession.builder.appName("Databricks_Table_Comparison").getOrCreate()

# Define catalog and databases
catalog = "core_tst_sys9"
src_db = "ods_views"
target_db = "ods"
output_table = "table_comparison_results"

# Fetch all table names from both databases
src_tables_df = spark.sql(f"SHOW TABLES IN {catalog}.{src_db}").select("tableName")
target_tables_df = spark.sql(f"SHOW TABLES IN {catalog}.{target_db}").select("tableName")

# Convert table lists
src_tables = [row["tableName"] for row in src_tables_df.collect()]
target_tables = [row["tableName"] for row in target_tables_df.collect()]

# Identify common tables for comparison
common_tables = list(set(src_tables).intersection(set(target_tables)))

# Prepare results list
results = []

# Start execution timer
start_time = time.time()

for table in common_tables:
    table_start_time = time.time()
    try:
        # Load source and target data
        df_src = spark.sql(f"SELECT * FROM {catalog}.{src_db}.{table}")
        df_target = spark.sql(f"SELECT * FROM {catalog}.{target_db}.{table}")

        # Get column lists
        src_columns = set(df_src.columns)
        target_columns = set(df_target.columns)

        # Identify missing columns
        src_missing_cols = list(target_columns - src_columns)  # In target but missing in source
        target_missing_cols = list(src_columns - target_columns)  # In source but missing in target

        # Drop mismatched columns before comparison
        common_columns = list(src_columns.intersection(target_columns))
        df_src = df_src.select(common_columns)
        df_target = df_target.select(common_columns)

        # Count comparison
        src_count = df_src.count()
        target_count = df_target.count()

        # Data row comparison
        df_src_except = df_src.exceptAll(df_target)  # Rows in source but not in target
        df_target_except = df_target.exceptAll(df_src)  # Rows in target but not in source
        src_vs_target_diff = df_src_except.count()
        target_vs_src_diff = df_target_except.count()

        # Column-wise null count comparison
        src_null_counts = df_src.select([count(when(col(c).isNull(), c)).alias(c) for c in common_columns]).collect()[0].asDict()
        target_null_counts = df_target.select([count(when(col(c).isNull(), c)).alias(c) for c in common_columns]).collect()[0].asDict()
        null_diff_columns = [col for col in common_columns if src_null_counts[col] != target_null_counts[col]]

        # Identify data mismatch columns
        mismatch_cols = []
        for col_name in common_columns:
            src_values = df_src.select(col_name).distinct()
            target_values = df_target.select(col_name).distinct()
            if src_values.exceptAll(target_values).count() > 0 or target_values.exceptAll(src_values).count() > 0:
                mismatch_cols.append(col_name)

        # Determine status & comment
        if not src_missing_cols and not target_missing_cols and src_count == target_count and src_vs_target_diff == 0 and target_vs_src_diff == 0 and not null_diff_columns and not mismatch_cols:
            status, comment = "PASS", "PASS"
        elif not src_missing_cols and not target_missing_cols and src_count != target_count:
            status, comment = "FAIL", "Count fails"
        elif not src_missing_cols and not target_missing_cols and src_vs_target_diff > 0 and target_vs_src_diff > 0:
            status, comment = "FAIL", "Data validation fails"
        elif not src_missing_cols and not target_missing_cols and not mismatch_cols and null_diff_columns:
            status, comment = "FAIL", "Null validation fails"
        else:
            status, comment = "FAIL", "DDL mismatch or data missing"

    except Exception as e:
        status, comment = "ERROR", f"Error processing table: {str(e)}"
        src_count, target_count, src_vs_target_diff, target_vs_src_diff, null_diff_columns, mismatch_cols = -1, -1, -1, -1, [], []

    # Capture run time per table
    table_end_time = time.time()
    runtime_seconds = round(table_end_time - table_start_time, 2)

    # Append results
    results.append((table, src_db, target_db, src_missing_cols, target_missing_cols, src_count, target_count, 
                    src_vs_target_diff, target_vs_src_diff, null_diff_columns, mismatch_cols, status, comment))

# Convert results to DataFrame
columns = ["table_name", "src_db", "target_db", "src_missing_columns", "target_missing_columns", 
           "src_count", "target_count", "src_vs_target_diff", "target_vs_src_diff", 
           "null_count_diff_columns", "data_mismatch_columns", "status", "comment"]
df_results = spark.createDataFrame(results, columns)

# Add run timestamp column
df_results = df_results.withColumn("run_timestamp", current_timestamp())

# Truncate and load results into the output table
df_results.write.mode("overwrite").saveAsTable(f"{catalog}.default.{output_table}")

# Display results in Databricks
display(df_results)

print(f"Script execution completed in {round(time.time() - start_time, 2)} seconds.")
