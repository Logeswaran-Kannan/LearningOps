from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, regexp_replace, lit, when
from pyspark.sql.types import StructType, StructField, StringType
import time

spark = SparkSession.builder.appName("ThreeWayDDLReconciliation").getOrCreate()

# Start timer
start_time = time.time()

# Parameters for testing
param_table_filter = [
    "billingcenter_sys9_pvw_change_bc_address",
    "billingcenter_sys9_bc_address",
    "billingcenter_sys9_pvw_change_bc_billinginstruction",
    "billingcenter_sys9_bc_billinginstruction"
]  # Use lowercase for matching

# 1. Load Oracle schema CSV
oracle_schema_path = "/mnt/data/oracle_schema.csv"
oracle_df = (
    spark.read.option("header", True).csv(oracle_schema_path)
    .filter(lower(col("IN_SCOPE")) == "true")
    .withColumn("TABLE_NAME", lower(col("TABLE_NAME")))
    .withColumn("COLUMN_NAME", lower(col("COLUMN_NAME")))
    .withColumn("DATA_TYPE", lower(col("MODIFIED_DATA_TYPE")).alias("DATA_TYPE"))
    .filter(~col("TABLE_NAME").rlike("[^a-z0-9_-]"))
    .filter(~col("COLUMN_NAME").isin("AZURE_LOAD_DATE", "DWH_FROM_DATE", "DWH_UNTIL_DATE", "ENT_FLAG"))
)

print(f"Oracle schema records loaded: {oracle_df.count()}")

# 2. Read Read Replica table metadata from information_schema
replica_catalog = "core_tst"
replica_db = "ODS"
spark.catalog.setCurrentCatalog(replica_catalog)
spark.catalog.setCurrentDatabase(replica_db)

replica_df = spark.read.table("information_schema.columns").filter(
    (lower(col("table_schema")) == replica_db.lower()) &
    ((col("table_name").startswith("billingcenter_") | col("table_name").startswith("policycenter_")) &
     col("table_name").contains("_sys9_")) |
    (col("table_name").startswith("claimcenter_"))
).filter(~col("table_name").contains("_pvw_change_") & ~col("table_name").contains("_delta"))

if param_table_filter:
    replica_df = replica_df.filter(lower(col("table_name")).isin([tbl.lower() for tbl in param_table_filter]))

replica_data = replica_df.select(
    lower(col("table_name")).alias("REPLICA_TABLE_NAME"),
    lower(col("column_name")).alias("REPLICA_COLUMN_NAME"),
    lower(col("full_data_type")).alias("REPLICA_DATA_TYPE"),
    lower(regexp_replace(regexp_replace(col("table_name"), "^billingcenter_sys9_", ""), "^policycenter_sys9_|^claimcenter_", "")).alias("TABLE_NAME")
)

print(f"Extracted {replica_data.count()} columns from REPLICA")

# 3. Read S3 table metadata from information_schema
s3_catalog = "core_tst_sys9"
s3_db = "ODS"
spark.catalog.setCurrentCatalog(s3_catalog)
spark.catalog.setCurrentDatabase(s3_db)

s3_df = spark.read.table("information_schema.columns").filter(
    (lower(col("table_schema")) == s3_db.lower()) &
    ((col("table_name").startswith("billingcenter_") | col("table_name").startswith("policycenter_")) &
     col("table_name").contains("_")) |
    (col("table_name").startswith("claimcenter_"))
).filter(~col("table_name").contains("_pvw_change_") & ~col("table_name").contains("_delta"))

if param_table_filter:
    s3_df = s3_df.filter(lower(col("table_name")).isin([tbl.lower() for tbl in param_table_filter]))

s3_data = s3_df.select(
    lower(col("table_name")).alias("S3_TABLE_NAME"),
    lower(col("column_name")).alias("S3_COLUMN_NAME"),
    lower(col("full_data_type")).alias("S3_DATA_TYPE"),
    lower(regexp_replace(regexp_replace(col("table_name"), "^billingcenter_sys9_", ""), "^policycenter_sys9_|^claimcenter_", "")).alias("TABLE_NAME")
)

print(f"Extracted {s3_data.count()} columns from S3")

# Normalize Oracle schema
temp_oracle_df = oracle_df.withColumn("SOURCE", lit("oracle"))
temp_oracle_df = temp_oracle_df.withColumn(
    "TABLE_NAME",
    lower(
        regexp_replace(
            regexp_replace(col("TABLE_NAME"), "^billingcenter_sys9_", ""),
            "^policycenter_sys9_|^claimcenter_", ""
        )
    )
)
temp_oracle_df = temp_oracle_df.select(
    col("TABLE_NAME"),
    col("COLUMN_NAME").alias("ORACLE_COLUMN_NAME"),
    col("DATA_TYPE").alias("ORACLE_DATA_TYPE")
)

# Join all datasets
combined_df = temp_oracle_df.join(replica_data, ["TABLE_NAME", col("ORACLE_COLUMN_NAME") == col("REPLICA_COLUMN_NAME")], "full_outer") \
    .join(s3_data, ["TABLE_NAME", col("ORACLE_COLUMN_NAME") == col("S3_COLUMN_NAME")], "full_outer")

final_df = combined_df.withColumn(
    "COMPARISON_RESULT",
    when(
        col("ORACLE_DATA_TYPE").isNotNull() & col("REPLICA_DATA_TYPE").isNotNull() & col("S3_DATA_TYPE").isNotNull(),
        when((col("ORACLE_DATA_TYPE") == col("REPLICA_DATA_TYPE")) & (col("REPLICA_DATA_TYPE") == col("S3_DATA_TYPE")), "MATCH")
        .otherwise("MISMATCH")
    ).when(
        col("ORACLE_DATA_TYPE").isNull() & col("REPLICA_DATA_TYPE").isNull() & col("S3_DATA_TYPE").isNull(), "ALL_MISSING"
    ).when(
        col("ORACLE_DATA_TYPE").isNotNull() & col("REPLICA_DATA_TYPE").isNull() & col("S3_DATA_TYPE").isNull(), "ORACLE_ONLY"
    ).when(
        col("ORACLE_DATA_TYPE").isNotNull() & col("REPLICA_DATA_TYPE").isNotNull() & col("S3_DATA_TYPE").isNull(), "ORACLE_REPLICA_ONLY"
    ).when(
        col("ORACLE_DATA_TYPE").isNotNull() & col("REPLICA_DATA_TYPE").isNull() & col("S3_DATA_TYPE").isNotNull(), "ORACLE_S3_ONLY"
    ).when(
        col("ORACLE_DATA_TYPE").isNull() & col("REPLICA_DATA_TYPE").isNotNull() & col("S3_DATA_TYPE").isNull(), "REPLICA_ONLY"
    ).when(
        col("ORACLE_DATA_TYPE").isNull() & col("REPLICA_DATA_TYPE").isNull() & col("S3_DATA_TYPE").isNotNull(), "S3_ONLY"
    ).when(
        col("ORACLE_DATA_TYPE").isNull() & col("REPLICA_DATA_TYPE").isNotNull() & col("S3_DATA_TYPE").isNotNull(), "REPLICA_S3_ONLY"
    )
)

# Write result to default schema
spark.catalog.setCurrentDatabase("default")
final_df.write.mode("overwrite").saveAsTable("DDL_COMPARE_TABLE")

# Log runtime
end_time = time.time()
print(f"DDL comparison completed in {round(end_time - start_time, 2)} seconds")
