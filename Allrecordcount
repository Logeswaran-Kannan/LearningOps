from pyspark.sql.functions import col, count, lit, expr, current_timestamp, max as spark_max, stddev, mean
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType
import datetime
import time

# Define catalog and databases
catalog = "core_tst_sys9"
source_db = f"{catalog}.ods"
target_db = f"{catalog}.ods_views"
output_table = f"{catalog}.default.data_migration_validation_results"
exploratory_output_table = f"{catalog}.default.data_migration_exploratory_result"

# Parameters
exclude_columns = {"DWH_FROM_DATE", "DWH_UNTIL_DATE", "DWH_CURRENT_FLAG"}
exclude_tables = set(spark.read.csv('dbfs:/path/to/exclusion_list.csv', header=True).filter(col('excludeindiciator') == 'Y').select('tablename').rdd.flatMap(lambda x: x).collect())  # Add table names to exclude
exclude_table_texts = ["_change_", "_sqldf"]  # Exclude tables containing this text

azure_load_date_start = "2024-10-08T09:07:48.000+00:00"
azure_load_date_end = "2024-10-10T09:07:48.000+00:00"

# Generate Run ID (incremental value)
try:
    max_run_id = spark.read.table(output_table).agg(spark_max("run_id")).collect()[0][0]
    run_id = 1 if max_run_id is None else max_run_id + 1
except Exception:
    run_id = 1

# Capture run timestamp
run_timestamp = datetime.datetime.now()

# Create Exploratory Table if not exists
try:
    spark.read.table(exploratory_output_table)
    print(f"Delta table {exploratory_output_table} exists.")
except Exception:
    print(f"Creating Delta table {exploratory_output_table}.")
    schema = StructType([
        StructField("run_id", IntegerType(), False),
        StructField("table_name", StringType(), False),
        StructField("src_database", StringType(), False),
        StructField("tgt_database", StringType(), False),
        StructField("missing_columns", StringType(), True),
        StructField("datatype_mismatch_columns", StringType(), True),
        StructField("null_count_diff_columns", StringType(), True),
        StructField("distinct_value_mismatch_columns", StringType(), True),
        StructField("string_length_violation_columns", StringType(), True),
        StructField("scientific_notation_columns", StringType(), True),
        StructField("non_float_columns_with_integer_values", StringType(), True),
        StructField("decimal_precision_violation_columns", StringType(), True),
        StructField("statistical_outlier_columns", StringType(), True),
        StructField("negative_values_in_unsigned_columns", StringType(), True),
        StructField("incorrect_date_format_columns", StringType(), True),
        StructField("status", StringType(), False),
        StructField("comment", StringType(), True),
        StructField("validation_timestamp", TimestampType(), True)
    ])
    spark.createDataFrame([], schema).write.format("delta").mode("overwrite").saveAsTable(exploratory_output_table)

# Fetch source tables
source_tables = [table.name for table in spark.catalog.listTables(source_db) if table.name not in exclude_tables and not any(text in table.name for text in exclude_table_texts)]

def validate_table(table_name):
    target_table_name = f"vm_{table_name}"
    
    try:
        src_df = spark.table(f"{source_db}.{table_name}")
        tgt_df = spark.table(f"{target_db}.{target_table_name}")
        
        common_cols = list(set(src_df.columns) & set(tgt_df.columns) - exclude_columns)
        missing_columns = list(set(src_df.columns) ^ set(tgt_df.columns))
        
        datatype_mismatches = [col for col in common_cols if src_df.schema[col].dataType != tgt_df.schema[col].dataType]
        null_count_diff_cols = [col for col in common_cols if src_df.filter(col(col).isNull()).count() != tgt_df.filter(col(col).isNull()).count()]
        distinct_mismatch_cols = [col for col in common_cols if src_df.select(col).distinct().count() != tgt_df.select(col).distinct().count()]
        string_length_violations = [col for col in common_cols if src_df.select(col).filter(expr(f'length({col}) > 250')).count() > 0]
        scientific_notation_cols = [col for col in common_cols if src_df.select(col).filter(expr(f'{col} like "%.E+%"')).count() > 0]
        decimal_precision_violations = [col for col in common_cols if src_df.select(col).filter(expr(f'abs({col}) - floor(abs({col})) > 1e-5')).count() > 0]
        outlier_cols = [col for col in common_cols if src_df.select(stddev(col), mean(col)).first()[0] and src_df.select(col).filter(col(col) > (src_df.select(mean(col)).first()[0] + 3 * src_df.select(stddev(col)).first()[0])).count() > 0]
        negative_unsigned_cols = [col for col in common_cols if src_df.select(col).filter(col(col) < 0).count() > 0]
        incorrect_date_format_cols = [col for col in common_cols if src_df.select(col).filter(expr(f"to_date({col}, 'yyyy-MM-dd') is null")).count() > 0]
        
        status = "PASS" if not any([missing_columns, datatype_mismatches, null_count_diff_cols, distinct_mismatch_cols, string_length_violations, scientific_notation_cols, decimal_precision_violations, outlier_cols, negative_unsigned_cols, incorrect_date_format_cols]) else "FAIL"
        comment = "Exploratory validation completed."
        
        result_data = [(run_id, table_name, source_db, target_db, ",".join(missing_columns), ",".join(datatype_mismatches), ",".join(null_count_diff_cols), ",".join(distinct_mismatch_cols), ",".join(string_length_violations), ",".join(scientific_notation_cols), ",".join(decimal_precision_violations), ",".join(outlier_cols), ",".join(negative_unsigned_cols), ",".join(incorrect_date_format_cols), status, comment, run_timestamp)]
        
        result_df = spark.createDataFrame(result_data, schema=spark.table(exploratory_output_table).schema)
        result_df.write.mode("append").saveAsTable(exploratory_output_table)
        
    except Exception as e:
        print(f"Error processing table {table_name}: {e}")

for table in source_tables:
    validate_table(table)

print("Exploratory Data Validation Completed.")
