from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, regexp_replace, expr, lit, when, coalesce, concat_ws
from pyspark.sql.types import StructType, StructField, StringType

# Initialize Spark session
spark = SparkSession.builder.appName("ThreeWayDDLReconciliation").getOrCreate()

# Parameters for testing
test_tables = ["some_table_1", "some_table_2"]  # Add your test table names here

# Load CSV schema file
schema_df = (
    spark.read.option("header", True).csv("/path/to/schema_file.csv")
    .filter(col("IN_SCOPE") == "true")
    .filter(~col("TABLE_NAME").rlike("[^\w-]"))
    .select("TABLE_NAME", "COLUMN_NAME", col("MODIFIED_DATA_TYPE").alias("DATA_TYPE"))
)

# Function to filter and normalize tables from a given catalog and database
def load_filtered_tables(catalog_name, db_name, prefix_filter):
    tables = spark.sql(f"SHOW TABLES IN {catalog_name}.{db_name}")

    filtered_tables = tables.filter(
        (
            (col("tableName").startswith("billingcenter_") & col("tableName").contains("_sys9_")) |
            (col("tableName").startswith("policycenter_") & col("tableName").contains("_sys9_")) |
            col("tableName").startswith("claimcenter_")
        ) & ~col("tableName").rlike("_pvw_change_|_delta")
    )

    # Apply test table filter after normalization
    def normalize_name(name):
        return name.replace("billingcenter_sys9_", "")\
                   .replace("policycenter_sys9_", "")\
                   .replace("claimcenter_", "")

    normalize_udf = expr(f"CASE WHEN tableName LIKE 'billingcenter\\_sys9\\_%' THEN regexp_replace(tableName, 'billingcenter_sys9_', '') "
                         f"WHEN tableName LIKE 'policycenter\\_sys9\\_%' THEN regexp_replace(tableName, 'policycenter_sys9_', '') "
                         f"WHEN tableName LIKE 'claimcenter\\_%' THEN regexp_replace(tableName, 'claimcenter_', '') ELSE tableName END")

    filtered_tables = filtered_tables.withColumn("NORMALIZED_NAME", normalize_udf)
    filtered_tables = filtered_tables.filter(col("NORMALIZED_NAME").isin(test_tables))

    table_list = [row["tableName"] for row in filtered_tables.select("tableName").collect()]

    all_columns = []
    for tbl in table_list:
        try:
            df = spark.table(f"{catalog_name}.{db_name}.{tbl}")
            for field in df.schema.fields:
                if field.name.upper() not in ["AZURE_LOAD_DATE", "DWH_FROM_DATE", "DWH_UNTIL_DATE", "ENT_FLAG"]:
                    norm_tbl = normalize_name(tbl)
                    all_columns.append((norm_tbl, field.name.upper(), field.dataType.simpleString().upper()))
        except Exception as e:
            print(f"Error reading table {tbl}: {e}")
            continue

    return spark.createDataFrame(all_columns, ["TABLE_NAME", "COLUMN_NAME", "DATA_TYPE"])

# Load data from Read Replica
read_replica_df = load_filtered_tables("core_tst", "ODS", ["billingcenter_", "policycenter_", "claimcenter_"])

# Load data from S3
s3_df = load_filtered_tables("core_tst_sys9", "ODS", ["billingcenter_", "policycenter_", "claimcenter_"])

# Normalize and deduplicate all sources
schema_df = schema_df.dropDuplicates(["TABLE_NAME", "COLUMN_NAME", "DATA_TYPE"])
read_replica_df = read_replica_df.dropDuplicates()
s3_df = s3_df.dropDuplicates()

# Join all three datasets for comparison
comparison_df = schema_df.alias("schema") \
    .join(read_replica_df.alias("replica"), ["TABLE_NAME", "COLUMN_NAME"], "outer") \
    .join(s3_df.alias("s3"), ["TABLE_NAME", "COLUMN_NAME"], "outer") \
    .select(
        coalesce(col("schema.TABLE_NAME"), col("replica.TABLE_NAME"), col("s3.TABLE_NAME")).alias("TABLE_NAME"),
        coalesce(col("schema.COLUMN_NAME"), col("replica.COLUMN_NAME"), col("s3.COLUMN_NAME")).alias("COLUMN_NAME"),
        col("schema.DATA_TYPE").alias("SCHEMA_DATA_TYPE"),
        col("replica.DATA_TYPE").alias("REPLICA_DATA_TYPE"),
        col("s3.DATA_TYPE").alias("S3_DATA_TYPE")
    )

# Add comparison result and mismatch detail
comparison_df = comparison_df \
    .withColumn(
        "RESULT",
        when(col("SCHEMA_DATA_TYPE").isNull() & col("REPLICA_DATA_TYPE").isNull() & col("S3_DATA_TYPE").isNull(), "ALL_MISSING")
        .when(col("SCHEMA_DATA_TYPE").isNotNull() & col("REPLICA_DATA_TYPE").isNull() & col("S3_DATA_TYPE").isNull(), "ONLY_SCHEMA_PRESENT")
        .when(col("SCHEMA_DATA_TYPE").isNotNull() & col("REPLICA_DATA_TYPE").isNull() & col("S3_DATA_TYPE").isNotNull(), "SCHEMA_AND_S3")
        .when(col("SCHEMA_DATA_TYPE").isNotNull() & col("REPLICA_DATA_TYPE").isNotNull() & col("S3_DATA_TYPE").isNull(), "SCHEMA_AND_REPLICA")
        .when(col("SCHEMA_DATA_TYPE").isNull() & col("REPLICA_DATA_TYPE").isNotNull() & col("S3_DATA_TYPE").isNull(), "ONLY_REPLICA_PRESENT")
        .when(col("SCHEMA_DATA_TYPE").isNull() & col("REPLICA_DATA_TYPE").isNull() & col("S3_DATA_TYPE").isNotNull(), "ONLY_S3_PRESENT")
        .when(col("SCHEMA_DATA_TYPE").isNotNull() & col("REPLICA_DATA_TYPE").isNotNull() & col("S3_DATA_TYPE").isNotNull() &
              (col("SCHEMA_DATA_TYPE") == col("REPLICA_DATA_TYPE")) &
              (col("REPLICA_DATA_TYPE") == col("S3_DATA_TYPE")), "MATCH")
        .otherwise("MISMATCH")
    ) \
    .withColumn(
        "MISMATCH_DETAIL",
        when(col("RESULT") != "MATCH",
             concat_ws(" | ",
                 when(col("SCHEMA_DATA_TYPE").isNull(), lit("SCHEMA: MISSING")).otherwise(concat_ws(": ", lit("SCHEMA"), col("SCHEMA_DATA_TYPE"))),
                 when(col("REPLICA_DATA_TYPE").isNull(), lit("REPLICA: MISSING")).otherwise(concat_ws(": ", lit("REPLICA"), col("REPLICA_DATA_TYPE"))),
                 when(col("S3_DATA_TYPE").isNull(), lit("S3: MISSING")).otherwise(concat_ws(": ", lit("S3"), col("S3_DATA_TYPE")))
             )
        ).otherwise(lit(""))
    )

# Write output to a Delta table
comparison_df.write.format("delta").mode("overwrite").saveAsTable("DDL_COMPARE_TABLE")
