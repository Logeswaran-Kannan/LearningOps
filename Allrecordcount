from pyspark.sql.functions import col, when, upper, lit, concat
from pyspark.sql import functions as F

# Load the data from the original table
source_table = "core_tst_sys9.default.readreplica_schema_file"
df = spark.table(source_table)

# Filter out table names containing special characters except underscore (_)
df_filtered = df.filter(~col("TABLE_NAME").rlike("[^A-Za-z0-9_]+"))

# Add MODIFIED_COLUMN based on logic
udt_upper = upper(col("udt_name"))
df_transformed = df_filtered.withColumn(
    "MODIFIED_COLUMN",
    when(udt_upper == "VARCHAR", "VARCHAR2")
    .when(udt_upper == "TIMESTAMP", "TIMESTAMP(6)")
    .when(udt_upper == "GEOGRAPHY", "SDO_GEOMETRY")
    .when(
        udt_upper.like("%NUMERIC%") &
        col("numeric_precision").isNotNull() & col("numeric_scale").isNotNull() &
        (col("numeric_precision") > 0) & (col("numeric_scale") > 0),
        concat(lit("DECIMAL("), col("numeric_precision").cast("string"), lit(","), col("numeric_scale").cast("string"), lit(")"))
    )
    .when(udt_upper.like("%NUMERIC%"), "DOUBLE")
    .when(udt_upper == "TEXT", "CLOB")
    .when(udt_upper == "BYTEA", "BLOB")
    .when(udt_upper == "BOOL", "CHAR")
    .when(udt_upper.like("%INT%"), "NUMBER")
    .otherwise(col("udt_name"))
)

# Add IN_SCOPE column (setting it to True by default, customize as needed)
df_transformed = df_transformed.withColumn("IN_SCOPE", lit(True))

# Define the name of the target table
target_table = "core_tst_sys9.default.readreplica_schema_file_copy"

# Write the transformed DataFrame to the new table
df_transformed.write.mode("overwrite").saveAsTable(target_table)

print(f"Table '{target_table}' created successfully with transformed data.")
