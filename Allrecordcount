from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, regexp_replace, lit, when
from pyspark.sql.types import StructType, StructField, StringType
import time

spark = SparkSession.builder.appName("ThreeWayDDLReconciliation").getOrCreate()

# Start timer
start_time = time.time()

# Parameters for testing
param_table_filter = [
    "billingcenter_sys9_pvw_change_bc_address",
    "billingcenter_sys9_bc_address",
    "billingcenter_sys9_pvw_change_bc_billinginstruction",
    "billingcenter_sys9_bc_billinginstruction"
]  # Use lowercase for matching

# 1. Load Oracle schema CSV
oracle_schema_path = "/mnt/data/oracle_schema.csv"
oracle_df = (
    spark.read.option("header", True).csv(oracle_schema_path)
    .filter(lower(col("IN_SCOPE")) == "true")
    .withColumn("TABLE_NAME", lower(col("TABLE_NAME")))
    .withColumn("COLUMN_NAME", lower(col("COLUMN_NAME")))
    .withColumn("DATA_TYPE", lower(col("MODIFIED_DATA_TYPE")).alias("DATA_TYPE"))
    .filter(~col("TABLE_NAME").rlike("[^a-z0-9_-]"))
    .filter(~col("COLUMN_NAME").isin("azure_load_date", "dwh_from_date", "dwh_until_date", "ent_flag"))
)

print(f"Oracle schema records loaded: {oracle_df.count()}")

# 2. Read Read Replica tables
replica_catalog = "core_tst"
replica_db = "ODS"
spark.catalog.setCurrentCatalog(replica_catalog)
spark.catalog.setCurrentDatabase(replica_db)
replica_tables = spark.catalog.listTables()

replica_filtered = [
    t.name for t in replica_tables
    if (
        (t.name.startswith("billingcenter_") or t.name.startswith("policycenter_")) and ("_sys9_" in t.name)
        or t.name.startswith("claimcenter_")
    ) and not ("_pvw_change_" in t.name or "_delta" in t.name)
]
if param_table_filter:
    replica_filtered = [tbl for tbl in replica_filtered if tbl.lower() in param_table_filter]
print(f"Filtered Read Replica tables: {len(replica_filtered)}")

# 3. Read S3 tables
s3_catalog = "core_tst_sys9"
s3_db = "ODS"
spark.catalog.setCurrentCatalog(s3_catalog)
spark.catalog.setCurrentDatabase(s3_db)
s3_tables = spark.catalog.listTables()

s3_filtered = [
    t.name for t in s3_tables
    if (
        (t.name.startswith("billingcenter_") or t.name.startswith("policycenter_")) and ("_sys9_" in t.name)
        or t.name.startswith("claimcenter_")
    ) and not ("_pvw_change_" in t.name or "_delta" in t.name)
]
if param_table_filter:
    s3_filtered = [tbl for tbl in s3_filtered if tbl.lower() in param_table_filter]
print(f"Filtered S3 tables: {len(s3_filtered)}")

def extract_columns(catalog, db, tables, source):
    spark.catalog.setCurrentCatalog(catalog)
    spark.catalog.setCurrentDatabase(db)
    rows = []
    for tbl in tables:
        try:
            df = spark.read.table(f"{catalog}.{db}.{tbl}").selectExpr("* LIMIT 1")  # Faster metadata read
            schema_fields = df.schema.fields
            for field in schema_fields:
                if field.name.lower() not in ["azure_load_date", "dwh_from_date", "dwh_until_date", "ent_flag"]:
                    normalized_name = (
                        tbl.replace("billingcenter_sys9_", "")
                        .replace("policycenter_sys9_", "")
                        .replace("claimcenter_", "")
                        .lower()
                    )
                    if not param_table_filter or tbl.lower() in param_table_filter:
                        rows.append((normalized_name, field.name.lower(), field.dataType.simpleString().lower(), source))
        except Exception as e:
            print(f"Failed to read table {tbl}: {e}")
    print(f"Extracted {len(rows)} columns from {source}")
    return rows

replica_data = extract_columns(replica_catalog, replica_db, replica_filtered, "REPLICA")
s3_data = extract_columns(s3_catalog, s3_db, s3_filtered, "S3")

oracle_data = oracle_df.withColumn("SOURCE", lit("oracle"))
oracle_data = oracle_data.withColumn(
    "NORMALIZED_TABLE_NAME",
    lower(
        regexp_replace(
            regexp_replace(col("TABLE_NAME"), "^billingcenter_sys9_", ""),
            "^policycenter_sys9_|^claimcenter_", ""
        )
    )
)
oracle_data = oracle_data.select(
    col("NORMALIZED_TABLE_NAME").alias("TABLE_NAME"), "COLUMN_NAME", "DATA_TYPE", "SOURCE"
)

schema = StructType([
    StructField("TABLE_NAME", StringType(), True),
    StructField("COLUMN_NAME", StringType(), True),
    StructField("DATA_TYPE", StringType(), True),
    StructField("SOURCE", StringType(), True),
])

combined_df = spark.createDataFrame(replica_data + s3_data, schema).unionByName(oracle_data)
print(f"Total combined records: {combined_df.count()}")

# Pivot the table
pivot_df = combined_df.groupBy("TABLE_NAME", "COLUMN_NAME").pivot("SOURCE", ["oracle", "REPLICA", "S3"]).agg(
    {"DATA_TYPE": "first"}
)

# Add comparison result
result_df = pivot_df.withColumn(
    "COMPARISON_RESULT",
    when(
        col("oracle").isNotNull() & col("REPLICA").isNotNull() & col("S3").isNotNull(),
        when((col("oracle") == col("REPLICA")) & (col("REPLICA") == col("S3")), "MATCH")
        .otherwise("MISMATCH")
    ).when(
        col("oracle").isNull() & col("REPLICA").isNull() & col("S3").isNull(), "ALL_MISSING"
    ).when(
        col("oracle").isNotNull() & col("REPLICA").isNull() & col("S3").isNull(), "ORACLE_ONLY"
    ).when(
        col("oracle").isNotNull() & col("REPLICA").isNotNull() & col("S3").isNull(), "ORACLE_REPLICA_ONLY"
    ).when(
        col("oracle").isNotNull() & col("REPLICA").isNull() & col("S3").isNotNull(), "ORACLE_S3_ONLY"
    ).when(
        col("oracle").isNull() & col("REPLICA").isNotNull() & col("S3").isNull(), "REPLICA_ONLY"
    ).when(
        col("oracle").isNull() & col("REPLICA").isNull() & col("S3").isNotNull(), "S3_ONLY"
    ).when(
        col("oracle").isNull() & col("REPLICA").isNotNull() & col("S3").isNotNull(), "REPLICA_S3_ONLY"
    )
)

# 4. Write result to DDL_COMPARE_TABLE
result_df.write.mode("overwrite").saveAsTable("DDL_COMPARE_TABLE")

# Log runtime
end_time = time.time()
print(f"DDL comparison completed in {round(end_time - start_time, 2)} seconds")



Ideally, this shouldn't be an issueâ€”as per our discussion with Paul yesterday, he confirmed that there's an alternate approach to access data from the read replica in Databricks via the staging layer.

For manual access to PostgreSQL, it should work fine using this tool.
Can we go ahead and raise a request to have this tool installed on our machines?
