from pyspark.sql import SparkSession
from pyspark.sql.functions import col, upper, regexp_replace, lit, when
from pyspark.sql.types import StructType, StructField, StringType
import time

spark = SparkSession.builder.appName("ThreeWayDDLReconciliation").getOrCreate()

# Start timer
start_time = time.time()

# Parameters for testing
param_table_filter = ["ACCOUNT", "CLAIM"]  # Add desired normalized table names for filtering

# 1. Load Oracle schema CSV
oracle_schema_path = "/mnt/data/oracle_schema.csv"
oracle_df = (
    spark.read.option("header", True).csv(oracle_schema_path)
    .filter(upper(col("IN_SCOPE")) == "TRUE")
    .withColumn("TABLE_NAME", upper(col("TABLE_NAME")))
    .withColumn("COLUMN_NAME", upper(col("COLUMN_NAME")))
    .withColumn("DATA_TYPE", upper(col("MODIFIED_DATA_TYPE")).alias("DATA_TYPE"))
    .filter(~col("TABLE_NAME").rlike("[^A-Z0-9_-]"))
    .filter(~col("COLUMN_NAME").isin("AZURE_LOAD_DATE", "DWH_FROM_DATE", "DWH_UNTIL_DATE", "ENT_FLAG"))
)

print(f"Oracle schema records loaded: {oracle_df.count()}")

# 2. Read Read Replica tables
replica_catalog = "core_tst"
replica_db = "ODS"
spark.catalog.setCurrentCatalog(replica_catalog)
spark.catalog.setCurrentDatabase(replica_db)
replica_tables = spark.catalog.listTables()

replica_filtered = [
    t.name for t in replica_tables
    if (
        (t.name.startswith("billingcenter_") or t.name.startswith("policycenter_")) and ("_sys9_" in t.name)
        or t.name.startswith("claimcenter_")
    ) and not ("_pvw_change_" in t.name or "_delta" in t.name)
]
print(f"Filtered Read Replica tables: {len(replica_filtered)}")

# 3. Read S3 tables
s3_catalog = "core_tst_sys9"
s3_db = "ODS"
spark.catalog.setCurrentCatalog(s3_catalog)
spark.catalog.setCurrentDatabase(s3_db)
s3_tables = spark.catalog.listTables()

s3_filtered = [
    t.name for t in s3_tables
    if (
        (t.name.startswith("billingcenter_") or t.name.startswith("policycenter_")) and ("_sys9_" in t.name)
        or t.name.startswith("claimcenter_")
    ) and not ("_pvw_change_" in t.name or "_delta" in t.name)
]
print(f"Filtered S3 tables: {len(s3_filtered)}")

def extract_columns(catalog, db, tables, source):
    spark.catalog.setCurrentCatalog(catalog)
    spark.catalog.setCurrentDatabase(db)
    rows = []
    for tbl in tables:
        try:
            df = spark.read.table(f"{catalog}.{db}.{tbl}").selectExpr("* LIMIT 1")  # Faster metadata read
            schema_fields = df.schema.fields
            for field in schema_fields:
                if field.name.upper() not in ["AZURE_LOAD_DATE", "DWH_FROM_DATE", "DWH_UNTIL_DATE", "ENT_FLAG"]:
                    normalized_name = (
                        tbl.replace("billingcenter_sys9_", "")
                        .replace("policycenter_sys9_", "")
                        .replace("claimcenter_", "")
                        .upper()
                    )
                    if normalized_name in param_table_filter:
                        rows.append((normalized_name, field.name.upper(), field.dataType.simpleString().upper(), source))
        except Exception as e:
            print(f"Failed to read table {tbl}: {e}")
    print(f"Extracted {len(rows)} columns from {source}")
    return rows

replica_data = extract_columns(replica_catalog, replica_db, replica_filtered, "REPLICA")
s3_data = extract_columns(s3_catalog, s3_db, s3_filtered, "S3")

oracle_data = oracle_df.withColumn("SOURCE", lit("ORACLE"))
oracle_data = oracle_data.withColumn(
    "NORMALIZED_TABLE_NAME",
    upper(
        regexp_replace(
            regexp_replace(col("TABLE_NAME"), "^BILLINGCENTER_SYS9_", ""),
            "^POLICYCENTER_SYS9_|^CLAIMCENTER_", ""
        )
    )
)
oracle_data = oracle_data.select(
    col("NORMALIZED_TABLE_NAME").alias("TABLE_NAME"), "COLUMN_NAME", "DATA_TYPE", "SOURCE"
)

schema = StructType([
    StructField("TABLE_NAME", StringType(), True),
    StructField("COLUMN_NAME", StringType(), True),
    StructField("DATA_TYPE", StringType(), True),
    StructField("SOURCE", StringType(), True),
])

combined_df = spark.createDataFrame(replica_data + s3_data, schema).unionByName(oracle_data)
print(f"Total combined records: {combined_df.count()}")

# Pivot the table
pivot_df = combined_df.groupBy("TABLE_NAME", "COLUMN_NAME").pivot("SOURCE", ["ORACLE", "REPLICA", "S3"]).agg(
    {"DATA_TYPE": "first"}
)

# Add comparison result
result_df = pivot_df.withColumn(
    "COMPARISON_RESULT",
    when(
        col("ORACLE").isNotNull() & col("REPLICA").isNotNull() & col("S3").isNotNull(),
        when((col("ORACLE") == col("REPLICA")) & (col("REPLICA") == col("S3")), "MATCH")
        .otherwise("MISMATCH")
    ).when(
        col("ORACLE").isNull() & col("REPLICA").isNull() & col("S3").isNull(), "ALL_MISSING"
    ).when(
        col("ORACLE").isNotNull() & col("REPLICA").isNull() & col("S3").isNull(), "ORACLE_ONLY"
    ).when(
        col("ORACLE").isNotNull() & col("REPLICA").isNotNull() & col("S3").isNull(), "ORACLE_REPLICA_ONLY"
    ).when(
        col("ORACLE").isNotNull() & col("REPLICA").isNull() & col("S3").isNotNull(), "ORACLE_S3_ONLY"
    ).when(
        col("ORACLE").isNull() & col("REPLICA").isNotNull() & col("S3").isNull(), "REPLICA_ONLY"
    ).when(
        col("ORACLE").isNull() & col("REPLICA").isNull() & col("S3").isNotNull(), "S3_ONLY"
    ).when(
        col("ORACLE").isNull() & col("REPLICA").isNotNull() & col("S3").isNotNull(), "REPLICA_S3_ONLY"
    )
)

# 4. Write result to DDL_COMPARE_TABLE
result_df.write.mode("overwrite").saveAsTable("DDL_COMPARE_TABLE")

# Log runtime
end_time = time.time()
print(f"DDL comparison completed in {round(end_time - start_time, 2)} seconds")
