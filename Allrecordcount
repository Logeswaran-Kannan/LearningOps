import time
from concurrent.futures import ThreadPoolExecutor

# Define workflow name
workflow_name = "UKS_SIT_DW_E2E_Workflow"

# Define tasks with dependencies and job cluster configuration
tasks = {
    "ValidationTask": {
        "notebook_path": "/Workspace/Repos/ODS_SIT_AUTOMATION_POC/notebooks/UKS_SIT_DW_E2E/ODS_BRONZE_VALIDATION_ESSENTIAL",
        "depends_on": [],
        "cluster_config": {
            "num_workers": 2,
            "spark_version": "13.3.x-scala2.12",
            "node_type_id": "Standard_DS3_v2"
        }
    },
    "DLT_PIPELINE": {
        "notebook_path": "/Workspace/Repos/ODS_SIT_AUTOMATION_POC/notebooks/UKS_SIT_DW_E2E/DLT_REFRESH_PIPELINE",
        "depends_on": ["ValidationTask"],
        "cluster_config": {
            "num_workers": 2,
            "spark_version": "13.3.x-scala2.12",
            "node_type_id": "Standard_DS3_v2"
        }
    }
}

# Function to run a notebook task on a job cluster
def run_notebook_on_cluster(task_key):
    task = tasks[task_key]
    notebook_path = task["notebook_path"]
    
    # Define cluster configuration
    cluster_settings = task["cluster_config"]

    try:
        print(f"üöÄ [{task_key}] Starting execution on a job cluster...")

        # Launch notebook with a new job cluster
        result = dbutils.notebook.run(
            notebook_path, 
            timeout_seconds=0, 
            arguments={
                "num_workers": cluster_settings["num_workers"],
                "spark_version": cluster_settings["spark_version"],
                "node_type_id": cluster_settings["node_type_id"]
            }
        )

        print(f"‚úÖ [{task_key}] Completed successfully - Result: {result}")
        return True
    except Exception as e:
        print(f"‚ùå [{task_key}] Failed: {str(e)}")
        return False

# Function to execute workflow with dependency management
def execute_workflow():
    print(f"üöÄ Executing Workflow: {workflow_name}")

    executed_tasks = {}

    while len(executed_tasks) < len(tasks):
        for task_key, task_info in tasks.items():
            # If task is already executed, skip it
            if task_key in executed_tasks:
                continue
            
            # Check if dependencies are met
            if all(dep in executed_tasks for dep in task_info["depends_on"]):
                # Run task on a job cluster and mark as executed
                if run_notebook_on_cluster(task_key):
                    executed_tasks[task_key] = True
                else:
                    print(f"‚ùå Workflow Stopped due to failure in {task_key}")
                    return
    
    print(f"‚úÖ Workflow Execution Completed: {workflow_name}")

# Run the workflow
execute_workflow()
