from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when, lit, array_except, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType
import time

# Initialize Spark Session
spark = SparkSession.builder.appName("Databricks_Table_Comparison").getOrCreate()

# Define catalog and databases
catalog = "core_tst_sys9"
src_db = "ods"           # Source database
target_db = "ods_views"  # Target database
output_table = "table_comparison_results"

# Fetch table list from the source database (ODS)
src_tables_df = spark.sql(f"SHOW TABLES IN {catalog}.{src_db}").select("tableName")
src_tables = [row["tableName"] for row in src_tables_df.collect()]

# Construct target table names (prepend "vw_" to each source table name)
target_tables = {f"vw_{table}": table for table in src_tables}  # Mapping {target_table: source_table}

# Fetch actual tables in target database (ODS_VIEWS)
target_tables_df = spark.sql(f"SHOW TABLES IN {catalog}.{target_db}").select("tableName")
existing_target_tables = set(row["tableName"] for row in target_tables_df.collect())

# Identify tables that exist in both source and target
common_tables = {tgt: src for tgt, src in target_tables.items() if tgt in existing_target_tables}

# Define Schema Explicitly
schema = StructType([
    StructField("table_name", StringType(), True),
    StructField("src_db", StringType(), True),
    StructField("target_db", StringType(), True),
    StructField("src_missing_columns", ArrayType(StringType()), True),
    StructField("target_missing_columns", ArrayType(StringType()), True),
    StructField("src_count", IntegerType(), True),
    StructField("target_count", IntegerType(), True),
    StructField("src_vs_target_diff", IntegerType(), True),
    StructField("target_vs_src_diff", IntegerType(), True),
    StructField("null_count_diff_columns", ArrayType(StringType()), True),
    StructField("data_mismatch_columns", ArrayType(StringType()), True),
    StructField("status", StringType(), True),
    StructField("comment", StringType(), True),
    StructField("run_timestamp", StringType(), True)
])

# Prepare results list
results = []

# Start execution timer
start_time = time.time()

for target_table, source_table in common_tables.items():
    table_start_time = time.time()
    try:
        # Load source and target data
        df_src = spark.sql(f"SELECT * FROM {catalog}.{src_db}.{source_table}")
        df_target = spark.sql(f"SELECT * FROM {catalog}.{target_db}.{target_table}")

        # Get column lists
        src_columns = set(df_src.columns)
        target_columns = set(df_target.columns)

        # Identify missing columns
        src_missing_cols = list(target_columns - src_columns)  # In target but missing in source
        target_missing_cols = list(src_columns - target_columns)  # In source but missing in target

        # Drop mismatched columns before comparison
        common_columns = list(src_columns.intersection(target_columns))
        if not common_columns:  # If no common columns, log and continue
            results.append((target_table, src_db, target_db, src_missing_cols, target_missing_cols, 0, 0, 0, 0, [], [], "FAIL", "No common columns", str(current_timestamp())))
            continue

        df_src = df_src.select(common_columns)
        df_target = df_target.select(common_columns)

        # Count comparison
        src_count = df_src.count()
        target_count = df_target.count()

        # Data row comparison
        df_src_except = df_src.exceptAll(df_target)  # Rows in source but not in target
        df_target_except = df_target.exceptAll(df_src)  # Rows in target but not in source
        src_vs_target_diff = df_src_except.count()
        target_vs_src_diff = df_target_except.count()

        # Column-wise null count comparison
        src_null_counts = df_src.select([count(when(col(c).isNull(), c)).alias(c) for c in common_columns]).collect()[0].asDict()
        target_null_counts = df_target.select([count(when(col(c).isNull(), c)).alias(c) for c in common_columns]).collect()[0].asDict()
        null_diff_columns = [col for col in common_columns if src_null_counts[col] != target_null_counts[col]]

        # Identify data mismatch columns
        mismatch_cols = []
        for col_name in common_columns:
            src_values = df_src.select(col_name).distinct()
            target_values = df_target.select(col_name).distinct()
            if src_values.exceptAll(target_values).count() > 0 or target_values.exceptAll(src_values).count() > 0:
                mismatch_cols.append(col_name)

        # Determine status & comment
        if not src_missing_cols and not target_missing_cols and src_count == target_count and src_vs_target_diff == 0 and target_vs_src_diff == 0 and not null_diff_columns and not mismatch_cols:
            status, comment = "PASS", "PASS"
        elif not src_missing_cols and not target_missing_cols and src_count != target_count:
            status, comment = "FAIL", "Count fails"
        elif not src_missing_cols and not target_missing_cols and src_vs_target_diff > 0 and target_vs_src_diff > 0:
            status, comment = "FAIL", "Data validation fails"
        elif not src_missing_cols and not target_missing_cols and not mismatch_cols and null_diff_columns:
            status, comment = "FAIL", "Null validation fails"
        else:
            status, comment = "FAIL", "DDL mismatch or data missing"

    except Exception as e:
        status, comment = "ERROR", f"Error processing table: {str(e)}"
        src_count, target_count, src_vs_target_diff, target_vs_src_diff, null_diff_columns, mismatch_cols = -1, -1, -1, -1, [], []

    # Capture run time per table
    table_end_time = time.time()
    runtime_seconds = round(table_end_time - table_start_time, 2)

    # Append results
    results.append((target_table, src_db, target_db, src_missing_cols, target_missing_cols, src_count, target_count, 
                    src_vs_target_diff, target_vs_src_diff, null_diff_columns, mismatch_cols, status, comment, str(current_timestamp())))

# If no valid data, create an empty DataFrame with schema
if not results:
    df_results = spark.createDataFrame([], schema)
else:
    # Convert results to DataFrame
    df_results = spark.createDataFrame(results, schema)

# Truncate and load results into the output table
df_results.write.mode("overwrite").saveAsTable(f"{catalog}.default.{output_table}")

# Display results in Databricks
display(df_results)

print(f"Script execution completed in {round(time.time() - start_time, 2)} seconds.")
