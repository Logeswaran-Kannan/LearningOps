import dlt
from pyspark.sql.functions import col, count, lit, expr, current_timestamp, max as spark_max
import time

# Define catalog and databases
catalog = "core_tst_sys9"
source_db = f"{catalog}.ods"
target_db = f"{catalog}.ods_views"
output_table = "data_migration_validation_results"

# Parameters
exclude_columns = {"DWH_FROM_DATE", "DWH_UNTIL_DATE", "DWH_CURRENT_FLAG"}
exclude_tables = {"table_to_exclude1", "table_to_exclude2"}  # Add table names to exclude
exclude_table_texts = ["_change_", "_sqldf"]  # Exclude tables containing this text

aure_load_date_start = "2024-10-08T09:07:48.000+00:00"
aure_load_date_end = "2024-10-10T09:07:48.000+00:00"

# Fetch list of tables from source, ensuring exclusions
@dlt.table(comment="Stores data migration validation results")
def data_migration_results():
    return spark.sql(f"""
        SELECT 
            CAST(NULL AS BIGINT) AS run_id,
            CAST(NULL AS STRING) AS table_name,
            CAST(NULL AS STRING) AS src_database,
            CAST(NULL AS STRING) AS tgt_database,
            CAST(NULL AS STRING) AS src_vs_tgt_missing_columns,
            CAST(NULL AS STRING) AS tgt_vs_src_missing_columns,
            CAST(NULL AS BIGINT) AS src_count,
            CAST(NULL AS BIGINT) AS tgt_count,
            CAST(NULL AS BIGINT) AS src_duplicate_count,
            CAST(NULL AS BIGINT) AS tgt_duplicate_count,
            CAST(NULL AS BIGINT) AS src_vs_tgt_data_mismatch_count,
            CAST(NULL AS BIGINT) AS tgt_vs_src_data_mismatch_count,
            CAST(NULL AS STRING) AS null_diff_columns,
            CAST(NULL AS STRING) AS data_mismatch_columns,
            CAST(NULL AS STRING) AS status,
            CAST(NULL AS STRING) AS comment,
            CAST(NULL AS DOUBLE) AS time_taken_seconds,
            CAST(NULL AS STRING) AS source_sql,
            CAST(NULL AS STRING) AS target_sql,
            CAST(NULL AS TIMESTAMP) AS validation_timestamp,
            CAST(NULL AS STRING) AS test_case_detail
        WHERE FALSE
    """)

# Generate Run ID (incremental value)
try:
    max_run_id = spark.read.table(output_table).agg(spark_max("run_id")).collect()[0][0]
    run_id = 1 if max_run_id is None else max_run_id + 1
except Exception:
    run_id = 1

def get_target_table_name(table_name):
    if "billingcenter_" in table_name:
        return f"vw_billingcenter_current_{table_name.split('billingcenter_')[-1]}"
    elif "policycenter_" in table_name:
        return f"vw_policycenter_current_{table_name.split('policycenter_')[-1]}"
    elif "claimcenter_" in table_name:
        return f"vw_claimcenter_current_{table_name.split('claimcenter_')[-1]}"
    else:
        return f"vm_{table_name}"

def validate_table(table_name):
    target_table_name = get_target_table_name(table_name)
    start_time = time.time()
    
    try:
        # Load source and target data to extract column names
        src_df = spark.table(f"{source_db}.{table_name}")
        tgt_df = spark.table(f"{target_db}.{target_table_name}")
        
        # Get common columns for both tables
        common_cols = list(set(src_df.columns) & set(tgt_df.columns) - exclude_columns)
        column_list = ", ".join(common_cols)
        
        # Generate SQL queries for source and target including column names
        source_sql = f"SELECT {column_list} FROM {source_db}.{table_name} WHERE AURE_LOAD_DATE BETWEEN '{aure_load_date_start}' AND '{aure_load_date_end}'"
        target_sql = f"SELECT {column_list} FROM {target_db}.{target_table_name} WHERE AURE_LOAD_DATE BETWEEN '{aure_load_date_start}' AND '{aure_load_date_end}'"
        
        # Execute SQL queries
        src_df = spark.sql(source_sql)
        tgt_df = spark.sql(target_sql)
        
        # Row count comparison
        src_count = src_df.count()
        tgt_count = tgt_df.count()
        count_match = src_count == tgt_count
        
        # Duplicate check
        src_duplicate_count = src_df.groupBy(common_cols).count().filter("count > 1").count()
        tgt_duplicate_count = tgt_df.groupBy(common_cols).count().filter("count > 1").count()
        
        # Null count comparison
        null_diffs = []
        for col_name in common_cols:
            src_null_count = src_df.filter(col(col_name).isNull()).count()
            tgt_null_count = tgt_df.filter(col(col_name).isNull()).count()
            if src_null_count != tgt_null_count:
                null_diffs.append(col_name)
        
        # Data comparison
        mismatched_cols = []
        comparison_count_src = src_df.exceptAll(tgt_df).count()
        comparison_count_tgt = tgt_df.exceptAll(src_df).count()
        
        if comparison_count_src > 0 or comparison_count_tgt > 0:
            for col_name in common_cols:
                if src_df.select(col_name).exceptAll(tgt_df.select(col_name)).count() > 0:
                    mismatched_cols.append(col_name)
        
        # Test case detail
        test_case_detail = f"Bronze_validation_{table_name}"
        
        # Determine status and comments
        if not common_cols:
            status = "FAIL"
            comment = "No common columns for comparison"
        elif not count_match:
            status = "FAIL"
            comment = "Count Validation Fails"
        elif comparison_count_src > 0 or comparison_count_tgt > 0:
            status = "FAIL"
            comment = "Data Validation Fails"
        elif null_diffs:
            status = "FAIL"
            comment = "Null Validation Fails"
        elif src_duplicate_count > 0 or tgt_duplicate_count > 0:
            status = "FAIL"
            comment = "Duplicate Data Found"
        else:
            status = "PASS"
            comment = "Pass"
        
        time_taken = float(time.time() - start_time)
        validation_timestamp = current_timestamp()
        
        # Append results to Delta Live Table
        result_df = spark.createDataFrame([
            (run_id, table_name, source_db, target_db, "", "", src_count, tgt_count, 
             src_duplicate_count, tgt_duplicate_count, comparison_count_src, comparison_count_tgt, ",".join(null_diffs), ",".join(mismatched_cols), 
             status, comment, time_taken, source_sql, target_sql, validation_timestamp, test_case_detail)
        ], schema=spark.table(output_table).schema)
        
        result_df.write.mode("append").saveAsTable(output_table)
        
    except Exception as e:
        print(f"Error processing table {table_name}: {e}")

# Fetch tables dynamically
source_tables = [row.tableName for row in spark.sql(f"SHOW TABLES IN {source_db}").collect()]
source_tables = [table for table in source_tables if table not in exclude_tables and not any(exclude_text in table for exclude_text in exclude_table_texts)]

# Iterate over each table and validate
for table in source_tables:
    validate_table(table)

print("Data migration validation completed.")
