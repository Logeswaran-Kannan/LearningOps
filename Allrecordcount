from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import re

# === Setup ===
catalog = "core_tst"
schemas = ["ods", "ods_views"]
excluded_columns = ["AZURE_LOAD_DATE", "DWH_FROM_DATE", "DWH_UNTIL_DATE", "ENT_FLAG"]

# === Output list ===
results = []

# === Helper function ===
def check_table_for_sys9(schema, table_name):
    try:
        full_table_name = f"{catalog}.{schema}.{table_name}"
        df = spark.read.table(full_table_name)
        for column in df.columns:
            if column.upper() not in excluded_columns:
                if df.schema[column].dataType.simpleString() in ['string', 'varchar']:
                    match_df = df.filter(col(column).contains("_sys9_")).select(column).limit(1)
                    if match_df.count() > 0:
                        value = match_df.collect()[0][0]
                        results.append((schema, table_name, column, value))
    except Exception as e:
        print(f"⚠️ Skipped {schema}.{table_name} due to error: {str(e)}")

# === Loop through schemas and tables ===
for schema in schemas:
    tables = spark.catalog.listTables(f"{catalog}.{schema}")
    for table in tables:
        check_table_for_sys9(schema, table.name)

# === Display results ===
result_df = spark.createDataFrame(results, ["schema", "table", "column", "sample_value"])
display(result_df)
