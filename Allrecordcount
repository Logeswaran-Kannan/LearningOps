def validate_table(table_name):
    target_table_name = get_target_table_name(table_name)
    start_time = time.time()
    
    try:
        src_df = spark.table(f"{source_db}.{table_name}")
        tgt_df = spark.table(f"{target_db}.{target_table_name}")
        
        common_cols = list(set(src_df.columns) & set(tgt_df.columns) - exclude_columns)
        column_list = ", ".join(common_cols)
        
        source_sql = f"SELECT {column_list} FROM {source_db}.{table_name} WHERE AZURE_LOAD_DATE BETWEEN '{azure_load_date_start}' AND '{azure_load_date_end}'"
        target_sql = f"SELECT {column_list} FROM {target_db}.{target_table_name} WHERE AZURE_LOAD_DATE BETWEEN '{azure_load_date_start}' AND '{azure_load_date_end}'"
        
        src_df = spark.sql(source_sql)
        tgt_df = spark.sql(target_sql)
        
        src_count = src_df.count()
        tgt_count = tgt_df.count()
        count_match = src_count == tgt_count
        
        src_duplicate_count = spark.sql(f"SELECT {column_list}, count(*) FROM {source_db}.{table_name} GROUP BY {column_list} HAVING count(*) > 1").count()
        tgt_duplicate_count = spark.sql(f"SELECT {column_list}, count(*) FROM {target_db}.{target_table_name} GROUP BY {column_list} HAVING count(*) > 1").count()
        
        null_diffs = []
        for col_name in common_cols:
            src_null_count = src_df.filter(col(col_name).isNull()).count()
            tgt_null_count = tgt_df.filter(col(col_name).isNull()).count()
            if src_null_count != tgt_null_count:
                null_diffs.append(col_name)
        
        mismatched_cols = []
        comparison_count_src = src_df.exceptAll(tgt_df).count()
        comparison_count_tgt = tgt_df.exceptAll(src_df).count()
        
        status = "PASS" if count_match and not null_diffs and not mismatched_cols else "FAIL"
        comment = "Validation Completed"
        
        time_taken = float(time.time() - start_time)
        
        result_data = [(run_id, table_name, source_db, target_db, "", "", 
                        src_count, tgt_count, src_duplicate_count, tgt_duplicate_count, 
                        comparison_count_src, comparison_count_tgt, ",".join(null_diffs) if null_diffs else None, 
                        ",".join(mismatched_cols) if mismatched_cols else None, 
                        status, comment, time_taken, source_sql, target_sql, 
                        run_timestamp, "Bronze_validation_{table_name}")]
        
        result_df = spark.createDataFrame(result_data, schema=spark.table(output_table).schema)
        result_df.write.mode("append").saveAsTable(output_table)
        
    except Exception as e:
        print(f"Error processing table {table_name}: {e}")

for table in source_tables:
    validate_table(table)

print("Data migration validation completed.")
