from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder.appName("CreateDatabricksJob").getOrCreate()

# Define job name
job_name = "UKS_SIT_DW_E2E_Deployment"

# Define the task list (Validation Task and DLT Pipeline Task)
tasks = [
    {
        "task_key": "ValidationTask",
        "notebook_task": {
            "notebook_path": "/Workspace/Repos/ODS_SIT_AUTOMATION_POC/notebooks/UKS_SIT_DW_E2E/ODS_BRONZE_VALIDATION_ESSENTIAL"
        },
        "new_cluster": {
            "spark_version": "12.2.x-scala2.12",
            "node_type_id": "Standard_DS3_v2",
            "num_workers": 2
        }
    },
    {
        "task_key": "DLT_PIPELINE",
        "notebook_task": {
            "notebook_path": "/Workspace/Repos/ODS_SIT_AUTOMATION_POC/notebooks/UKS_SIT_DW_E2E/DLT_REFRESH_PIPELINE"
        },
        "depends_on": [
            {
                "task_key": "ValidationTask"
            }
        ],
        "new_cluster": {
            "spark_version": "12.2.x-scala2.12",
            "node_type_id": "Standard_DS3_v2",
            "num_workers": 2
        }
    }
]

# Create the Job in Databricks
dbutils.notebook.run("/Workspace/Repos/ODS_SIT_AUTOMATION_POC/notebooks/Create_Job_Helper", timeout_seconds=0, arguments={"job_name": job_name, "tasks": tasks})

print("Job Created Successfully!")
